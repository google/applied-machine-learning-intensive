{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
    "name": "Dimensionality Reduction",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
      "copyright",
        "exercise-1-key-1",
        "exercise-2-key-1",
        "exercise-3-key-1",
        "exercise-4-key-1",
        "exercise-5-key-1",
        "exercise-6-key-1",
        "exercise-7-key-1",
        "exercise-8-key-1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2019 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7PLP9Q30PKtv",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "evBbn0N5ec2H"
      },
      "source": [
        "# Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f5W9rkuBmBu9"
      },
      "source": [
        "Principal Components Analysis (PCA) is one of the most common applications of dimensionality, or complexity reduction.  PCA creates a representation of a variable, or group of variables that explain the most variance in a set of independent, and dependent variables.  In a regression, or classification problem that would mean reducing the number of variables or features to the most important aggregate components, and perhaps discarding those which add little value to our model's predictive power. This is known as feature extraction, and can help simplify your model.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vSwFn8YlaDL2"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xHVHPJhSaEEM"
      },
      "source": [
        "### Learning Objectives\n",
        "\n",
        " * Why we use PCA\n",
        " * How to prepare Data for PCA\n",
        " * When not to use PCA\n",
        " * Selecting an Optimal number of components\n",
        " * Calculate Percent Variance Explained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gGrUipVEawYy"
      },
      "source": [
        "### Prerequisites\n",
        "\n",
        "* Sklearn\n",
        "* Pandas\n",
        "* Numpy\n",
        "* Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0bExBYnwa7i2"
      },
      "source": [
        "### Estimated Duration\n",
        "\n",
        "90 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SavoD7IUa-vY"
      },
      "source": [
        "### Grading Criteria\n",
        "\n",
        "Each exercise is worth 3 points. The rubric for calculating those points is:\n",
        "\n",
        "| Points | Description |\n",
        "|--------|-------------|\n",
        "| 0      | No attempt at exercise |\n",
        "| 1      | Attempted exercise, but code does not run |\n",
        "| 2      | Attempted exercise, code runs, but produces incorrect answer |\n",
        "| 3      | Exercise completed successfully |\n",
        "\n",
        "There are 1 exercises in this Colab so there are 3 points available. The grading scale will be 3 points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nRNG-gImKwQL"
      },
      "source": [
        "### Load Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SfuP5J47Kyfm",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "397CUO9TdKbk",
        "colab_type": "text"
      },
      "source": [
        "## Why PCA?\n",
        "The curse of dimensionality states that analyzing data with high dimensionality can lead to overly complex models that are inefficient, can suffer from overfitting, and tend to have less predictive power.  In machine learning, this often means that your feature space is too large.  Maybe there are more features than columns of data, or perhaps your data is too sparse to draw any statistically significant inferences. PCA may be undesirable In the case where you want your model to be interpretable using your original features, and not the principal components.\n",
        "\n",
        "As a rule of thumb, if your optimal number of components is greater than or equal to your original feature count, you probably shouldn't use PCA.  It is all about finding the the optimal component count that explain the most variance in your model.  In other words, choosing the best feature for your model. \n",
        "\n",
        "Additionally, PCA, and other techniques for dimensionality reduction exist to visualize, and analyze higher dimensional data either in 2D or 3D.  Sometimes PCA is referred to as Singular Value Decomposition (SVD), but we will call it PCA for now.  \n",
        "\n",
        "If you'd like to take a deep dive into the math read [this](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G8u2lYRWbE37"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "PCA works best when features are normally distributed, and have low [Multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity). Because PCA is performing rotations in N-dimensional space, we typically need to standardize, or scale our data down. Essentially, we are reducing the space that our data occupies in higher dimensions by standardizing the distribution, or scaling the range of values down to `(min=0,max=1)`. Each method of scaling has its own data requirements, and there are several flavors of scaling and standardization, so a thorough data analysis of your features should be conducted first in order to make this assessment. \n",
        "\n",
        "You may see the terms scaling, standardizing or centering, and normalizing used interchangeably, and this can be confusing, so let's break down these terms.\n",
        "\n",
        "1. Scaling:  Does not affect the distribution, it just changes the range. \\\n",
        "2. Standardize: Changes the distribution by calculating the standard score  \n",
        "3. Normalizing: Typically normalizes the rows of your dataset.\n",
        "\n",
        "When using PCA to build a predictive model, we typically want to standardize the data with standard scaler.  But some cases like cluster analysis, or NLP may require normalization of rows, not columns.  There also may be other cases outside of PCA where you will need to scale or standardize.\n",
        "\n",
        "Here is a helpful [guide](https://docs.google.com/spreadsheets/d/1woVi7wq13628HJ-tN6ApaRGVZ85OdmHsDBKLAf5ylaQ/edit?usp=sharing) to choosing which method is right for your data.\n",
        "\n",
        "And always checkout the documentation for the implementation library, and you can view the source code for standard scaler [sklearn.preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCoiu8fD_ZOO",
        "colab_type": "text"
      },
      "source": [
        "## Download Wine Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RswSwDKs2bi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'\n",
        "      'machine-learning-databases/wine/wine.data',\n",
        "                header=None)\n",
        "display(df_wine.head())\n",
        "display(df_wine.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHDmQfOdeAXa",
        "colab_type": "text"
      },
      "source": [
        "## Split into training and test data, and Standardize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsFz2xjWDPoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split into training and testing sets\n",
        "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3,\n",
        "    stratify=y, random_state=0\n",
        ")\n",
        "# standardize the features\n",
        "sc = StandardScaler()\n",
        "X_train_std = sc.fit_transform(X_train)\n",
        "X_test_std = sc.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mve0lG1wHT4K",
        "colab_type": "text"
      },
      "source": [
        "## Covariance\n",
        "\n",
        "If variance is the sum of the squared differences in each column of our data, then covariance is \n",
        "\n",
        "$$\\sigma_{jk}=\\frac{1}{n}\\sum{(x_j^i-\\mu_j)^2(x_k^i-\\mu_k)^2}$$\n",
        "\n",
        "Where $\\mu_j$ is the sample mean of feature $j$ and $\\mu_k$ is the sample mean of feature $k$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPxFz0pldSAn",
        "colab_type": "text"
      },
      "source": [
        "## Eigenvectors and values\n",
        "\n",
        "Eigenvectors represent the directional vectors that we search for in the N-dimensional space.  Eigenvalues represent the length of these vectors, and inform us of how much variance is explained by the Nth principal component.  An Eigenvalue of 1 means there is no more information gained beyond the original feature, so it is desirable to have principal components with values greater than 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm28K3vZCaGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cov_mat = np.cov(X_train_std.T)\n",
        "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyy4IAtj--AM",
        "colab_type": "text"
      },
      "source": [
        "## How does PCA work?\n",
        "We use the Covariance matrix above to search for a first component, aka a vector that minimizes the error, or distance from that vector and the data.  This process iterates until a `n_components`, or number of vectors to build n principal components is found.  In Sklearn, you can choose a number of components to solve for, or there automatically choose optimal number of components.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgREpxM9FOTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate cumulative sum of explained variances\n",
        "tot = sum(eigen_vals)\n",
        "var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
        "cum_var_exp = np.cumsum(var_exp)\n",
        "\n",
        "ax, fig = plt.subplots(1,1,figsize=(8,4))\n",
        "# plot explained variances\n",
        "plt.bar(range(1,14), var_exp, alpha=0.5,\n",
        "        align='center', label='individual explained variance')\n",
        "plt.step(range(1,14), cum_var_exp, where='mid',\n",
        "         label='cumulative explained variance')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal component index')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YANCg7rLGWs3",
        "colab_type": "text"
      },
      "source": [
        "Using PCA, we can see the explained variance of each component.  The most variance is explained by the first principal component and drops off around 4 PCs.  We can also see that the cumulative explained variance hits approximately 90% with 8 PCs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCXvnof_dkFW",
        "colab_type": "text"
      },
      "source": [
        "## PCA for feature extraction\n",
        "\n",
        "PCA is just one form of dimensionality reduction, and you will come across others related, as well as other types of dataset transformations.  Do not get too attached to your data.\n",
        "\n",
        "By sorting the eigenpairs (vectors and their values) we can project that data into a lower dimensional space.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arren9U0hQzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a list of (eigenvalue, eigenvector) tuples\n",
        "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))]\n",
        "\n",
        "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
        "eigen_pairs.sort(key=lambda k: k[0], reverse=True)\n",
        "\n",
        "w = np.hstack((eigen_pairs[0][1][:, np.newaxis], eigen_pairs[1][1][:, np.newaxis]))\n",
        "print('Matrix W:\\n', w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MGs2_GLhjjA",
        "colab_type": "text"
      },
      "source": [
        "The result is a projection matrix that is 13x2 created from the top-2 eigenvectors.  We can now use this projection matrix, $W$, to map any sample, $x$ to its 2-dimensional sample vector $x'$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwu8kH47iS42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Project training data onto PC1 and PC2 \n",
        "X_train_pca = X_train_std.dot(w)\n",
        "\n",
        "# Visualize Projection\n",
        "colors = ['r', 'b', 'g']\n",
        "markers = ['s', 'x', 'o']\n",
        "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
        "    plt.scatter(X_train_pca[y_train==l, 0], \n",
        "                X_train_pca[y_train==l, 1], \n",
        "                c=c, label=l, marker=m) \n",
        "plt.xlabel('PC 1')\n",
        "plt.ylabel('PC 2')\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytNtxPhemo7O",
        "colab_type": "text"
      },
      "source": [
        "That is how you can implement PCA from scratch using a covariance matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f858dJUQi84w",
        "colab_type": "text"
      },
      "source": [
        "## Using PCA with Sklearn\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmIPKsW5mQ9X",
        "colab_type": "text"
      },
      "source": [
        "We can now use sklearn to implement PCA, and understand all of the explained variance per component.  If we choose `n_components` to be `None`, then we will get a number of components equal to the number of features in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziKmquQnlHjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=None)\n",
        "X_train_pca = pca.fit_transform(X_train_std)\n",
        "pca.explained_variance_ratio_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oba5UZy_nM8o",
        "colab_type": "text"
      },
      "source": [
        "Now we can use our PCA in a Logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_rPiqcUi7On",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# intialize pca and logistic regression model\n",
        "pca = PCA(n_components=2)\n",
        "lr = LogisticRegression(multi_class='auto', solver='liblinear',\n",
        "                       random_state=0)\n",
        "\n",
        "# fit and transform data\n",
        "X_train_pca = pca.fit_transform(X_train_std)\n",
        "X_test_pca = pca.transform(X_test_std)\n",
        "lr.fit(X_train_pca, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxB0C2I2kKCL",
        "colab_type": "text"
      },
      "source": [
        "This can be visualized using plot decision regions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8Usks4VkrVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
        "    # setup marker generator and color map\n",
        "    markers = ('s', 'x', 'o', '^', 'v')\n",
        "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    # plot the decision surface\n",
        "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                           np.arange(x2_min, x2_max, resolution))\n",
        "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "    Z = Z.reshape(xx1.shape)\n",
        "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    # plot class samples\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x=X[y == cl, 0], \n",
        "                    y=X[y == cl, 1],\n",
        "                    alpha=0.6, \n",
        "                    c=[cmap(idx)],\n",
        "                    edgecolor='black',\n",
        "                    marker=markers[idx], \n",
        "                    label=cl)# plot decision regions for training set\n",
        "\n",
        "\n",
        "plot_decision_regions(X_train_pca, y_train, classifier=lr)\n",
        "plt.xlabel('PC 1')\n",
        "plt.ylabel('PC 2')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgwQuFy6k27f",
        "colab_type": "text"
      },
      "source": [
        "Now plot the decision regions of the classifier, and see if the classes are separable by eye."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdugzEp9lFXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot decision regions for test set\n",
        "plot_decision_regions(X_test_pca, y_test, classifier=lr)\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5WFLU7hWbO8_"
      },
      "source": [
        "## Resources\n",
        "\n",
        "- Examples adapted from \n",
        "[TDS](https://towardsdatascience.com/principal-component-analysis-for-dimensionality-reduction-115a3d157bad)\n",
        "\n",
        "- [Tutorial](https://www.ics.forth.gr/mobile/pca.pdf)\n",
        " \n",
        "\n",
        "- [Math](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf)\n",
        "- [Feature Selection](http://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Swt2fxm-fG_B"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iWq38ASlb2aY"
      },
      "source": [
        "## Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvu2un49oASj",
        "colab_type": "text"
      },
      "source": [
        "Watch this video from [Siraj Raval](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A?feature=embeds_subscribe_title) in class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuVejNj-oP0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%html\n",
        "\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/jPmV3j1dAv4\" \n",
        "        frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; \n",
        "                               gyroscope; picture-in-picture\" allowfullscreen>\n",
        "</iframe>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eKS_GsFa-5Q_"
      },
      "source": [
        "### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jkam75BLTBrm",
        "colab": {}
      },
      "source": [
        "# No Solution Needed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-1-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-1-solution-1"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NJknZ91rn6Vd",
        "colab": {}
      },
      "source": [
        "# Discuss as a class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jv6F4nYJ-9DB"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SiykOJtrWvRi",
        "colab": {}
      },
      "source": [
        "# No validation needed"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}