{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/content/04_classification/06_images_and_video/01-open_cv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "copyright"
   },
   "source": [
    "#### Copyright 2020 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7PLP9Q30PKtv"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5W9rkuBmBu9"
   },
   "source": [
    "# OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIykBQbYXrXA"
   },
   "source": [
    "[OpenCV](https://opencv.org/) is an open-source computer vision library. It comes packaged with many powerful computer vision tools, including image and video processing utilities. The library has a lot of the same functionality as the [Python Image Library (PIL)](https://python-pillow.org/) but also includes some computer vision support that PIL doesn't include.\n",
    "\n",
    "In this lesson we will learn how to use OpenCV to process images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G8u2lYRWbE37"
   },
   "source": [
    "## Load an Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RRFdZwtwLKKV"
   },
   "source": [
    "Start by downloading a small (640x360) version of [this image of a car](https://pixabay.com/illustrations/car-sports-car-racing-car-speed-49278/) from Pixabay and then uploading it to this Colab.\n",
    "\n",
    "**Be sure to load the small 640x360 version of the image for this lab.**\n",
    "\n",
    "After loading the image, we can use matplotlib to view the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RmoZ6R9bKnEH"
   },
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_file = 'car-49278_640.jpg'\n",
    "\n",
    "image = cv.imread(image_file)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GVzr4XfQLO23"
   },
   "source": [
    "### Color Ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQTzqEf1K3Tv"
   },
   "source": [
    "Does something look off? Wasn't the car red when we downloaded the image?\n",
    "\n",
    "OpenCV assumes the image is stored with blue-green-red (BGR) encoding instead of [red-green-blue (RGB)](https://en.wikipedia.org/wiki/RGB_color_model), but matplotlib assumes RGB. So, the reds and blues in the image are inverted when displayed.\n",
    "\n",
    "Why does OpenCV assume images are BGR?\n",
    "\n",
    "BGR was historically a popular storage format used by digital camera manufacturers and many software packages. At the time it was a good choice for a default. Defaults are difficult to change, so BGR is here to stay in OpenCV.\n",
    "\n",
    "It doesn't really matter which format is used as long as the inputs to our model are consistent. However, it can be annoying to look at images with inverted colors. You just need to know how to tell OpenCV to fix it.\n",
    "\n",
    "Luckily it is easy to change from BGR to RGB. We can just use `cvtColor`. There are [scores of conversions](https://docs.opencv.org/3.1.0/d7/d1b/group__imgproc__misc.html) possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OrxvjU6gK7Fy"
   },
   "outputs": [],
   "source": [
    "image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nYyEWvL8Lf8H"
   },
   "source": [
    "## Drawing on Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yMGhqtBvLilk"
   },
   "source": [
    "### Drawing Rectangles on Images\n",
    "\n",
    "Suppose we want to draw a rectangle around objects we identify in an image. This can be done with the OpenCV `rectangle` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mQxOqUiSLhoN"
   },
   "outputs": [],
   "source": [
    "left = 100\n",
    "right = 580\n",
    "top = 100\n",
    "bottom = 300\n",
    "\n",
    "r = 255\n",
    "g = 0\n",
    "b = 0\n",
    "\n",
    "cv.rectangle(image, (left, top), (right, bottom), (r, g, b), thickness=2)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-eyAAN5ZLqcX"
   },
   "source": [
    "### Drawing Text on Images\n",
    "\n",
    "You can also draw text on images using `putText`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytbjF3IxLuzl"
   },
   "outputs": [],
   "source": [
    "left = 150\n",
    "top = 50\n",
    "\n",
    "r = 0\n",
    "g = 0\n",
    "b = 0\n",
    "scale = 1.0\n",
    "thickness = 2\n",
    "\n",
    "cv.putText(image, \"It is a car!\", (left, top), cv.FONT_HERSHEY_SIMPLEX, scale,\n",
    "           [r, g, b], thickness)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MkLGTqteLEHC"
   },
   "source": [
    "## Image Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NW-RlIP9LHfx"
   },
   "source": [
    "Models are trained with images scaled to a specific size and are sensitive to the input size being consistent. One solution is to simply scale the image to the required size using the `resize` method.\n",
    " \n",
    "In the example below, we scale the image to `300x300` pixels. This creates a pretty distorted image, which might affect the training and predictions made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KzsGZK3mLW5t"
   },
   "outputs": [],
   "source": [
    "image_scaled = cv.resize(image, (300, 300))\n",
    "\n",
    "plt.imshow(image_scaled)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mBKsIs4ykMtR"
   },
   "source": [
    "## Cropping With Edge Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGr77pUvkROt"
   },
   "source": [
    "Another strategy is to crop the image using \"edge detection\", then scale the image after you have cropped it down. This strategy can be error-prone, but it can also be really helpful in isolating individual objects in an image.\n",
    "\n",
    "In the case of the car image that we have loaded, cropping based on edge detection is both simple and effective. In images with more noise in the background, automatic cropping will be much more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DbHQuRbjk29f"
   },
   "source": [
    "To begin cropping, we'll rely on OpenCV's [Canny](https://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=canny#canny) detection algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LlNs5Wb-ZOz3"
   },
   "outputs": [],
   "source": [
    "threshold = 200\n",
    "image = cv.imread(image_file)\n",
    "image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "edges = cv.Canny(image, threshold, threshold*2)\n",
    "\n",
    "fig, (orig, edge) = plt.subplots(2)\n",
    "orig.imshow(image, cmap='gray')\n",
    "edge.imshow(edges, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2L5zF8EomT7v"
   },
   "source": [
    "The `threshold` parameter is a tuning value set to the images you are processing. More details can be found on [Canny's Wikipedia page](https://en.wikipedia.org/wiki/Canny_edge_detector).\n",
    "\n",
    "Let's see a few different thresholds in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8TQu_1D8mTiE"
   },
   "outputs": [],
   "source": [
    "fig, (orig, t1, t50, t100, t200, t300, t500) = plt.subplots(7, figsize=(5, 25))\n",
    "\n",
    "orig.imshow(image)\n",
    "t1.imshow(cv.Canny(image, 10, 10*2), cmap='gray')\n",
    "t50.imshow(cv.Canny(image, 50, 50*2), cmap='gray')\n",
    "t100.imshow(cv.Canny(image, 100, 100*2), cmap='gray')\n",
    "t200.imshow(cv.Canny(image, 200, 200*2), cmap='gray')\n",
    "t300.imshow(cv.Canny(image, 300, 300*2), cmap='gray')\n",
    "t500.imshow(cv.Canny(image, 500, 500*2), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ayTn7dXyoa_J"
   },
   "source": [
    "None of these settings do too badly, though a threshold of 10 has a lot of noise, and a threshold of 500 barely outlines the car. We have to remember that our goal is to build a bounding box around the car and crop on that bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a8zzsxWko1G0"
   },
   "source": [
    "Another consideration is that the edge detection algorithm is often more effective if the image is grayscale and if there is some blurring.\n",
    " \n",
    "First let's convert the image to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IZSbCNDXaGSV"
   },
   "outputs": [],
   "source": [
    "img_gray = cv.cvtColor(image, cv.COLOR_RGB2GRAY)\n",
    "_ = plt.imshow(img_gray, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MHRWyjhYpN3P"
   },
   "source": [
    "And now we'll blur the image a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIUmsFFrpL-1"
   },
   "outputs": [],
   "source": [
    "img_gray = cv.blur(img_gray, (3,3))\n",
    "_ = plt.imshow(img_gray, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-anEZ9NopW37"
   },
   "source": [
    "Given this new grayscale and blurred image, we can run the edge detection algorithm again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tbyr6n9Opcrt"
   },
   "outputs": [],
   "source": [
    "fig, (orig, t1, t50, t100, t200, t300, t500) = plt.subplots(7, figsize=(5, 25))\n",
    "\n",
    "orig.imshow(img_gray, cmap='gray')\n",
    "t1.imshow(cv.Canny(img_gray, 10, 10*2), cmap='gray')\n",
    "t50.imshow(cv.Canny(img_gray, 50, 50*2), cmap='gray')\n",
    "t100.imshow(cv.Canny(img_gray, 100, 100*2), cmap='gray')\n",
    "t200.imshow(cv.Canny(img_gray, 200, 200*2), cmap='gray')\n",
    "t300.imshow(cv.Canny(img_gray, 300, 300*2), cmap='gray')\n",
    "t500.imshow(cv.Canny(img_gray, 500, 500*2), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DkIIp4JZpnlz"
   },
   "source": [
    "In this case our edges completely disappear at higher thresholds!\n",
    "\n",
    "The threshold of 200 seemed to perform reasonably well in both situations, so let's stick with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P4KI438bp7Dv"
   },
   "outputs": [],
   "source": [
    "img_canny = cv.Canny(img_gray, 200, 200*2)\n",
    "\n",
    "plt.imshow(img_canny, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "grY78Cwwp-Qk"
   },
   "source": [
    "We now need to find the bounding box around the item in the image that we want to crop. The first step in doing this is to utilize the [findContours](https://docs.opencv.org/3.4/d3/dc0/group__imgproc__shape.html#ga17ed9f5d79ae97bd4c7cf18403e1689a) function. This function returns a list of contours found in the output of the Canny algorithm. The contours are defined by lists of $(x, y)$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Thhz47TwrFow"
   },
   "outputs": [],
   "source": [
    "contours, _ = cv.findContours(img_canny, cv.RETR_TREE,\n",
    "                                 cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "print(len(contours))\n",
    "print(contours[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L41cRvQHr5IA"
   },
   "source": [
    "Given the contours, we can approximate the polygon that the contour forms and then create a bounding box around each contour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dSBmgd_fbHJn"
   },
   "outputs": [],
   "source": [
    "bounding_boxes = []\n",
    "contours_poly = []\n",
    "\n",
    "for contour in contours:\n",
    "  polygon = cv.approxPolyDP(contour, 3, True)\n",
    "  contours_poly.append(polygon)\n",
    "  bounding_boxes.append(cv.boundingRect(polygon))\n",
    "\n",
    "print(len(contours_poly))\n",
    "print(len(bounding_boxes))\n",
    "print(bounding_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EQ1NJtuTUS9g"
   },
   "source": [
    "Let's take a look at all of the bounding boxes on the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GEDykg6JUABc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "image_copy = np.copy(image)\n",
    "\n",
    "x, y, width, height = largest_box\n",
    "for box in bounding_boxes:\n",
    "  cv.rectangle(image_copy, \n",
    "               (box[0], box[1]), (box[0]+box[2], box[1]+box[3]),\n",
    "               [0, 0, 255],\n",
    "               2)\n",
    "\n",
    "_ = plt.imshow(image_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHBNnusgUb2X"
   },
   "source": [
    "No single box seems to capture the entire car, but we can use the outer boundaries to find a unified box.\n",
    "\n",
    "We'll use a very simple algorithm that simply finds the outer boundaries and doesn't care if the boxes overlap. In practice you'd likely want to use a more sophisticated algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xO_Tj80rUrbI"
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = 640, 640, 0, 0\n",
    "\n",
    "for box in bounding_boxes:\n",
    "  if box[0] < x1:\n",
    "    x1 = box[0]\n",
    "  if box[1] < y1:\n",
    "    y1 = box[1]\n",
    "  if box[0] + box[2] > x2:\n",
    "    x2 = box[0] + box[2]\n",
    "  if box[1] + box[3] > y2:\n",
    "    y2 = box[1] + box[3]\n",
    "\n",
    "x1, y1, x2, y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vUB3uIE5VWtm"
   },
   "source": [
    "And then we can draw the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rS2tJnyjVYUh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "image_copy = np.copy(image)\n",
    "\n",
    "cv.rectangle(image_copy, \n",
    "             (x1, y1), (x2, y2),\n",
    "             [0, 0, 255],\n",
    "             2)\n",
    "\n",
    "_ = plt.imshow(image_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DD8Kbvo2BWD"
   },
   "source": [
    "The box does clip the car a bit, but for the most part, the car is within the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AwKwraUa2GT3"
   },
   "source": [
    "Now we need to crop the image to just the car itself.\n",
    "\n",
    "Notice that we pair the `x` coordinate with `height` and the `y` with `width`. This is because we want all of the rows for a given height and the columns for a given width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6aM1Q8cid_Xv"
   },
   "outputs": [],
   "source": [
    "x, y, width, height = largest_box\n",
    "cropped_img = image[y1:y2, x1:x2]\n",
    "_ = plt.imshow(cropped_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYccBq4k2bQv"
   },
   "source": [
    "Now we need to make the image into a square by padding the image. We find the longest side and then pad the shorter side with the necessary pixels to make the image a square.\n",
    "\n",
    "To add the padding we use OpenCV's `copyMakeBorder` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tygNEuGXe2Aj"
   },
   "outputs": [],
   "source": [
    "height = cropped_img.shape[0]\n",
    "width = cropped_img.shape[1]\n",
    "\n",
    "left_pad, right_pad, top_pad, bottom_pad = 0, 0, 0, 0\n",
    "if height > width:\n",
    "  left_pad = int((height-width) / 2)\n",
    "  right_pad = height-width-left_pad\n",
    "elif width > height:\n",
    "  top_pad = int((width-height) / 2)\n",
    "  bottom_pad = width-height-top_pad\n",
    "\n",
    "img_square = cv.copyMakeBorder(\n",
    "    cropped_img,\n",
    "    top_pad,\n",
    "    bottom_pad,\n",
    "    left_pad,\n",
    "    right_pad,\n",
    "    cv.BORDER_CONSTANT,\n",
    "    value=(255,255,255))\n",
    "\n",
    "_ = plt.imshow(img_square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-XW_qEgu23z9"
   },
   "source": [
    "And finally, we can scale the image down to a 300x300 image to feed to our model using OpenCV's `resize` function again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Im_q3Xf3ggP2"
   },
   "outputs": [],
   "source": [
    "image_scaled = cv.resize(img_square, (300, 300))\n",
    "\n",
    "plt.imshow(image_scaled)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xo1wnMSl3BuN"
   },
   "source": [
    "## Rotating Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MRANxOVN3Hgn"
   },
   "source": [
    "It is sometimes useful to rotate images before feeding them to your model. This increases the size of your training data, and it makes your model more resilient to subtle patterns that might exist within your base images.\n",
    " \n",
    "For example, in a popular fashion image dataset, most boots are pointed in one direction and sandals in the other. When the model attempts to identify a boot pointed in the wrong direction, it will often predict 'sandal' based purely on the orientation of the object.\n",
    " \n",
    "To flip an image on the horizontal or vertical axis, we can just use the `flip` function.\n",
    " \n",
    "Here is an example of flipping an image on the horizontal axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1x9PemhtgrRv"
   },
   "outputs": [],
   "source": [
    "horizontal_img = cv.flip(image_scaled, 0)\n",
    "plt.imshow(horizontal_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A7IbPf3l4tbt"
   },
   "source": [
    "And now the vertical axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ORzVDhHfgyEy"
   },
   "outputs": [],
   "source": [
    "vertical_img = cv.flip(image_scaled, 1)\n",
    "plt.imshow(vertical_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HfXe8OOm4vPZ"
   },
   "source": [
    "And finally, both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhhdEiutg250"
   },
   "outputs": [],
   "source": [
    "horizontal_and_vertical_img = cv.flip(image_scaled, -1)\n",
    "plt.imshow(horizontal_and_vertical_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5WFLU7hWbO8_"
   },
   "source": [
    "# Resources\n",
    "\n",
    "* [OpenCV Documentation on Edge Detection](https://docs.opencv.org/3.4/da/d0c/tutorial_bounding_rects_circles.html)\n",
    "* Canny Edge Detector: [Wikipedia](https://en.wikipedia.org/wiki/Canny_edge_detector), [OpenCV Documentation](https://docs.opencv.org/3.1.0/da/d22/tutorial_py_canny.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Swt2fxm-fG_B"
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vaiwWLygsq5M"
   },
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iWq38ASlb2aY"
   },
   "source": [
    "We have seen how to rotate an image on its horizontal and vertical axes. This technique works well for increasing the size of your training set and the capabilities of your model, while also providing resiliency to biases that might be hidden in your data.\n",
    "\n",
    "It is also possible to rotate an image by different angles.\n",
    "\n",
    "Use OpenCV to take our `image_scaled` image from above and rotate it so that the car is angled at 45 degrees. Do this for every corner of the squared image.\n",
    " \n",
    "There should be eight images in total. The order of the images isn't important, but the variety is. There should be one image for each case below:\n",
    " \n",
    "1. Car pointed to the top-left corner of the image\n",
    "1. Upside-down car pointed to the top-left corner of the image\n",
    "1. Car pointed to the top-right corner of the image\n",
    "1. Upside-down car pointed to the top-right corner of the image\n",
    "1. Car pointed to the bottom-left corner of the image\n",
    "1. Upside-down car pointed to the bottom-left corner of the image\n",
    "1. Car pointed to the bottom-right corner of the image\n",
    "1. Upside-down car pointed to the bottom-right corner of the image\n",
    "\n",
    "Display the images using `matplotlib.pyplot`.\n",
    " \n",
    "Hint: Check out the `getRotationMatrix2D` and `warpAffine` methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CYZEXNK1VDIJ"
   },
   "source": [
    "### **Student Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TI_WxOyjcfNu"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4lI64FdEeP1-"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "copyright",
    "exercise-1-key-1"
   ],
   "include_colab_link": true,
   "name": "OpenCV",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
