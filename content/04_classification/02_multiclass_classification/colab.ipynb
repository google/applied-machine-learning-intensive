{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/content/04_classification/02_multiclass_classification/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7xdhY7iQgUSU"
   },
   "source": [
    "#### Copyright 2020 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sTI6TJZfOdZH"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ZfzpqO9atfK"
   },
   "source": [
    "# Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S8pjZcxYWZ8Y"
   },
   "source": [
    "We previously created a binary classification model that determined if a piece of fruit was an orange or a grapefruit. There are many problems where binary classification can provide impactful solutions: spam or not spam in an email classifier, hit or hold in a blackjack simulation, buy or not in a stock market analysis. The list is basically endless.\n",
    "\n",
    "There are other cases, however, where we want to make a decision across three or more classes. This is multiclass classification.\n",
    "\n",
    "For many applications, multiclass classification can be broken down into many binary classification problems. These models employ a one-vs-all or one-vs-one strategy to create many binary classification tasks that are then aggregated into a multiclass classification model. Neural networks, decision trees, and k-nearest neighbors models are all capable of performing multiclass classification directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a1cjMb2GPREz"
   },
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUC2a6L5OqIl"
   },
   "source": [
    "For this unit we are going to use a classic machine learning dataset, the [Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set). This is a dataset that was used in 1936 by British biologist and statistician Ronald Fisher to classify iris flowers into one of three species based on four measurements:\n",
    "\n",
    "- The length of the petals\n",
    "- The width of the petals\n",
    "- The length of the sepals (the green petal-looking bits that are found at the base of the petals)\n",
    "- The width of the sepals\n",
    "\n",
    "Conveniently, the iris dataset is built into the scikit-learn library, so it is readily available to us. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "839qQ8bWalSl"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris_bunch = datasets.load_iris()\n",
    "iris_bunch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zweFHsbUgEUL"
   },
   "source": [
    "Scikit-learn datasets are usually delivered in the form of a dictionary-like object called a `Bunch`. This `Bunch` contains the following fields:\n",
    "\n",
    "- *DESCR*: A string describing the dataset.\n",
    "- *data*: An array containing the features we are using for classifying. In this case, it's the four measurements listed above for each of 150 plants.\n",
    "- *feature_names*: Labels for the data.\n",
    "- *filename*: the file that this data came from.\n",
    "- *target*: the values that we are trying to classify these flowers into. In this case, since we are dealing with three species of iris, we use three numbers (0, 1 and 2) to identify each species.\n",
    "- *target_names*: labels for the target values. In this case, 0 refers to the setosa species, 1 to the versicolor species, and 2 to the virginica species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GqOmZ2ORJhTj"
   },
   "source": [
    "We'll create a list of columns that we'll use for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iMqyeotgJoAo"
   },
   "outputs": [],
   "source": [
    "FEATURES = iris_bunch['feature_names']\n",
    "TARGET = 'species'\n",
    "\n",
    "FEATURES, TARGET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fQiAihz5DCfP"
   },
   "source": [
    "Next we will load the feature and target data into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4df5QLtC8kK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iris_df = pd.DataFrame(iris_bunch['data'], columns=FEATURES)\n",
    "iris_df[TARGET] = iris_bunch['target']\n",
    "\n",
    "iris_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8NjnaQldDJLl"
   },
   "source": [
    "Let's take a look at a description of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNZ2fI-gDWMF"
   },
   "outputs": [],
   "source": [
    "iris_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9mEyzlzmDkNR"
   },
   "source": [
    "There are 150 data points. No columns seem to be missing data and no values seem to be too far out of expected ranges. For example, there are no zero or negative lengths or widths, and the length and width values fall well within what we'd expect for a tulip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7sOwA2U2EAJs"
   },
   "source": [
    "We are interested in using the measurement features to predict the species of an iris. Let's take a closer look at the values we'll be predicting.\n",
    "\n",
    "In this case we'll group by our 'species' feature and get a count of each species in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FPb2c7WZD4P_"
   },
   "outputs": [],
   "source": [
    "iris_df.groupby(TARGET).agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TRzeO3bmcd25"
   },
   "source": [
    "We have 50 examples of each species of iris, and overall we have only 150 samples. This presents two challenges. First, we don't have much data to actually build a model from. Second, the data that we do have is evenly distributed over class types. We might want to make sure that we train over the same distribution.\n",
    "\n",
    "Luckily, there are solutions to both of these issues!\n",
    "\n",
    "When we have data that has some weighted distribution across classes, we can do a **stratified split** to ensure that every class appears proportionally in our training data.\n",
    "\n",
    "When we don't have enough data to properly train a model and don't feel that we can pull training data away, we can do a **k-fold cross validation** in order to utilize all of our data for training, while still trying to minimize model overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "viXOR3AseNmB"
   },
   "source": [
    "## Stratified Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lE_6f7eMsiW0"
   },
   "source": [
    "Let's first split off a set of data to use for our final model testing. We can use scikit-learn's `train_test_split` function to do this.\n",
    "\n",
    "Since we have so little data, we'll only hold out 10% of the data for the final test.\n",
    "\n",
    "After we make the split, we can see how many data points we will train off of for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E9y3IXRaeYPD"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris_df[FEATURES],\n",
    "    iris_df[TARGET],\n",
    "    test_size=0.1,\n",
    "    random_state=45)\n",
    "\n",
    "y_train.groupby(y_train).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "08U3RbJ-vzDn"
   },
   "source": [
    "Yikes! We kept 50 data points for training class 1. That means we left none for final testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YtO4hGGCv5rc"
   },
   "outputs": [],
   "source": [
    "y_test.groupby(y_test).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x--twmjowI6D"
   },
   "source": [
    "### Exercise 1: Stratified Train Test Split\n",
    "\n",
    "We risk not holding out a data point for every class if we don't stratify our train test split. Rewrite the split above to create a stratified split. (If you don't remember how, try looking at the [documentation for `train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) and finding the argument that can be used to stratify the data.) When you are done, there should be 45 data points for each class in the training data and five data points for each class in the testing data. Print the counts to verify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzk1dDO-wrWr"
   },
   "source": [
    "#### **Student Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJ1Cf4YMwutU"
   },
   "outputs": [],
   "source": [
    "# Your Code Goes Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJg9KuFiwv4o"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yGLRCakNgXHa"
   },
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5pjS5qdBxaFJ"
   },
   "source": [
    "Another problem we have is that we have very little data to work with. We only had 150 data points in total and are only going to train using 135 of those data points. If we are going to be hyperparameter tuning, we'll need a test and validation holdout, which will leave us very little data to train on.\n",
    "\n",
    "One way to get around this is to use cross-validation. Cross-validation splits the data into a fixed number of tranches and trains on `n-1` of the tranches. Then it calculates a score using the holdout tranche. It does this repeatedly, holding out one tranche of data for each training pass. By looking at the mean of the scores for each training pass, you can get an idea of how well your model performs without having to specify a test dataset.\n",
    "\n",
    "The `cross_val_score` method is used to perform the cross-validation. In the example below, we divide the data into five tranches and get five scores.\n",
    "\n",
    "Since we are cross-validating with a classifier, scikit-learn automatically performs stratified splits for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cWZ97teWgZuk"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "estimator = SGDClassifier()\n",
    "\n",
    "scores = cross_val_score(\n",
    "    estimator,\n",
    "    X_train[FEATURES],\n",
    "    y_train,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iqc-VZMV-2AN"
   },
   "source": [
    "We can now find the mean score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q76bjKa--Vy7"
   },
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTHzxEVW-4bN"
   },
   "source": [
    "What does this score represent, though? It turns out that it uses the default scoring method for the classifier that we used. In this case we used the [`SGDClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html), which reports accuracy by default.\n",
    "\n",
    "Also note that the estimator isn't trained after running cross-validation. You can run cross-validation to test different data preprocessing pipelines and hyperparameters. Once you are happy with a specific setup, you'll need to train the model with the chosen pipeline and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uSikKKbKAWuG"
   },
   "source": [
    "### Exercise 2: F1 Scoring\n",
    "\n",
    "What if we wanted to use F1 for our scoring metric instead of accuracy?\n",
    "\n",
    "Run `cross_val_score` on an `SGDClassifier` and get the F1 score. Check out the documentation for [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html), [`make_scorer`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html), and [`f1_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) for clues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fk3hK0WGA5WU"
   },
   "source": [
    "#### **Student Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVC1CiX0A6-A"
   },
   "outputs": [],
   "source": [
    "# Your Code Goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irNxnD-VA8Pb"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "74WtdKWkBkAj"
   },
   "source": [
    "# The Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lg-iH2WuCHRd"
   },
   "source": [
    "Since we are now using cross-validation to train the model, we can use our testing holdout data as a final validation. Let's make that clear by renaming the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OnwrWu3fCTe5"
   },
   "outputs": [],
   "source": [
    "X_validation = X_test\n",
    "y_validation = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h9juL1rrCfqK"
   },
   "source": [
    "Now we can work on tuning the model and the model pipeline.\n",
    "\n",
    "Let's first look back at the data going into the model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUCG4eT6DOX5"
   },
   "outputs": [],
   "source": [
    "iris_df[FEATURES].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YIN9j51JDw-a"
   },
   "source": [
    "The data is all in the same order of magnitude, but columns like 'sepal length (cm)' are considerably larger than columns like 'petal width (cm)'.\n",
    "\n",
    "We need to perform some preprocessing to get the data into a more uniform shape before feeding it to the model. To do that we'll use the [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), which removes the mean and subtracts the unit variance from each column of data.\n",
    "\n",
    "To use the `StandardScaler`, we create the object, `fit()` the data, and then `transform()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mdy8I__jDSnY"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(iris_df[FEATURES])\n",
    "\n",
    "pd.DataFrame(\n",
    "    scaler.transform(iris_df[FEATURES]),\n",
    "    columns=FEATURES\n",
    ").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ueBrxZcpEm3C"
   },
   "source": [
    "You can see in the output of `describe()` that the data now all has a standard deviation that approaches one.\n",
    "\n",
    "We need to perform this preprocessing to features before training the model and before getting predictions. It can be error-prone to try to remember to do this. To make the task easier, we can create an estimator [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) that applies our transformations and calls our estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hl0syxvSFO_c"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "estimator = Pipeline(\n",
    "  steps=[\n",
    "    ['scale', StandardScaler()],\n",
    "    ['classifier', SGDClassifier()],\n",
    "  ]\n",
    ")\n",
    "\n",
    "scores = cross_val_score(\n",
    "    estimator,\n",
    "    X_train[FEATURES],\n",
    "    y_train,\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cDkGp3ykFYea"
   },
   "source": [
    "Scaling gave us a considerable jump in accuracy score. Hopefully you see similar results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ylEPncqoFgYu"
   },
   "source": [
    "### Exercise 3: Final Validation\n",
    "\n",
    "Our accuracy results were pretty good, so we aren't going to do any more hyperparameter tuning in this lab. Before we declare victory, though, we should find the F1 score of our validation data. Using our estimator pipeline, calculate the F1 score for `X_validation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lxBqABGUFyXv"
   },
   "outputs": [],
   "source": [
    "# Your Code Goes Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iXoRDEorF0AR"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TaQgAcKkFWcV"
   },
   "source": [
    "# Exercise 4: Winemaker Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y9VTg8uUJfwb"
   },
   "source": [
    "Scikit-learn comes prepackaged with many toy datasets. These can be found in the [`sklearn.datasets` package](https://scikit-learn.org/stable/datasets/index.html). In this exercise we'll be working with the [wine dataset](https://scikit-learn.org/stable/datasets/index.html#wine-dataset).\n",
    "\n",
    "The dataset contains information about the properties of wines produced by three different producers. The grapes that the producers used all come from the same region.\n",
    "\n",
    "The columns are:\n",
    "\n",
    "* alcohol\n",
    "* malic_acid\n",
    "* ash\n",
    "* alcalinity_of_ash\n",
    "* magnesium\n",
    "* total_phenols\n",
    "* flavanoids\n",
    "* nonflavanoid_phenols\n",
    "* proanthocyanins\n",
    "* color_intensity\n",
    "* hue\n",
    "* od280/od315_of_diluted_wines\n",
    "* proline\n",
    "\n",
    "The target column is a 0, 1, or 2. Each number represents a different producer.\n",
    "\n",
    "Your task in this exercise is to create a classifier that can identify the producer based on the wine properties.\n",
    "\n",
    "Use as many code blocks as necessary to examine the data and build and validate your model. Document your process using text blocks and/or comments in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TMNL5-jmJHXY"
   },
   "source": [
    "**Student Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c80bh__Ff3Mx"
   },
   "outputs": [],
   "source": [
    "# Your Code Goes Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJHBoilPKfTU"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "7xdhY7iQgUSU",
    "3cpqbG_ewwv3",
    "Ol20ceK5A-OH",
    "wHbcwpSqF0-9",
    "exercise-7-key-1"
   ],
   "include_colab_link": true,
   "name": "Multiclass Classification",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
