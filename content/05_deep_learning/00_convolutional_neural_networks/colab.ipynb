{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/content/05_deep_learning/00_convolutional_neural_networks/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "axhMVhTdjZ-7"
   },
   "source": [
    "#### Copyright 2020 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bpiN4SUKjbOW"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A-ivEsaENhCB"
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XqPpkBh3Nnct"
   },
   "source": [
    "Convolutional Neural Networks (CNNs) are deep neural networks with the addition of two very special types of layers: **convolutional layers** and **pooling layers**. We will take a look at both in this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YBKkcm6ROLLn"
   },
   "source": [
    "## Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "537m9PSIONyR"
   },
   "source": [
    "Convolutional layers are layers in a neural network that only partially connect to their input layers. The layer is divided into receptive fields that each only look at a portion of the input layer and apply filters to it.\n",
    "\n",
    "Let's see this in action. First, we will create a 100 x 100 x 3 image that contains red vertical stripes centered every 10 pixels on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PxJ9X6YaH6ta"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create an image that is completely black.\n",
    "vertical_stripes = np.zeros((100, 100, 3))\n",
    "\n",
    "# Loop over the image 10 pixels at a time, turning the centerline of vertical\n",
    "# pixels red.\n",
    "for x in range(4, 101, 10):\n",
    "  vertical_stripes[:, x:x+2, 0] = 1.0\n",
    "\n",
    "_ = plt.imshow(vertical_stripes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K0uXETOLP6Jc"
   },
   "source": [
    "Now let's create a filter we'll apply using TensorFlow's [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) function.\n",
    "\n",
    "For illustrative purposes we'll create a filter to extract the red out of the image we just created. The filter will be `10 x 10 x 3`. (`10 x 10` is the size of our receptor because our vertical red lines are centered within every 10 pixels. `3` is the number of color channels we are reading because our image has RGB values.) The final number in the filter (1) is the number of output channels we'd like the filter to produce. These output channels are called \"feature maps.\" You get one feature map per filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kGs7yR7GLFz5"
   },
   "outputs": [],
   "source": [
    "receptor_height, receptor_width = 10, 10\n",
    "input_color_channels, output_color_channels = 3, 1\n",
    "\n",
    "filters = np.zeros(shape=(receptor_height, receptor_width, input_color_channels,\n",
    "                          output_color_channels), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-APXHrDtRNRy"
   },
   "source": [
    "We created our filter and set it to all zeros. We now need to indicate what portion of the receptor field we want to extract data from. In this case we are trying to extract the vertical red line, which we know is centered every ten pixels (pixels 5 and 6). To capture the red line, we'll tell the filter that we only care about the 5th and 6th pixel in every row of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dL9L8RlsRMho"
   },
   "outputs": [],
   "source": [
    "filters[:, 5:7, :, 0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ze9FVE1VSBt_"
   },
   "source": [
    "Now let's get our image ready to pass to our convolutional layer. To do that we package the 3-dimensional image in yet another array to create a dataset for TensorFlow. TensorFlow's convolutional function expects a 4-dimensional dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxp8DjcxSKon"
   },
   "outputs": [],
   "source": [
    "dataset = np.array([vertical_stripes], dtype=np.float32)\n",
    "image_count, image_height, image_width, color_channels = dataset.shape\n",
    "\n",
    "image_count, image_height, image_width, color_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zS8Vmt-5TAgG"
   },
   "source": [
    "To get the image into TensorFlow we need to convert it into a Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y19a0mrfS-cy"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.convert_to_tensor(dataset, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0rnO7icvTkYm"
   },
   "source": [
    "To create our convolutional layer, we use [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). The arguments we are passing it are:\n",
    "\n",
    "*  The image that we are processing.\n",
    "*  The filters we want to apply to the data. In this case we are passing in the filter that will capture the middle vertical pixels in a 10x10 receptor.\n",
    "*  The strides we want the layer to take when operating on the data. In this case we want the input data to be processed for every image and every color channel. The 10s cause the receptor to shift by 10 pixels every vertical and horizontal step through the image. This is exactly our filter size, and it allows us to stay centered on the red vertical lines. In practice you'd likely want some overlap.\n",
    "*  A padding argument we input as \"SAME\", which causes TensorFlow to pad the image if necessary (equal padding on each size) in order to make the filter process the entire image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JAVxgudyTjtX"
   },
   "outputs": [],
   "source": [
    "convolution = tf.nn.conv2d(X, filters, strides=[1, 10, 10, 1], padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sbdSrKvCUmob"
   },
   "source": [
    "We can now run our convolutional layer using a TensorFlow session.\n",
    "\n",
    "Notice our output shape reduces the input image to a 10 x 10 x 1 matrix from a 100 x 100 x 3 matrix. This is because we processed the image using a 10 x 10 single-channel output filter and stepped 10 pixels each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JwRTfa2WLham"
   },
   "outputs": [],
   "source": [
    "output = convolution.numpy()\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pnS92oLHWK2u"
   },
   "source": [
    "Looking at the image isn't very telling. It simply looks like a single-color image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a16GKcd9M4_i"
   },
   "outputs": [],
   "source": [
    "plt.imshow(output[0, :, :, 0 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ynnmYDmDWTdQ"
   },
   "source": [
    "When we look at the data, we can see that the values are uniformly 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q8Q-xZ0_WSad"
   },
   "outputs": [],
   "source": [
    "np.unique(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "anfmmmNtYAWQ"
   },
   "source": [
    "What happens if we include some black pixels by increasing our vertical filter to capture all four vertical pixels in the center (pixels 4-7, rather than just pixels 5-6)? Our output number changes to 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1LEydsmQXepS"
   },
   "outputs": [],
   "source": [
    "filters = np.zeros(shape=(receptor_height, receptor_width, input_color_channels,\n",
    "                          output_color_channels), dtype=np.float32)\n",
    "filters[:, 4:8, :, :] = 1\n",
    "\n",
    "X = tf.convert_to_tensor(dataset)\n",
    "convolution = tf.nn.conv2d(X, filters, strides=[1,10,10,1], padding=\"SAME\")\n",
    "\n",
    "output = convolution.numpy()\n",
    "\n",
    "np.unique(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wqz32nZMYk7-"
   },
   "source": [
    "If we move our filter to only capture black pixels, our output becomes 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OH3g9ZFiXyDo"
   },
   "outputs": [],
   "source": [
    "filters = np.zeros(shape=(receptor_height, receptor_width, input_color_channels,\n",
    "                          output_color_channels), dtype=np.float32)\n",
    "filters[:, :2, :, :] = 1\n",
    "\n",
    "X = tf.convert_to_tensor(dataset)\n",
    "convolution = tf.nn.conv2d(X, filters, strides=[1,10,10,1], padding=\"SAME\")\n",
    "\n",
    "output = convolution.numpy()\n",
    "\n",
    "np.unique(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eDDOOkvsZLRG"
   },
   "source": [
    "Let's look at a convolutional layer on a real image. We'll load a sample image from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QoIJALbzZP48"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_image\n",
    "\n",
    "china = load_sample_image('china.jpg')\n",
    "\n",
    "plt.imshow(china)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xsVJwfWcZWFz"
   },
   "source": [
    "We will package the image in a 4-dimensional matrix for processing by TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zqzNI8cwZb4q"
   },
   "outputs": [],
   "source": [
    "dataset = np.array([china], dtype=np.float32)\n",
    "image_count, image_height, image_width, color_channels = dataset.shape\n",
    "\n",
    "image_count, image_height, image_width, color_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kdL-m32vZh5a"
   },
   "source": [
    "To see the convolutional layer in action, let's recreate our vertical line filter and apply it to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GiOxub1oY_Pb"
   },
   "outputs": [],
   "source": [
    "receptor_height, receptor_width = 10, 10\n",
    "input_color_channels, output_color_channels = 3, 1\n",
    "filters = np.zeros(shape=(receptor_height, receptor_width, input_color_channels,\n",
    "                          output_color_channels), dtype=np.float32)\n",
    "filters[:, 5:7, :, :] = 1\n",
    "\n",
    "image_count, image_height, image_width, color_channels = dataset.shape\n",
    "X = tf.convert_to_tensor(dataset)\n",
    "\n",
    "convolution = tf.nn.conv2d(X, filters, strides=[1,4,4,1], padding=\"SAME\")\n",
    "\n",
    "output = convolution.numpy()\n",
    "\n",
    "plt.imshow(output[0, :, :, 0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zh-uhsWqcfUD"
   },
   "source": [
    "You won't typically define your own filters. You can let TensorFlow discover them by using [tf.keras.layers.Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) instead of [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d).\n",
    "\n",
    "In this example we ask for three features with a 5x5 visual receptor, stepping two pixels at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ufZGmFneaTfE"
   },
   "outputs": [],
   "source": [
    "image_count, image_height, image_width, color_channels = dataset.shape\n",
    "X = tf.convert_to_tensor(dataset)\n",
    "\n",
    "convolution = tf.keras.layers.Conv2D(filters=3, kernel_size=5, strides=[2,2],\n",
    "                               padding=\"SAME\")\n",
    "\n",
    "output = convolution(X)\n",
    "output = output.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eVRW0OmFjJds"
   },
   "source": [
    "Let's look at the first feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AVy5abuBjIIZ"
   },
   "outputs": [],
   "source": [
    "plt.imshow(output[0, :, :, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NiKLRKzui1Qz"
   },
   "source": [
    "Here is the second feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HD7LNVBRiw4y"
   },
   "outputs": [],
   "source": [
    "plt.imshow(output[0, :, :, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LN0iB6JNi4Ox"
   },
   "source": [
    "And the third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WoLh4DO4iy74"
   },
   "outputs": [],
   "source": [
    "plt.imshow(output[0, :, :, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wc3X_R2ac_g9"
   },
   "source": [
    "## Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ri1rpacidD3k"
   },
   "source": [
    "Pooling layers are used to shrink the data from their input layer by sampling the data per receptor. Let's look at an example. We'll first load a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJiUonz2dmnP"
   },
   "outputs": [],
   "source": [
    "flower = load_sample_image('flower.jpg')\n",
    "\n",
    "plt.imshow(flower)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YsFrv0BCdrPX"
   },
   "source": [
    "We can package this image in a 4-dimensional matrix and pass it to the [tf.nn.max_pool](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool) function. This function extracts the maximum value from each receptor field.\n",
    "\n",
    "In the example below, we create a 2 x 2 receptor and move it around the image, shifting 2 pixels each time. This reduces the height and width of the image by half, effectively reducing our dataset size by 75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J_8YirYZswEH"
   },
   "outputs": [],
   "source": [
    "dataset = np.array([flower], dtype=np.float32)\n",
    "\n",
    "X = tf.convert_to_tensor(dataset)\n",
    "max_pool = tf.nn.max_pool(X, ksize=[1,2,2,1], strides=[1,2,2,1],\n",
    "                          padding=\"VALID\")\n",
    "\n",
    "output = max_pool.numpy()\n",
    "\n",
    "plt.imshow(output[0].astype(np.uint8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWwPocAWentg"
   },
   "source": [
    "## Exercise 1: Manual Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JD9H1pj9vK9z"
   },
   "source": [
    "Use [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) to apply a stack of filters to the scikit-learn built-in flower image mentioned earlier in this colab.\n",
    "\n",
    "* Create a (7, 7, 3, 2) filter set. The `2` at the end indicates that we'll create two filters and get two output channels (feature maps).\n",
    "* Make the first filter be a vertical line filter on the middle pixel of each row.\n",
    "* Make the second filter be a horizontal line filter on the middle pixel of each row.\n",
    "* Pass the flower image and filters to [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d), stepping 3 pixels vertically and horizontally.\n",
    "* Display the first feature map as an image.\n",
    "* Display the second feature map as an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QK1tz626LjZU"
   },
   "source": [
    "### **Student Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PlBg4KCAgumu"
   },
   "outputs": [],
   "source": [
    "# Create your filters and apply them to the flower image using TensorFlow here.\n",
    "\n",
    "# Use PyPlot to output the first feature map here.\n",
    "\n",
    "# Use PyPlot to output the second feature map here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IR06VZ5WahH1"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b3WIUe0E9449"
   },
   "source": [
    "## Building a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fmstsPo997_3"
   },
   "source": [
    "Now that we have learned about the component parts of a convolutional neural network, let's actually build one.\n",
    "\n",
    "In this section we will use the [Fruits 360](https://www.kaggle.com/moltean/fruits) dataset that is hosted on Kaggle.\n",
    "\n",
    "Upload your `kaggle.json` file and run the code below to download the file with the Kaggle API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FgG0xYRd_VcG"
   },
   "outputs": [],
   "source": [
    "! chmod 600 kaggle.json && (ls ~/.kaggle 2>/dev/null || mkdir ~/.kaggle) && mv kaggle.json ~/.kaggle/ && echo 'Done'\n",
    "! kaggle datasets download moltean/fruits\n",
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "REwLrScD_p5M"
   },
   "source": [
    "The dataset file is `fruits.zip`. Let's unzip and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pZe_vKWN_tKG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "zipfile.ZipFile('fruits.zip').extractall()\n",
    "os.listdir('./fruits-360/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U4WFEhm0AuhS"
   },
   "source": [
    "We've listed the unzipped directory. Inside it there are two primary folders we'll work with in this dataset:\n",
    "\n",
    "* Test\n",
    "* Training\n",
    "\n",
    "There are folders for each category in the `Test` and `Training` folders. Let's make sure all of the categories are represented in test and train, and let's see how many categories we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mJwh4DU_5eGb"
   },
   "outputs": [],
   "source": [
    "train_dir = './fruits-360/Training'\n",
    "train_categories = set(os.listdir(train_dir))\n",
    "test_dir = './fruits-360/Test'\n",
    "test_categories = set(os.listdir(test_dir))\n",
    "\n",
    "if train_categories.symmetric_difference(test_categories):\n",
    "  print(\"Warning!: \", train_categories.symmetric_difference(test_categories))\n",
    "\n",
    "print(sorted(train_categories))\n",
    "print(len(train_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nUJr6eu15_pG"
   },
   "source": [
    "`131` categories, each with representation in test and train.\n",
    "\n",
    "According to the documentation, the images are all `100x100` pixels. Let's load one and see what the images look like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "io3fIE-91Yt2"
   },
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_dir = os.path.join(train_dir, 'Lychee')\n",
    "img = cv.imread(os.path.join(sample_dir, os.listdir(sample_dir)[0]))\n",
    "_ = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0xzM67AT7ZnA"
   },
   "source": [
    "We can also verify that the shape is what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PjmCDDpf7YGI"
   },
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lt2uTaUB7dUN"
   },
   "source": [
    "We find a `100x100` pixel image with three channels of color.\n",
    "\n",
    "We can see the color encoding range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fXr_vnFi7jLy"
   },
   "outputs": [],
   "source": [
    "img.min(), img.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vepamLrL7lNt"
   },
   "source": [
    "This hints at a `[0, 255]` range. Depending on how long our model takes to train, it might be wise to scale the values down to `[0.0, 1.0]`, but we'll hold off for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0-JhkdKH3GU"
   },
   "source": [
    "Now we need to find a way to get the images into the model. TensorFlow Keras has a class called [`DirectoryIterator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/DirectoryIterator) that can help with that.\n",
    "\n",
    "The iterator pulls images from a directory and passes them to our model in batches. There are many settings we can change. In our example here, we set the `target_size` to the size of our input images. Notice that we don't provide a third dimension even though these are RGB files. This is because the default `color_mode` is `'rgb'`, which implies three values.\n",
    "\n",
    "We also set `image_data_generator` to `None`. If we wanted to, we could have passed an [`ImageDataGenerator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) to augment the image and increase the size of our dataset. We'll save this for an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AyPRFN8_H5o-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_dir = './fruits-360/Training'\n",
    "\n",
    "train_image_iterator = tf.keras.preprocessing.image.DirectoryIterator(\n",
    "    target_size=(100, 100),\n",
    "    directory=train_dir,\n",
    "    batch_size=128,\n",
    "    image_data_generator=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e_vxVJVnPiMs"
   },
   "source": [
    "The output for the code above notes that `67,692` images were found across `131` classes. These classes are the directories that were in our root folder. They are sorted, so the actual values of the classes are:\n",
    "\n",
    "* 0 - Apple Braeburn\n",
    "* 1 - Apple Crimson Snow\n",
    "* 2 - Apple Golden 1\n",
    "* ... \n",
    "* 128 - Tomato not Ripened\n",
    "* 129 - Walnut\n",
    "* 130 - Watermelon\n",
    "\n",
    "We can validate that using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pEYbHTgEZVov"
   },
   "outputs": [],
   "source": [
    "print(train_image_iterator.filepaths[np.where(train_image_iterator.labels == 0)[0][0]])\n",
    "print(train_image_iterator.filepaths[np.where(train_image_iterator.labels == 1)[0][0]])\n",
    "print(train_image_iterator.filepaths[np.where(train_image_iterator.labels == 2)[0][0]])\n",
    "print('...')\n",
    "print(train_image_iterator.filepaths[np.where(train_image_iterator.labels == 128)[0][0]])\n",
    "print(train_image_iterator.filepaths[np.where(train_image_iterator.labels == 129)[0][0]])\n",
    "print(train_image_iterator.filepaths[np.where(train_image_iterator.labels == 130)[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "353M9kccQZiY"
   },
   "source": [
    "Let's build our model now. We'll use the [`Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) and [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) classes that we've used in many previous labs, as well as a few new classes:\n",
    "\n",
    "* [`Conv2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) which creates a convolutional layer.\n",
    "* [`MaxPool2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPooling2D) which creates a pooling layer.\n",
    "* [`Flatten`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) which creates a layer that converts a multidimensional tensor down to a flat tensor.\n",
    "\n",
    "You can see the entire model below. We input our images into a convolutional layer followed by a pooling layer. After stacking a few convolutional layers and pooling layers, we flatten the final pooling output and finish with some traditional dense layers. The final dense layer is `131` nodes wide and is activated by softmax. This layer represents our classification predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gyiyMgh5MSRP"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu',\n",
    "                           input_shape=(100, 100, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(131, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UUnosERsR5iC"
   },
   "source": [
    "Now let's start training. Let one or two epochs run but then **!!!! STOP THE CELL FROM RUNNING !!!!**\n",
    "\n",
    "How long was each epoch taking? Ours was taking about `4` minutes. Let's do the math. If each epoch took `4` minutes and we ran `100` epochs, then we'd be training for `400` minutes. That's just under `7` hours of training!\n",
    "\n",
    "Luckily there is a better way. In the menu click on 'Runtime' and then 'Change runtime type'. In the modal that appears, there is an option called 'Hardware accelerator' that is set to 'None'. Change this to 'GPU' and save your settings.\n",
    "\n",
    "Your runtime will change, so you'll need to go back to the start of this section and run all of the cells from the start. Don't forget to upload your `kaggle.json` again.\n",
    "\n",
    "When you get back to this cell a second time and start it running, you should notice a big improvement in training time. We were getting `9` seconds per epoch, which is about `900` seconds total. This totals `15` minutes, which is much better. Let the cell run to completion (hopefully about `15` minutes). You should see it progressing as it is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iMidmU_EMUTu"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_image_iterator,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a57P5HKHUK_r"
   },
   "source": [
    "You might have noticed that each epoch only processed `529` items. These are batches, not images. We set our [`DirectoryIterator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/DirectoryIterator) batch size to `128`. We have `67,692` images. `67,692 / 129 = 524.744186047`, which is close to the `529` number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vNzOc52MUo4L"
   },
   "source": [
    "Now let's plot our training accuracy over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DL0IZZjYSl6D"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list(range(len(history.history['accuracy']))),\n",
    "         history.history['accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-HlbpSdUskN"
   },
   "source": [
    "And our loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ova5HkZhSnYp"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list(range(len(history.history['loss']))), history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOpyloTfUvEV"
   },
   "source": [
    "Over `99%` training accuracy. Let's see how well this generalizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzlENPPuGTtf"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "test_dir = './fruits-360/Test'\n",
    "\n",
    "test_image_iterator = tf.keras.preprocessing.image.DirectoryIterator(\n",
    "    target_size=(100, 100),\n",
    "    directory=test_dir,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    image_data_generator=None)\n",
    "\n",
    "model.evaluate(test_image_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d5pUR7zlH5ZY"
   },
   "source": [
    "When we ran this, we got just under `90%` accuracy, so we are definitely overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5hVo041SIXfT"
   },
   "source": [
    "We can also make predictions. The code below selects the next batch, gets predictions for it, and then returns the first prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ku4AlEdTIC0W"
   },
   "outputs": [],
   "source": [
    "predicted_class = np.argmax(model(next(test_image_iterator)[0])[0])\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUp-vD-TIn6s"
   },
   "source": [
    "This maps to the directory in that position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BJYaW1x3U9vf"
   },
   "outputs": [],
   "source": [
    "os.listdir(train_dir)[predicted_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WriJWpQeX6gF"
   },
   "source": [
    "Overall the model seemed to train well, though overfit a bit. We'll try to address this in the exercise below by augmenting our images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vc6jl5bWaneu"
   },
   "source": [
    "### Exercise 2: `ImageDataGenerator`\n",
    "\n",
    "Recreate the model above using an [`ImageDataGenerator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) to augment the training dataset. When running `fit` be sure to pay attention to the `steps_per_epoch` parameter. It defaults to unbounded, and the generator just keeps on generating if you don't set it.\n",
    "\n",
    "When you have finished training your model, visualize your training loss. \n",
    "\n",
    "Next, use the model to make predictions, and then calculate the F1 score of your validation results.\n",
    "\n",
    "Explain your work.\n",
    "\n",
    "*Use as many code blocks and text blocks as necessary below.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "teJ-Y617kRlg"
   },
   "source": [
    "#### **Student Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0FfIkINkUJi"
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AW_OcgWukNEw"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "axhMVhTdjZ-7",
    "exercise-9-key-1",
    "a85YjVkkkOep"
   ],
   "include_colab_link": true,
   "name": "Convolutional Neural Networks",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
