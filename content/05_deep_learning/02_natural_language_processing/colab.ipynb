{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/content/05_deep_learning/02_natural_language_processing/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "copyright"
   },
   "source": [
    "#### Copyright 2020 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24p97VuTvYVT"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CVmV0M74xwm7"
   },
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XuFjSsW53I9_"
   },
   "source": [
    "Look almost anywhere around you, and you'll see an application of natural language processing (NLP) at work. This broad field covers everything from spellcheck to translation between languages to full machine understanding of human language.\n",
    "\n",
    "In this lesson we'll work through the typical process of an NLP problem. We'll first use a bag-of-words approach to train a simple classifier model. Then we'll use a sequential approach (considering the order of words) to train an RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jk3COdKGIB7l"
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "We will use the [Sentiment Labelled Sentences Data Set](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences) from the UCI Machine Learning Repository. This dataset was used in the paper 'From Group to Individual Labels using Deep Features,' Kotzias et. al., KDD 2015 and contains 3000 user reviews from IMDB, Amazon, and Yelp with the corresponding sentiment of each review (positive: 1 or negative: 0). This supervised problem of predicting sentiment is often called a \"sentiment analysis task.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zu7rUHNXWBue"
   },
   "source": [
    "### Download the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "reZu0k-C1I0S"
   },
   "source": [
    "In order to get reproducible results for this lab, we'll first seed the random number generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PNHl0JpzQWZK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rg-lvIxS1VUb"
   },
   "source": [
    "Next we'll download and unzip the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZXDE22TPKDI"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import io\n",
    "import shutil\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip'\n",
    "\n",
    "# Download zip file from url.\n",
    "zipdata = io.BytesIO()\n",
    "zipdata.write(urllib.request.urlopen(url).read())\n",
    "\n",
    "# Extract zip files.\n",
    "zfile = zipfile.ZipFile(zipdata)\n",
    "zfile.extractall()\n",
    "zfile.close()\n",
    "\n",
    "# Rename directory to \"data\".\n",
    "shutil.rmtree('./data', ignore_errors=True)\n",
    "shutil.move('sentiment labelled sentences', 'data')\n",
    "\n",
    "os.listdir('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZcS-Oi6d1ZKq"
   },
   "source": [
    "There are three files that we'll use in our model: `amazon_cells_labelled.txt`, `imdb_labelled.txt`, and `yelp_labelled.txt`. As you can tell from the `_labelled` portion of the names, this will be a supervised learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u6NIquihWLYa"
   },
   "source": [
    "### Load the Data\n",
    "\n",
    "The downloaded data is split across three files: `amazon_cells_labelled.txt`, `imdb_labelled.txt`, and `yelp_labelled.txt`. Each file has two tab-separated columns, one containing the review text and one containing the sentiment label. Let's combine all the files into one DataFrame, and then get a sense of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-bVLDn02AgU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['review', 'label'])\n",
    "\n",
    "for file in sorted(os.listdir('data')):\n",
    "  if file.endswith('_labelled.txt'):\n",
    "    df = df.append(pd.read_csv(os.path.join('data', file), \n",
    "                               sep='\\t',\n",
    "                               names=['review', 'label']))\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w131eFYj9j-N"
   },
   "source": [
    "Interesting. We were expecting `3000` data points, but only got `2748`. What's going on?\n",
    "\n",
    "It turns out that the IMDB data contains some rows with single double quotes. By default, when the parser sees double quotes, it stops performing a search for another tab until it finds a closing double quote. Since this quote is alone on the line, it causes the parser to \"eat\" quite a few lines of the data file, as illustrated by the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZufwoBxC-h3t"
   },
   "outputs": [],
   "source": [
    "df.iloc[1019]['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J43aVTr2_OIb"
   },
   "source": [
    "In order to get around this, we need to tell the parser to turn off quote detection using the `quoting` argument. The possible values are:\n",
    "\n",
    "Value | Meaning\n",
    "------|----------\n",
    "0     | QUOTE_MINIMAL (default)\n",
    "1     | QUOTE_ALL\n",
    "2     | QUOTE_NONNUMERIC\n",
    "3     | QUOTE_NONE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPhL-XMd_FSn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['review', 'label'])\n",
    "\n",
    "for file in sorted(os.listdir('data')):\n",
    "  if file.endswith('_labelled.txt'):\n",
    "    df = df.append(pd.read_csv(os.path.join('data', file), \n",
    "                               sep='\\t',\n",
    "                               names=['review', 'label'],\n",
    "                               quoting=3))\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_iiV63o_ljO"
   },
   "source": [
    "That looks much better. We got lucky that none of the reviews had embedded tabs, or they would have been quoted and our simple fix would not have worked.\n",
    "\n",
    "Notice that the `read_csv()` call didn't return an error when it encountered an unbalanced quote on a line. It happily loaded the file thinking that the quote was intentional and meant to make the data span multiple lines. *Always verify that the data you loaded looks like you expected it to!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "taAdVw56APF7"
   },
   "source": [
    "Now let's look at a few of the reviews. The [documentation](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences) says that positive reviews are labelled with a `1` and negative with a `0`. Let's sample a few and see if we agree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UGwdM1avAmxD"
   },
   "source": [
    "First the bad,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRPlO9svAZSu"
   },
   "outputs": [],
   "source": [
    "df[df['label'] == 0].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GJMMWjOAoXR"
   },
   "source": [
    "And then the good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rprvFz4_Ajpc"
   },
   "outputs": [],
   "source": [
    "df[df['label'] == 1].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SXuUvgTkAp0U"
   },
   "source": [
    "The sentiment seems to check out. This concludes the EDA that we'll do for this dataset. Let's move on to data preparation for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aARHjVFdA3Gs"
   },
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awhrphCCpc0m"
   },
   "source": [
    "We'll create two different models in this lab. Common to both is the need to split the dataset so that 80% is used for training and the other 20% is used for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPz5DCClTOWQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  df['review'], df['label'].astype('int'),\n",
    "  test_size=0.2, random_state=1000)\n",
    "\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BGMwjSleBaW0"
   },
   "source": [
    "The labels are simple `0` and `1` values, so we don't need to do any preprocessing there. The reviews themselves are variable length text strings. Each model will handle them slightly differently, so we'll save the model-specific preprocessing for when we encounter each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S7mnDwNQW5Y4"
   },
   "source": [
    "## Bag-of-Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "01yjd6ejB6E1"
   },
   "source": [
    "We will first use a bag-of-words (BOW) approach to vectorize the sentences. This means we will consider each review as a \"bag of words,\" where the order of the words does not matter. Using this bag we'll try to assign sentiment to the review.\n",
    "\n",
    "In order to create the bags, we'll use scikit-learn's [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class. The class converts a corpus of text into a sparse matrix that represents the counts of the number of times each word appears in the text.\n",
    "\n",
    "Before applying this to our dataset, let's make sure we understand what's going on. Say we have a couple of sentences that we want to vectorize. One about [bullied buffalo in Buffalo, NY](https://en.wikipedia.org/wiki/Buffalo_buffalo_Buffalo_buffalo_buffalo_buffalo_Buffalo_buffalo) and the other about their peers in Seattle, WA. We can count-vectorize the data, as shown in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GbwJzLjBH3uW"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data = [\n",
    "  \"Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo\",\n",
    "  \"Seattle buffalo Seattle buffalo buffalo buffalo Seattle buffalo\",\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer(data)\n",
    "vectorizer.fit(data)\n",
    "\n",
    "data_vec = vectorizer.transform(data)\n",
    "\n",
    "print(data_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RVMu-guDK3JH"
   },
   "source": [
    "The resultant matrix is:\n",
    "\n",
    "Sentence | buffalo | seattle\n",
    "--|---------|--------\n",
    "\"Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo\" |    8    |    0\n",
    "\"Seattle buffalo Seattle buffalo buffalo buffalo Seattle buffalo\" |    5    |    3\n",
    "\n",
    "As you can see, the first sentence has eight instances of the word *buffalo* and no instances of *seattle*, while the second sentence has five *buffalo* and three *seattle*. Case does not matter, nor does context (used as a noun, verb, etc.). Only the letters count.\n",
    "\n",
    "The representation is a [sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix). In these two sentences consisting of two words, that seems a little strange. But if you think about the fact that there are currently almost `200,000` English words in use while the average sentence is less than `20` words, you can see why sparse matrices make sense here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B4niGgxbMkOJ"
   },
   "source": [
    "And what happens if the data we're transforming contains words we didn't fit the vectorizer to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2UmzNaWVMoBo"
   },
   "outputs": [],
   "source": [
    "data = ['Buffalo Buffalo wings']\n",
    "\n",
    "data_vec = vectorizer.transform(data)\n",
    "\n",
    "print(data_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8NQQNjhEMxaX"
   },
   "source": [
    "Unknown words, such as 'wings' in this case, just don't appear in the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GIQ5moe1NWbW"
   },
   "source": [
    "Let's count-vectorize our training data and see how many words are in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HrQ9vi7xFQfo"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWQShRy7NSrR"
   },
   "source": [
    "We can now transform our training data into a count vector and train a model. For a basic model, we'll use a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oax7EogMFhWT"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "X_train_vec = vectorizer.transform(X_train)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "print('Training accuracy: {}'.format(model.score(X_train_vec, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WctDYqtGNyr7"
   },
   "source": [
    "That is excellent training accuracy. Let's see how well it generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uIqyMEbxN2v9"
   },
   "outputs": [],
   "source": [
    "X_test_vec = vectorizer.transform(X_test)\n",
    "print('Testing accuracy: {}'.format(model.score(X_test_vec, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MtJ2jKdKN6eG"
   },
   "source": [
    "It seems like our model might have overfit a bit. With over `97%` training accuracy and only `86%` testing accuracy, we likely need to work on making our model generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Goq0Frawyuk"
   },
   "source": [
    "### Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7CLmex0HOh6g"
   },
   "source": [
    "So far we have only used a bag of words on raw words to train our model. That's fine in some cases since words are often grammatically in the same class. But what about when they are not? In our \"Buffalo buffalo...\" example, the same word was used to represent a mix of nouns, verbs, and other parts of speech. What if the number of adjectives or nouns or some other part of speech affected the sentiment of the review that we are classifying?\n",
    "\n",
    "We can test this by using a toolkit that classifies words in sentences, and then we feed those classifications into our model. In this section we'll use [spaCY](https://spacy.io/) to add metadata to our reviews and then pass the reviews and metadata through our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JniWAfqQUBD"
   },
   "source": [
    "[spaCy](https://spacy.io) is a library for advanced NLP tools. It's built based on state-of-the-art research and designed to be efficient for industry use. spaCy is extremely useful for extracting more complex linguistic features from text. Another mature and popular Python NLP toolkit is [NLTK](https://www.nltk.org/), which is a bit more academic-oriented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3dwcRnLuQZSa"
   },
   "source": [
    "We must specify a linguistic model for spaCy to use. For this exercise we'll use their \"medium-sized\" English language model. If you already have this model downloaded, you can skip to the `load` step below.\n",
    "\n",
    "**Note:** This is a large file, so it may take a few minutes to download and process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "euRZuE9MLHd0"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpwgrPznQuVQ"
   },
   "source": [
    "After the model is downloaded, we can import it directly using a Python `import` statement. After the import we can load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "en6zi294NrUn"
   },
   "outputs": [],
   "source": [
    "import en_core_web_md\n",
    "\n",
    "spacy_model = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62Dz35jQRNIX"
   },
   "source": [
    "And now we can use spaCY to annotate our data. Let's look at one of our reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D_a7uTn-RItA"
   },
   "outputs": [],
   "source": [
    "X_train.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d6QuqWl0RVhh"
   },
   "source": [
    "We can then call spaCY directly and get information such as the part of speech of each word in our review.\n",
    "\n",
    "spaCy language models process raw text into a `Doc` object, which is a collection of `Token` objects. Each `Token` contains many useful [linguistic annotations](https://spacy.io/usage/linguistic-features). For example, `.text` stores the raw text of a `Token` and `.pos_` stores its Part of Speech (pos) tag.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pr5Rgh1tRGaL"
   },
   "outputs": [],
   "source": [
    "tokens = spacy_model(X_train.iloc[0])\n",
    "for token in tokens:\n",
    "  print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0UnJKG4mR2hZ"
   },
   "source": [
    "Many of the annotations are obvious, such as **NOUN**, but others are less so. The [spaCY annotation documentation](https://spacy.io/api/annotation) is a good place to look if you are unsure about an annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "akEL_tANSLYG"
   },
   "source": [
    "So how do we actually add annotations to our reviews?\n",
    "\n",
    "Since we are using \"bag of words\" annotations at this point, we have a bit of flexibility. We could just add the spaCY output at the end of the sentence:\n",
    "\n",
    "```\n",
    "  the big dog jumps DET ADJ NOUN VERB\n",
    "```\n",
    "\n",
    "or we could add it after each word:\n",
    "\n",
    "```\n",
    "  the DET big ADJ dog NOUN jumps VERB\n",
    "```\n",
    "\n",
    "Functionally these are the same in \"bag of words\" models. Order and case don't matter. If the absolute number of adjectives matter or some other factor like that, then this type of feature engineering could be useful.\n",
    "\n",
    "What if it matters to us \"how\" a word was used, not just \"that\" a word was used? In this case we need to combine the grammar with the word.\n",
    "\n",
    "Let's create a function to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lrVTFtb3TMJL"
   },
   "outputs": [],
   "source": [
    "def add_pos_tags(reviews_raw):\n",
    "  reviews = []\n",
    "  for i, review in enumerate(reviews_raw):\n",
    "    tokens = spacy_model(review)\n",
    "    review_with_pos = []\n",
    "    for token in tokens:\n",
    "      review_with_pos.append(token.text+\"_\"+token.pos_)\n",
    "    reviews.append(' '.join(review_with_pos))\n",
    "  return reviews\n",
    "\n",
    "print(add_pos_tags(\"the big dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m3orSj6uUWcD"
   },
   "source": [
    "Let's now apply this to our entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KhW-TrHJHNj6"
   },
   "outputs": [],
   "source": [
    "X_train_annotated = add_pos_tags(X_train)\n",
    "X_test_annotated = add_pos_tags(X_test)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train_annotated)\n",
    "\n",
    "X_train_vec = vectorizer.transform(X_train_annotated)\n",
    "X_test_vec = vectorizer.transform(X_test_annotated)\n",
    "\n",
    "print(X_train_annotated[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "btnG3mMoHTaM"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "print('Training accuracy: {}'.format(model.score(X_train_vec, y_train)))\n",
    "print('Testing accuracy: {}'.format(model.score(X_test_vec, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YhO3OKgoudYt"
   },
   "source": [
    "Our training accuracy really went up, but our testing accuracy went down. We are overfitting even more now.\n",
    "\n",
    "This isn't much of a surprise, but is interesting to see that adding even more context (features) can allow a model to fit even tighter than can be done with raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w9Clq0KWL2dD"
   },
   "source": [
    "## Sequential Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f8DaPLm2tki9"
   },
   "source": [
    "Much of the meaning of language depends on the order of words: \"That movie was not really good\" is not quite the same as \"That movie was really not good.\" For more complicated NLP tasks, a bag-of-words approach does not capture enough useful information. In this section we will instead work with a Recurrent Neural Network (RNN) model, which is specifically designed to capture information about the order of sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnTNKN6-IDf6"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "We can't use `CountVectorizer` here, so we will need to do some slightly different preprocessing. We can first use the `keras` `Tokenizer` to learn a vocabulary, and then transform each review into a list of indices. Note that we will not include part-of-speech information for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LfcdK_js_dQd"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_tokenized = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tokenized = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "print(X_train.iloc[0])\n",
    "print(X_train_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MpjwSl-cXioW"
   },
   "source": [
    "We need to pad our input so all vectors have the same length. A quick histogram of review lengths shows that almost all reviews have fewer than 100 words. Let's take a closer look at the distribution of lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxrSvmFVWnes"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "review_lengths = [len(review) for review in X_train]\n",
    "plt.hist(review_lengths, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TD1eYtMgKm_z"
   },
   "source": [
    "Almost all reviews have fewer than 50 words! Therefore, we will pad to a maximum review length of 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2-Vzg7YpILj"
   },
   "outputs": [],
   "source": [
    "maxlen = 50\n",
    "\n",
    "X_train_padded = keras.preprocessing.sequence.pad_sequences(\n",
    "    X_train_tokenized, padding='post', maxlen=maxlen)\n",
    "X_test_padded = keras.preprocessing.sequence.pad_sequences(\n",
    "    X_test_tokenized, padding='post', maxlen=maxlen)\n",
    "\n",
    "print(X_train_padded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i9g65GfQ920_"
   },
   "source": [
    "### Pre-Trained Word Embeddings\n",
    "\n",
    "Word embeddings are foundational to most NLP tasks. It's common to experiment with embeddings, feature extraction, or a combination of both to determine what works best with your specific data and problem.\n",
    "\n",
    "In practice, instead of training our own embeddings, we can often take advantage of existing embeddings that have already been trained. This is especially useful when we have a small dataset and want or need the richer meaning that comes from embeddings trained on a larger dataset. \n",
    "\n",
    "There are a variety of extensively pre-trained word embeddings. One of the most powerful and widely-used is [GloVe (Global Vectors for Word Representation)](https://nlp.stanford.edu/projects/glove/). Luckily for us, the spaCy model we downloaded is already integrated with 300-dimensional GloVe embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Knn-312W921J"
   },
   "source": [
    "All we need to do is load these embeddings into an `embedding_matrix` so that each word index properly matches with the words in our dataset. We can access the `tokenizer`'s vocabulary using `.word_index`.\n",
    "\n",
    "*Note: This may take a few minutes to run.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Y0-sVSeuCW6"
   },
   "outputs": [],
   "source": [
    "# Include an extra index for the \"<PAD>\" token.\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "  token = spacy_model(word)[0]\n",
    "  # Make sure spaCy has an embedding for this token.\n",
    "  if not token.is_oov:\n",
    "    embedding_matrix[i] = token.vector\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NRgcHn2WMxwH"
   },
   "source": [
    "Loading the embeddings may take a little while to run. When it's done we'll have an `embedding_matrix` where each word index corresponds to a 300-dimensional GloVe vector. We can load this into an `Embedding` layer to train a model or visualize the embeddings.\n",
    "\n",
    "Also note that we have slightly more tokens now than from using `CountVectorizer`. This means that Keras' `Tokenizer` splits sentences into tokens using slightly different rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v2XyVjq4KBxv"
   },
   "source": [
    "### RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JFw-Fd4YFZZR"
   },
   "source": [
    "This model will have three layers:\n",
    "\n",
    "1. `Embedding`\n",
    "\n",
    "   We initialize its weights using the `embedding_matrix` of pre-trained GloVe embeddings. We set `trainable=False` to prevent the weights from being updated during training. You can keep `trainable=True` to allow for additional training, or \"fine-tuning\", of these weights. We also set `mask_zero=True` to ensure we do not train parameters based on the `\"<PAD>\"` tokens.\n",
    "\n",
    "2. `LSTM` (Long Short-Term Memory)\n",
    "\n",
    "   This is a type of RNN architecture that is especially good at handling long sequences of information. This layer takes input of dimensions `(batch size, maxlen, embedding dimension)` and returns output of dimensions `(batch size, 64)`. A larger output size means a more complex model; we have chosen 64 after tuning based on model performance.\n",
    "\n",
    "3. `Dense`\n",
    "\n",
    "   A final layer to return a prediction of either positive or negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GXGR-ledW0KK"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "  keras.layers.Embedding(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=False,\n",
    "    mask_zero=True\n",
    "  ),\n",
    "  keras.layers.LSTM(64),\n",
    "  keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VbPiVaMfRnZm"
   },
   "source": [
    "We will train this model for 10 epochs since it is slower to train per epoch and reaches high training accuracy after 10 epochs. We use a batch size of 64 based on hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ios8hOAOr6dy"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-xT4yVcBQgH"
   },
   "source": [
    "And finally, we can evaluate the accuracy of the model on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ZjxVWCUr7l7"
   },
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_test_padded, y_test)\n",
    "print('Test accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rpSjxlrOeonH"
   },
   "source": [
    "Note that the final testing set accuracy is not significantly higher than that of our Logistic Regression model. We are using a complex model on a small dataset, which is prone to overfitting. You can usually achieve more generalizable results with a larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SdqWCg3eLldl"
   },
   "source": [
    "# Exercise 1: A Tale of Two Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7UzN5qJnJ8lG"
   },
   "source": [
    "In this exercise we will create a model that can determine if a paragraph was written by Jane Austen or Charles Dickens. We'll use a [dataset containing the works of the two authors](https://www.kaggle.com/joshmcadams/jane-austen-and-charles-dickens) sourced from [Project Gutenberg](https://www.gutenberg.org/).\n",
    "\n",
    "Your task is to download the data and build a classifier that can distinguish between the works of the two authors using techniques covered earlier in this lab. Experiment with different types of models, and see if you can build one that trains and generalizes well.\n",
    "\n",
    "Use as many text and code cells as you need. Be sure to explain your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eEVrX5IuZ0Vk"
   },
   "source": [
    "## **Student Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbFKVbMWZ2ZC"
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RATiicNtZ33w"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "copyright",
    "Dex6HBiIZ4yh"
   ],
   "include_colab_link": true,
   "name": "Natural Language Processing",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
