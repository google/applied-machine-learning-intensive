{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dimensionality Reduction",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "copyright",
        "exercise-1-key-1",
        "exercise-2-key-1",
        "exercise-3-key-1",
        "exercise-4-key-1",
        "exercise-5-key-1",
        "exercise-6-key-1",
        "exercise-7-key-1",
        "exercise-8-key-1"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/content/xx_misc/dimensionality_reduction/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7PLP9Q30PKtv",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "evBbn0N5ec2H"
      },
      "source": [
        "# Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f5W9rkuBmBu9"
      },
      "source": [
        "Principal Component Analysis (PCA) is one of the most common ways to perform dimensionality reduction. PCA takes a set of independent and dependent variables (dimensions) and creates a representation of the variable, or group of variables, that explains the most variance. In a regression or classification problem, that would mean reducing the number of variables or features to the most important aggregate components and perhaps discarding those which add little value to our model's predictive power. This is known as feature extraction, and can help simplify your model.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nRNG-gImKwQL"
      },
      "source": [
        "### Load Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SfuP5J47Kyfm",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "397CUO9TdKbk",
        "colab_type": "text"
      },
      "source": [
        "## Why PCA?\n",
        "\n",
        "The [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) states that analyzing data with high dimensionality can lead to overly complex models that are inefficient, suffer from overfitting, and tend to have less predictive power. In machine learning, this often means that your feature space is too large. Maybe there are more features than columns of data, or perhaps your data is too sparse to draw any statistically significant inferences. PCA simplifies the feature set into a set of \"principal components,\" which are linear combinations of the original features and have low correlation between themselves. PCA may be undesirable in a case where you want your model to be interpretable using your original features, not the principal components.\n",
        "\n",
        "As a rule of thumb, if your optimal number of components is greater than or equal to your original feature count, you probably shouldn't use PCA. It is all about finding the optimal component count, where the components explain the most variance in your model. In other words, you want to choose the best features for your model.\n",
        "\n",
        "PCA and other techniques for dimensionality reduction also help to visualize and analyze higher dimensional data either in 2D or 3D. Sometimes PCA is referred to as Singular Value Decomposition (SVD), but we will call it PCA for now.  \n",
        "\n",
        "If you'd like to take a deep dive into the math (and there is quite a bit of math!), read [these helpful lecture notes](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf) from a statistics course at Carnegie Mellon University."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G8u2lYRWbE37"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "PCA works best when features are normally distributed and have low [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity). Because PCA is performing rotations in N-dimensional space, we typically need to standardize our data. Essentially, we are reducing the space that our data occupies in higher dimensions by standardizing the distribution, or scaling the range of values down to $[0, 1]$. Each method of scaling has its own data requirements, and there are several flavors of scaling and standardization. Therefore, you should first conduct a thorough data analysis of your features in order to make this assessment.\n",
        "\n",
        "You may see the terms \"scaling,\" \"standardizing,\" \"centering,\" and \"normalizing\" used interchangeably. This can be confusing, so let's break down these terms.\n",
        "\n",
        "1. Scaling: Changes the range of the data but does not affect the distribution.\n",
        "\n",
        "2. Standardizing: Changes the distribution of the data by calculating the standard normal score.\n",
        "\n",
        "3. Centering: Shifts the distribution of the data so that the mean is zero.\n",
        "\n",
        "4. Normalizing: [normalizes](https://en.wikipedia.org/wiki/Normalization_(statistics) the rows of your dataset.\n",
        "\n",
        "When using PCA to build a predictive model, we typically want to standardize the data with standard scalers. But some cases (e.g., cluster analysis or NLP) may require normalization of rows, not columns. There also may be other cases outside of PCA where you will need to scale or standardize.\n",
        "\n",
        "[Here is a helpful guide](https://docs.google.com/spreadsheets/d/1woVi7wq13628HJ-tN6ApaRGVZ85OdmHsDBKLAf5ylaQ/edit?usp=sharing) for choosing which method is right for your data.\n",
        "\n",
        "You can always check out the documentation for the implementation library, and scikit-learn's website has [another helpful guide](https://scikit-learn.org/stable/modules/preprocessing.html) to its preprocessing methods.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCoiu8fD_ZOO",
        "colab_type": "text"
      },
      "source": [
        "## Download Wine Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RswSwDKs2bi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_wine = pd.read_csv(\n",
        "    'http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',\n",
        "    header=None)\n",
        "display(df_wine.head())\n",
        "display(df_wine.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHDmQfOdeAXa",
        "colab_type": "text"
      },
      "source": [
        "## Split Into Training and Test Data, and Then Standardize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsFz2xjWDPoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split into training and testing sets.\n",
        "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=0)\n",
        "\n",
        "# Standardize the features.\n",
        "sc = StandardScaler()\n",
        "X_train_std = sc.fit_transform(X_train)\n",
        "X_test_std = sc.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mve0lG1wHT4K",
        "colab_type": "text"
      },
      "source": [
        "## Covariance\n",
        "\n",
        "The [covariance](https://en.wikipedia.org/wiki/Covariance) of features $M$ and $N$ is defined as follows:\n",
        "\n",
        "$$ \\sigma_{MN}^2 = \\frac{1}{n}\\sum_i {(m_i-\\mu_M)^2(n_i-\\mu_N)^2}$$\n",
        "\n",
        "- $\\mu_M$ is the sample mean of feature $M$\n",
        "- $\\mu_N$ is the sample mean of feature $N$.\n",
        "\n",
        "Covariance is an extension of variance; it is an indication of variability within a set of two features, just as variance is an indicator of variability within a feature. Don't worry too much about the math here. The implementation of PCA hides the details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPxFz0pldSAn",
        "colab_type": "text"
      },
      "source": [
        "## Eigenvectors and Values\n",
        "\n",
        "Eigenvectors represent the directional vectors that we search for in the N-dimensional space. Eigenvalues represent the length of these vectors, and they inform us of how much variance is explained by the Nth principal component. An eigenvalue of 1 means there is no more information gained beyond the original feature, so it is desirable to have principal components with values greater than 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm28K3vZCaGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cov_mat = np.cov(X_train_std.T)\n",
        "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrgSvW1tBchy",
        "colab_type": "text"
      },
      "source": [
        "Again, don't worry too much about how eigenvalues and eigenvectors are calculated; most of it is under the hood in scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyy4IAtj--AM",
        "colab_type": "text"
      },
      "source": [
        "## How Does PCA Work?\n",
        "\n",
        "We use the covariance defined above to search for a first component (a vector that minimizes the error or distance from that vector and the data). This process iterates until a `n_components`, or number of vectors to build n principal components is found. In scikit-learn, you can choose a number of components to solve for, or let scikit-learn automatically choose the optimal number of components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgREpxM9FOTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate cumulative sum of explained variances.\n",
        "tot = sum(eigen_vals)\n",
        "var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
        "cum_var_exp = np.cumsum(var_exp)\n",
        "\n",
        "ax, fig = plt.subplots(1,1,figsize=(8, 4))\n",
        "# Plot explained variances.\n",
        "plt.bar(range(1, 14), var_exp, alpha=0.5, align='center',\n",
        "        label='individual explained variance')\n",
        "plt.step(range(1, 14), cum_var_exp, where='mid',\n",
        "         label='cumulative explained variance')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal component index')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YANCg7rLGWs3",
        "colab_type": "text"
      },
      "source": [
        "Using PCA, we can see the explained variance of each component. The most variance is explained by the first principal component and drops off around 4 PCs. We can also see that the cumulative explained variance hits approximately 90% with 8 PCs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCXvnof_dkFW",
        "colab_type": "text"
      },
      "source": [
        "## PCA for Feature Extraction\n",
        "\n",
        "PCA is just one form of dimensionality reduction, and you will come across other related forms, as well as other types of dataset transformations. Transforming your dataset is a key technique in model-building, so don't get too attached to your original dataset.\n",
        "\n",
        "By sorting the eigenpairs (vectors and their values), we can project that data into a lower dimensional space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arren9U0hQzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a list of (eigenvalue, eigenvector) tuples.\n",
        "eigen_pairs = [(np.abs(eigen_vals[i]),\n",
        "                eigen_vecs[:, i]) for i in range(len(eigen_vals))]\n",
        "\n",
        "# Sort the (eigenvalue, eigenvector) tuples from high to low.\n",
        "eigen_pairs.sort(key=lambda k: k[0], reverse=True)\n",
        "\n",
        "w = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
        "               eigen_pairs[1][1][:, np.newaxis]))\n",
        "print('Matrix W:\\n', w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MGs2_GLhjjA",
        "colab_type": "text"
      },
      "source": [
        "The result is a 13x2 projection matrix that is created from the top-2 eigenvectors. We can now use this projection matrix, $W$, to map any sample, $x$, to its 2-dimensional sample vector $x'$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwu8kH47iS42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Project training data onto PC1 and PC2.\n",
        "X_train_pca = X_train_std.dot(w)\n",
        "\n",
        "# Visualize projection.\n",
        "colors = ['r', 'b', 'g']\n",
        "markers = ['s', 'x', 'o']\n",
        "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
        "    plt.scatter(X_train_pca[y_train==l, 0], \n",
        "                X_train_pca[y_train==l, 1], \n",
        "                c=c, label=l, marker=m) \n",
        "plt.xlabel('PC 1')\n",
        "plt.ylabel('PC 2')\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytNtxPhemo7O",
        "colab_type": "text"
      },
      "source": [
        "That is how you can implement PCA from scratch using a covariance matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f858dJUQi84w",
        "colab_type": "text"
      },
      "source": [
        "## Using PCA With scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmIPKsW5mQ9X",
        "colab_type": "text"
      },
      "source": [
        "We can now use scikit-learn to implement PCA and to understand all of the explained variance per component. If we choose `n_components` to be `None`, then we will get a number of components equal to the number of features in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziKmquQnlHjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=None)\n",
        "X_train_pca = pca.fit_transform(X_train_std)\n",
        "pca.explained_variance_ratio_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oba5UZy_nM8o",
        "colab_type": "text"
      },
      "source": [
        "Now we can use our PCA in a logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_rPiqcUi7On",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize pca and logistic regression model.\n",
        "pca = PCA(n_components=2)\n",
        "lr = LogisticRegression(multi_class='auto', solver='liblinear', random_state=0)\n",
        "\n",
        "# Fit and transform data.\n",
        "X_train_pca = pca.fit_transform(X_train_std)\n",
        "X_test_pca = pca.transform(X_test_std)\n",
        "lr.fit(X_train_pca, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxB0C2I2kKCL",
        "colab_type": "text"
      },
      "source": [
        "This can be visualized using plot decision regions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8Usks4VkrVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
        "    # Setup marker generator and color map.\n",
        "    markers = ('s', 'x', 'o', '^', 'v')\n",
        "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    # Plot the decision surface.\n",
        "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                           np.arange(x2_min, x2_max, resolution))\n",
        "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "    Z = Z.reshape(xx1.shape)\n",
        "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    # Plot class samples.\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x=X[y == cl, 0], \n",
        "                    y=X[y == cl, 1],\n",
        "                    alpha=0.6, \n",
        "                    c=[cmap(idx)],\n",
        "                    edgecolor='black',\n",
        "                    marker=markers[idx], \n",
        "                    label=cl) # Plot decision regions for training se.\n",
        "\n",
        "plot_decision_regions(X_train_pca, y_train, classifier=lr)\n",
        "plt.xlabel('PC 1')\n",
        "plt.ylabel('PC 2')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgwQuFy6k27f",
        "colab_type": "text"
      },
      "source": [
        "Now let's plot the decision regions of the classifier and see if the classes are separable by eye."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdugzEp9lFXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot decision regions for test set.\n",
        "plot_decision_regions(X_test_pca, y_test, classifier=lr)\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5WFLU7hWbO8_"
      },
      "source": [
        "## Resources\n",
        "\n",
        "- Examples adapted from \n",
        "[TDS](https://towardsdatascience.com/principal-component-analysis-for-dimensionality-reduction-115a3d157bad)\n",
        "- [Tutorial](https://www.ics.forth.gr/mobile/pca.pdf)\n",
        "- [Math](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf)\n",
        "- [Feature Selection](http://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Swt2fxm-fG_B"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvu2un49oASj",
        "colab_type": "text"
      },
      "source": [
        "Watch this video from [Siraj Raval](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A?feature=embeds_subscribe_title) in class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFc86L_II1nX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import IFrame\n",
        "\n",
        "IFrame(src=\"https://www.youtube.com/embed/jPmV3j1dAv4\", width=\"560\",\n",
        "       height=\"315\", frameborder=\"0\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
