{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Support Vector Machines",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "copyright",
        "exercise-1-key-1",
        "exercise-2-key-1",
        "exercise-3-key-1",
        "exercise-4-key-1"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/content/06_other_models/05_svm/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZJWsDPX6cIC0",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DTotGjoicova"
      },
      "source": [
        "# Support Vector Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bTOkXjCecolX"
      },
      "source": [
        "Support Vector Machines (SVM) are powerful tools for performing both classification and regression tasks. In this colab we'll create a classification model using an SVM in scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zxkLg1AkNtGb"
      },
      "source": [
        "## Load the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lEKsqN3jwYfm"
      },
      "source": [
        "Let's begin by loading a dataset that we'll use for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BXTshi78c4iv",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris_bunch = load_iris()\n",
        "\n",
        "iris_df = pd.DataFrame(iris_bunch.data, columns=iris_bunch.feature_names)\n",
        "iris_df['species'] = iris_bunch.target\n",
        "\n",
        "iris_df.describe() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8BkQvJdEwuOl"
      },
      "source": [
        "You can see in the data description above that the range of values for each of the columns is quite a bit different. For instance, the mean sepal length is almost twice as big as the mean sepal width.\n",
        "\n",
        "SVM is sensitive to features with different scales. We'll run the data through the `StandardScaler` to get all of the feature data scaled.\n",
        "\n",
        "First let's create the scalar and fit it to our features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ecXA8GndgrP",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(iris_df[iris_bunch.feature_names])\n",
        "\n",
        "scaler.mean_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mz8AZsCtxVRb"
      },
      "source": [
        "We can now transform the data by applying the `scaler`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mYBrKIUbeauS",
        "colab": {}
      },
      "source": [
        "iris_df[iris_bunch.feature_names] = scaler.transform(\n",
        "    iris_df[iris_bunch.feature_names])\n",
        "\n",
        "iris_df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xKlDUcz0xr36"
      },
      "source": [
        "Since we scaled the data, the column names are now a bit deceiving. These are no longer unaltered centimeters, but normalized lengths. Let's rename the columns to get \"(cm)\" out of the names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uobpnTYfiTr7",
        "colab": {}
      },
      "source": [
        "iris_df = iris_df.rename(index=str, columns={\n",
        "  'sepal length (cm)': 'sepal_length',\n",
        "  'sepal width (cm)': 'sepal_width',\n",
        "  'petal length (cm)': 'petal_length',\n",
        "  'petal width (cm)': 'petal_width'})\n",
        "iris_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EauTJlfmx5FA"
      },
      "source": [
        "We could use all of the features to train our model, but in this case we are going to pick two features so that we can make some nice visualizations later on in the colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JukPMMb2ivPT",
        "colab": {}
      },
      "source": [
        "features = ['petal_length', 'petal_width']\n",
        "target = 'species'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AfZKRDJwyLRm"
      },
      "source": [
        "Now we can create and train a classifier. There are multiple ways to create an SVM model in scikit-learn. We are going to use the [linear support vector classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "weVHIEeue5xM",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "classifier = LinearSVC()\n",
        "classifier.fit(iris_df[features], iris_df[target])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3YdEuhgWyhFO"
      },
      "source": [
        "We can now use our model to make predictions. We'll make predictions on the data we just trained on in order to get an F1 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "swJb81CdgIRe",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "predictions = classifier.predict(iris_df[features])\n",
        "\n",
        "f1_score(iris_df[target], predictions, average='micro')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d5eMo6VRyyfH"
      },
      "source": [
        "We can visualize the decision boundaries using the pyplot `contourf` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FsuT2Fc7g0b0",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Find the smallest value in the feature data. We are looking across both\n",
        "# features since we scaled them. Make the min value a little smaller than\n",
        "# reality in order to better see all of the points on the chart.\n",
        "min_val = min(iris_df[features].min()) - 0.25\n",
        "\n",
        "# Find the largest value in the feature data. Make the max value a little bigger\n",
        "# than reality in order to better see all of the points on the chart.\n",
        "max_val = max(iris_df[features].max()) + 0.25\n",
        "\n",
        "# Create a range of numbers from min to max with some small step. This will be\n",
        "# used to make multiple predictions that will create the decision boundary\n",
        "# outline.\n",
        "rng = np.arange(min_val, max_val, .02)\n",
        "\n",
        "# Create a grid of points.\n",
        "xx, yy = np.meshgrid(rng, rng)\n",
        "\n",
        "# Make predictions on every point in the grid.\n",
        "predictions = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Reshape the predictions for plotting.\n",
        "zz = predictions.reshape(xx.shape)\n",
        "\n",
        "# Plot the predictions on the grid.\n",
        "plt.contourf(xx, yy, zz)\n",
        "\n",
        "# Plot each class of iris with a different marker.\n",
        "#   Class 0 with circles\n",
        "#   Class 1 with triangles\n",
        "#   Class 2 with squares\n",
        "for species_and_marker in ((0, 'o'), (1, '^'), (2, 's')):\n",
        "  plt.scatter(\n",
        "    iris_df[iris_df[target] == species_and_marker[0]][features[0]],\n",
        "    iris_df[iris_df[target] == species_and_marker[0]][features[1]],\n",
        "    marker=species_and_marker[1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Tg74ogrczAA"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2kvNGHbE1oCr"
      },
      "source": [
        "## Exercise 1: Polynomial SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d1VfxQIclnA1"
      },
      "source": [
        "The scikit-learn module also has an [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) classifier that can use non-linear kernels. Create an `SVC` classifier with a 3-degree polynomial kernel, and train it on the iris data. Make predictions on the iris data that you trained on, and then print out the F1 score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AFj-ndSgOSXG"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xkEjTGN8OU49",
        "colab": {}
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQCTfdK2Xc7J",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-1-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JLzvOQ6-vKYf",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "iris_bunch = load_iris()\n",
        "\n",
        "iris_df = pd.DataFrame(iris_bunch.data, columns=iris_bunch.feature_names)\n",
        "iris_df['species'] = iris_bunch.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(iris_df[iris_bunch.feature_names])\n",
        "\n",
        "iris_df[iris_bunch.feature_names] = scaler.transform(\n",
        "    iris_df[iris_bunch.feature_names])\n",
        "\n",
        "iris_df = iris_df.rename(index=str, columns={\n",
        "  'sepal length (cm)': 'sepal_length',\n",
        "  'sepal width (cm)': 'sepal_width',\n",
        "  'petal length (cm)': 'petal_length',\n",
        "  'petal width (cm)': 'petal_width'})\n",
        "\n",
        "features = ['petal_length', 'petal_width']\n",
        "target = 'species'\n",
        "\n",
        "classifier = SVC(kernel='poly', degree=3)\n",
        "classifier.fit(iris_df[features], iris_df[target])\n",
        "predictions = classifier.predict(iris_df[features])\n",
        "\n",
        "print(f1_score(iris_df[target], predictions, average='micro'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYzUD3ZHXexz",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tHrAE0uP17Pb"
      },
      "source": [
        "## Exercise 2: Plotting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rtwiXlXVlpA3"
      },
      "source": [
        "Create a plot that shows the decision boundaries of the polynomial SVC that you created in exercise 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t-LvggLkOgn_"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "199xq34kOoHz",
        "colab": {}
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM72MxryXjQW",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-2-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-2-solution-1"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J4v51aS51ffx",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "iris_bunch = load_iris()\n",
        "\n",
        "iris_df = pd.DataFrame(iris_bunch.data, columns=iris_bunch.feature_names)\n",
        "iris_df['species'] = iris_bunch.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(iris_df[iris_bunch.feature_names])\n",
        "\n",
        "iris_df[iris_bunch.feature_names] = scaler.transform(\n",
        "    iris_df[iris_bunch.feature_names])\n",
        "\n",
        "iris_df = iris_df.rename(index=str, columns={\n",
        "  'sepal length (cm)': 'sepal_length',\n",
        "  'sepal width (cm)': 'sepal_width',\n",
        "  'petal length (cm)': 'petal_length',\n",
        "  'petal width (cm)': 'petal_width'})\n",
        "\n",
        "features = ['petal_length', 'petal_width']\n",
        "target = 'species'\n",
        "\n",
        "classifier = SVC(kernel='poly', degree=3)\n",
        "classifier.fit(iris_df[features], iris_df[target])\n",
        "predictions = classifier.predict(iris_df[features])\n",
        "\n",
        "print(f1_score(iris_df[target], predictions, average='micro'))\n",
        "\n",
        "min_val = min(iris_df[features].min()) - 1\n",
        "max_val = max(iris_df[features].max()) + 1\n",
        "xx, yy = np.meshgrid(np.arange(min_val, max_val, .02),\n",
        "                     np.arange(min_val, max_val, .02))\n",
        "predictions = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "zz = predictions.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, zz)\n",
        "\n",
        "for species_and_marker in ((0, 'o'), (1, '^'), (2, 's')):\n",
        "  plt.scatter(\n",
        "    iris_df[iris_df[target] == species_and_marker[0]][features[0]],\n",
        "    iris_df[iris_df[target] == species_and_marker[0]][features[1]],\n",
        "    marker=species_and_marker[1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6pK-Z0vXlD2",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ocYP9cb-28Hk"
      },
      "source": [
        "## Exercise 3: C Hyperparameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IiH7HqPalrS4"
      },
      "source": [
        "We accepted the default 1.0 C hyperparameter in the classifier above. Try halving and doubling the C value. How does it affect the F1 score?\n",
        "\n",
        "Visualize the decision boundaries. Do they visibly change?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kdckx6PUOzX0"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8GRkvd4F3i4r",
        "colab": {}
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7k6MPqvX1gJ",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-3-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxVmCsfEX9HV",
        "colab_type": "text"
      },
      "source": [
        "With `C` halved, the decision boundaries smooth out a bit, and the F1 score goes down."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J3wEVWmqO3Eu",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "iris_bunch = load_iris()\n",
        "\n",
        "iris_df = pd.DataFrame(iris_bunch.data, columns=iris_bunch.feature_names)\n",
        "iris_df['species'] = iris_bunch.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(iris_df[iris_bunch.feature_names])\n",
        "\n",
        "iris_df[iris_bunch.feature_names] = scaler.transform(\n",
        "    iris_df[iris_bunch.feature_names])\n",
        "\n",
        "iris_df = iris_df.rename(index=str, columns={\n",
        "  'sepal length (cm)': 'sepal_length',\n",
        "  'sepal width (cm)': 'sepal_width',\n",
        "  'petal length (cm)': 'petal_length',\n",
        "  'petal width (cm)': 'petal_width'})\n",
        "\n",
        "features = ['petal_length', 'petal_width']\n",
        "target = 'species'\n",
        "\n",
        "classifier = SVC(kernel='poly', degree=3, C=0.5)\n",
        "classifier.fit(iris_df[features], iris_df[target])\n",
        "predictions = classifier.predict(iris_df[features])\n",
        "\n",
        "print(f1_score(iris_df[target], predictions, average='micro'))\n",
        "\n",
        "min_val = min(iris_df[features].min()) - 1\n",
        "max_val = max(iris_df[features].max()) + 1\n",
        "xx, yy = np.meshgrid(np.arange(min_val, max_val, .02),\n",
        "                     np.arange(min_val, max_val, .02))\n",
        "predictions = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "zz = predictions.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, zz)\n",
        "\n",
        "for species_and_marker in ((0, 'o'), (1, '^'), (2, 's')):\n",
        "  plt.scatter(\n",
        "    iris_df[iris_df[target] == species_and_marker[0]][features[0]],\n",
        "    iris_df[iris_df[target] == species_and_marker[0]][features[1]],\n",
        "    marker=species_and_marker[1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PvL_cmYYHre",
        "colab_type": "text"
      },
      "source": [
        "With `C` doubled, the decision boundaries get a little more curved, and the F1 score increases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViTbgeuJYCyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "iris_bunch = load_iris()\n",
        "\n",
        "iris_df = pd.DataFrame(iris_bunch.data, columns=iris_bunch.feature_names)\n",
        "iris_df['species'] = iris_bunch.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(iris_df[iris_bunch.feature_names])\n",
        "\n",
        "iris_df[iris_bunch.feature_names] = scaler.transform(\n",
        "    iris_df[iris_bunch.feature_names])\n",
        "\n",
        "iris_df = iris_df.rename(index=str, columns={\n",
        "  'sepal length (cm)': 'sepal_length',\n",
        "  'sepal width (cm)': 'sepal_width',\n",
        "  'petal length (cm)': 'petal_length',\n",
        "  'petal width (cm)': 'petal_width'})\n",
        "\n",
        "classifier = SVC(kernel='poly', degree=3, C=2.0)\n",
        "classifier.fit(iris_df[features], iris_df[target])\n",
        "predictions = classifier.predict(iris_df[features])\n",
        "\n",
        "print(f1_score(iris_df[target], predictions, average='micro'))\n",
        "\n",
        "min_val = min(iris_df[features].min()) - 1\n",
        "max_val = max(iris_df[features].max()) + 1\n",
        "xx, yy = np.meshgrid(np.arange(min_val, max_val, .02),\n",
        "                     np.arange(min_val, max_val, .02))\n",
        "predictions = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "zz = predictions.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, zz)\n",
        "\n",
        "for species_and_marker in ((0, 'o'), (1, '^'), (2, 's')):\n",
        "  plt.scatter(\n",
        "    iris_df[iris_df[target] == species_and_marker[0]][features[0]],\n",
        "    iris_df[iris_df[target] == species_and_marker[0]][features[1]],\n",
        "    marker=species_and_marker[1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApupNrepX7Hy",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DJGWvH4k3krS"
      },
      "source": [
        "## Exercise 4: Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jBGddbZvvRT7"
      },
      "source": [
        "Use the [LinearSVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html) to predict Boston housing prices in the [Boston housing dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html). Hold out some test data and print your final RMSE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OTERkp24O63j"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kH72ajYw3pb9",
        "colab": {}
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9JbV1mXZaYP",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-4-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-g5PiWr2PBZL",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVR\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "boston_bunch = load_boston()\n",
        "\n",
        "boston_df = pd.DataFrame(boston_bunch.data, columns=boston_bunch.feature_names)\n",
        "boston_df['price'] = boston_bunch.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(boston_df[boston_bunch.feature_names])\n",
        "\n",
        "boston_df[boston_bunch.feature_names] = scaler.transform(\n",
        "    boston_df[boston_bunch.feature_names])\n",
        "\n",
        "model = LinearSVR()\n",
        "model.fit(boston_df[boston_bunch.feature_names], boston_df['price'])\n",
        "predictions = model.predict(boston_df[boston_bunch.feature_names])\n",
        "\n",
        "math.sqrt(mean_squared_error(boston_df['price'], predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAjQSf3mZeHH",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    }
  ]
}
