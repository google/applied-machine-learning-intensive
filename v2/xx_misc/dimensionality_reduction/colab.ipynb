{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dimensionality Reduction",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "copyright",
        "exercise-1-key-1",
        "exercise-2-key-1",
        "exercise-3-key-1",
        "exercise-4-key-1",
        "exercise-5-key-1",
        "exercise-6-key-1",
        "exercise-7-key-1",
        "exercise-8-key-1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2019 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7PLP9Q30PKtv",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "evBbn0N5ec2H"
      },
      "source": [
        "# Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f5W9rkuBmBu9"
      },
      "source": [
        "Principal Components Analysis (PCA) is one of the most common ways to perform dimensionality reduction. PCA creates a representation of a variable, or group of variables that explain the most variance in a set of independent, and dependent variables. In a regression or classification problem, that would mean reducing the number of variables or features to the most important aggregate components, and perhaps discarding those which add little value to our model's predictive power. This is known as feature extraction, and can help simplify your model.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nRNG-gImKwQL"
      },
      "source": [
        "### Load Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SfuP5J47Kyfm",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "397CUO9TdKbk",
        "colab_type": "text"
      },
      "source": [
        "## Why PCA?\n",
        "\n",
        "The curse of dimensionality states that analyzing data with high dimensionality can lead to overly complex models that are inefficient, that suffer from overfitting, and that tend to have less predictive power.  In machine learning, this often means that your feature space is too large.  Maybe there are more features than columns of data, or perhaps your data is too sparse to draw any statistically significant inferences. PCA simplifies the feature set into a set of \"principal components\", which are linear combinations of the original features, and between them have low correlation. PCA may be undesirable in the case where you want your model to be interpretable using your original features, and not the principal components.\n",
        "\n",
        "As a rule of thumb, if your optimal number of components is greater than or equal to your original feature count, you probably shouldn't use PCA.  It is all about finding the optimal component count, where the components explain the most variance in your model; in other words, choosing the best feature for your model.\n",
        "\n",
        "PCA and other techniques for dimensionality reduction also help to visualize and analyze higher dimensional data either in 2D or 3D.  Sometimes PCA is referred to as Singular Value Decomposition (SVD), but we will call it PCA for now.  \n",
        "\n",
        "If you'd like to take a deep dive into the math (and there is quite a bit of math!), read [this](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G8u2lYRWbE37"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "PCA works best when features are normally distributed, and have low [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity). Because PCA is performing rotations in N-dimensional space, we typically need to standardize our data. Essentially, we are reducing the space that our data occupies in higher dimensions by standardizing the distribution, or scaling the range of values down to $[0, 1]$. Each method of scaling has its own data requirements, and there are several flavors of scaling and standardization, so a thorough data analysis of your features should be conducted first in order to make this assessment.\n",
        "\n",
        "You may see the terms \"scaling\", \"standardizing\" \"centering\", and \"normalizing\" used interchangeably, and this can be confusing, so let's break down these terms.\n",
        "\n",
        "1. Scaling: Does not affect the distribution, it just changes the range.\n",
        "\n",
        "2. Standardize: Changes the distribution by calculating the standard normal score.\n",
        "\n",
        "3. Centering: Shifts the distribution so the mean is zero.\n",
        "\n",
        "4. Normalizing: Typically normalizes the rows of your dataset.\n",
        "\n",
        "When using PCA to build a predictive model, we typically want to standardize the data with standard scalers.  But some cases like cluster analysis, or NLP may require normalization of rows, not columns.  There also may be other cases outside of PCA where you will need to scale or standardize.\n",
        "\n",
        "[Here is a helpful guide](https://docs.google.com/spreadsheets/d/1woVi7wq13628HJ-tN6ApaRGVZ85OdmHsDBKLAf5ylaQ/edit?usp=sharing) for choosing which method is right for your data.\n",
        "\n",
        "You can always check out the documentation for the implementation library, and you can view the source code for standard scaler [sklearn.preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCoiu8fD_ZOO",
        "colab_type": "text"
      },
      "source": [
        "## Download Wine Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RswSwDKs2bi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_wine = pd.read_csv(\n",
        "    'http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',\n",
        "    header=None)\n",
        "display(df_wine.head())\n",
        "display(df_wine.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHDmQfOdeAXa",
        "colab_type": "text"
      },
      "source": [
        "## Split into training and test data, and standardize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsFz2xjWDPoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split into training and testing sets.\n",
        "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=0)\n",
        "\n",
        "# Standardize the features.\n",
        "sc = StandardScaler()\n",
        "X_train_std = sc.fit_transform(X_train)\n",
        "X_test_std = sc.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mve0lG1wHT4K",
        "colab_type": "text"
      },
      "source": [
        "## Covariance\n",
        "\n",
        "The [covariance](https://en.wikipedia.org/wiki/Covariance) of features $M$ and $N$ is defined as follows:\n",
        "\n",
        "$$ \\sigma_{MN}^2 = \\frac{1}{n}\\sum_i {(m_i-\\mu_M)^2(n_i-\\mu_N)^2}$$\n",
        "\n",
        "- $\\mu_M$ is the sample mean of feature $M$\n",
        "- $\\mu_N$ is the sample mean of feature $N$.\n",
        "\n",
        "Covariance is an extension of variance; it is an indication of variability within a set of two features, just as variance is an indicator of variability within a feature. Don't worry too much about the math here, the implementation of PCA hides the details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPxFz0pldSAn",
        "colab_type": "text"
      },
      "source": [
        "## Eigenvectors and values\n",
        "\n",
        "Eigenvectors represent the directional vectors that we search for in the N-dimensional space.  Eigenvalues represent the length of these vectors, and inform us of how much variance is explained by the Nth principal component.  An eigenvalue of 1 means there is no more information gained beyond the original feature, so it is desirable to have principal components with values greater than 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm28K3vZCaGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cov_mat = np.cov(X_train_std.T)\n",
        "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrgSvW1tBchy",
        "colab_type": "text"
      },
      "source": [
        "Again, don't worry too much about how eigenvalues and eigenvectors are calculated; most of it is under the hood in sklearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyy4IAtj--AM",
        "colab_type": "text"
      },
      "source": [
        "## How does PCA work?\n",
        "\n",
        "We use the covariance defined above to search for a first component (a vector that minimizes the error, or distance from that vector and the data).  This process iterates until a `n_components`, or number of vectors to build n principal components is found.  In sklearn, you can choose a number of components to solve for, or let sklearn automatically choose the optimal number of components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgREpxM9FOTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate cumulative sum of explained variances.\n",
        "tot = sum(eigen_vals)\n",
        "var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
        "cum_var_exp = np.cumsum(var_exp)\n",
        "\n",
        "ax, fig = plt.subplots(1,1,figsize=(8, 4))\n",
        "# Plot explained variances.\n",
        "plt.bar(range(1, 14), var_exp, alpha=0.5, align='center',\n",
        "        label='individual explained variance')\n",
        "plt.step(range(1, 14), cum_var_exp, where='mid',\n",
        "         label='cumulative explained variance')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal component index')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YANCg7rLGWs3",
        "colab_type": "text"
      },
      "source": [
        "Using PCA, we can see the explained variance of each component.  The most variance is explained by the first principal component and drops off around 4 PCs.  We can also see that the cumulative explained variance hits approximately 90% with 8 PCs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCXvnof_dkFW",
        "colab_type": "text"
      },
      "source": [
        "## PCA for feature extraction\n",
        "\n",
        "PCA is just one form of dimensionality reduction, and you will come across others related, as well as other types of dataset transformations. Transforming your dataset is a key technique in model-building, so don't get too attached to your original dataset.\n",
        "\n",
        "By sorting the eigenpairs (vectors and their values) we can project that data into a lower dimensional space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arren9U0hQzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a list of (eigenvalue, eigenvector) tuples.\n",
        "eigen_pairs = [(np.abs(eigen_vals[i]),\n",
        "                eigen_vecs[:, i]) for i in range(len(eigen_vals))]\n",
        "\n",
        "# Sort the (eigenvalue, eigenvector) tuples from high to low.\n",
        "eigen_pairs.sort(key=lambda k: k[0], reverse=True)\n",
        "\n",
        "w = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
        "               eigen_pairs[1][1][:, np.newaxis]))\n",
        "print('Matrix W:\\n', w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MGs2_GLhjjA",
        "colab_type": "text"
      },
      "source": [
        "The result is a projection matrix that is 13x2 created from the top-2 eigenvectors.  We can now use this projection matrix, $W$, to map any sample, $x$ to its 2-dimensional sample vector $x'$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwu8kH47iS42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Project training data onto PC1 and PC2.\n",
        "X_train_pca = X_train_std.dot(w)\n",
        "\n",
        "# Visualize projection.\n",
        "colors = ['r', 'b', 'g']\n",
        "markers = ['s', 'x', 'o']\n",
        "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
        "    plt.scatter(X_train_pca[y_train==l, 0], \n",
        "                X_train_pca[y_train==l, 1], \n",
        "                c=c, label=l, marker=m) \n",
        "plt.xlabel('PC 1')\n",
        "plt.ylabel('PC 2')\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytNtxPhemo7O",
        "colab_type": "text"
      },
      "source": [
        "That is how you can implement PCA from scratch using a covariance matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f858dJUQi84w",
        "colab_type": "text"
      },
      "source": [
        "## Using PCA with Sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmIPKsW5mQ9X",
        "colab_type": "text"
      },
      "source": [
        "We can now use sklearn to implement PCA, and understand all of the explained variance per component.  If we choose `n_components` to be `None`, then we will get a number of components equal to the number of features in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziKmquQnlHjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=None)\n",
        "X_train_pca = pca.fit_transform(X_train_std)\n",
        "pca.explained_variance_ratio_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oba5UZy_nM8o",
        "colab_type": "text"
      },
      "source": [
        "Now we can use our PCA in a logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_rPiqcUi7On",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize pca and logistic regression model.\n",
        "pca = PCA(n_components=2)\n",
        "lr = LogisticRegression(multi_class='auto', solver='liblinear', random_state=0)\n",
        "\n",
        "# Fit and transform data.\n",
        "X_train_pca = pca.fit_transform(X_train_std)\n",
        "X_test_pca = pca.transform(X_test_std)\n",
        "lr.fit(X_train_pca, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxB0C2I2kKCL",
        "colab_type": "text"
      },
      "source": [
        "This can be visualized using plot decision regions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8Usks4VkrVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
        "    # Setup marker generator and color map.\n",
        "    markers = ('s', 'x', 'o', '^', 'v')\n",
        "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    # Plot the decision surface.\n",
        "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                           np.arange(x2_min, x2_max, resolution))\n",
        "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "    Z = Z.reshape(xx1.shape)\n",
        "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    # Plot class samples.\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x=X[y == cl, 0], \n",
        "                    y=X[y == cl, 1],\n",
        "                    alpha=0.6, \n",
        "                    c=[cmap(idx)],\n",
        "                    edgecolor='black',\n",
        "                    marker=markers[idx], \n",
        "                    label=cl) # Plot decision regions for training se.\n",
        "\n",
        "plot_decision_regions(X_train_pca, y_train, classifier=lr)\n",
        "plt.xlabel('PC 1')\n",
        "plt.ylabel('PC 2')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgwQuFy6k27f",
        "colab_type": "text"
      },
      "source": [
        "Now, let's plot the decision regions of the classifier, and see if the classes are separable by eye."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdugzEp9lFXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot decision regions for test set.\n",
        "plot_decision_regions(X_test_pca, y_test, classifier=lr)\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5WFLU7hWbO8_"
      },
      "source": [
        "## Resources\n",
        "\n",
        "- Examples adapted from \n",
        "[TDS](https://towardsdatascience.com/principal-component-analysis-for-dimensionality-reduction-115a3d157bad)\n",
        "- [Tutorial](https://www.ics.forth.gr/mobile/pca.pdf)\n",
        "- [Math](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf)\n",
        "- [Feature Selection](http://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Swt2fxm-fG_B"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvu2un49oASj",
        "colab_type": "text"
      },
      "source": [
        "Watch this video from [Siraj Raval](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A?feature=embeds_subscribe_title) in class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFc86L_II1nX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import IFrame\n",
        "\n",
        "IFrame(src=\"https://www.youtube.com/embed/jPmV3j1dAv4\", width=\"560\",\n",
        "       height=\"315\", frameborder=\"0\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
