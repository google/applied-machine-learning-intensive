{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Activation Functions",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "copyright",
        "Swt2fxm-fG_B",
        "iWq38ASlb2aY",
        "CYZEXNK1VDIJ",
        "exercise-1-key-1"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/v2/xx_misc/activation_functions/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7PLP9Q30PKtv",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f5W9rkuBmBu9"
      },
      "source": [
        "# Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zIykBQbYXrXA"
      },
      "source": [
        "Activation functions are core components of neural networks. These functions are used in every node of network to reduce a vector of inputs into an output value.\n",
        "\n",
        "Learning when to apply specific activation functions is a critical skill for any buiding deep learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G8u2lYRWbE37"
      },
      "source": [
        "## What is an activation function?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhT2IYeaX7H8",
        "colab_type": "text"
      },
      "source": [
        "Picture yourself as a node in a neural network. On one side of you there are multiple input streams passing data from the prior layer. On the other side there are multiple output streams that we use to pass data to every node in the next layer.\n",
        "\n",
        "We expect the data from our input layer to contain many different values since we are getting data from different nodes. On the output-side we'll give everh node in the next layer the same value. Distilling the multiple diverse inputs into a single value that we can hand to the next layer is the job of an activation function.\n",
        "\n",
        "In mathmatical terms it looks something like this:\n",
        "\n",
        "$$a = activation(\\sum_{i=0}^{n}{x_i} + bias)$$\n",
        "\n",
        "We sum our inputs from prior nodes, $x$, and our bias. We then pass that summation through an activation function in order to get our output value, $y$, that we then pass to every node in the next layer of the network.\n",
        "\n",
        "Though activation functions are used in every layer of a network, it is particularly important to understand how they behave at the output layer of a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxY-NIdqaw-a",
        "colab_type": "text"
      },
      "source": [
        "## Pass-through Activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9QLdlAEa8bh",
        "colab_type": "text"
      },
      "source": [
        "The most basic activation function is the [linear](https://www.tensorflow.org/api_docs/python/tf/keras/activations/linear) activation function. This function take the sum of inputs and bias, doest nothing to it, and hands the result to the next layer of the network.\n",
        "\n",
        "Let's plot the linear activation function in the code block below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkNEfo1RbURY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def linear(x):\n",
        "  return x\n",
        "\n",
        "inputs = np.linspace(-10, 10, 10)\n",
        "outputs = [linear(x) for x in inputs]\n",
        "_ = plt.plot(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfQC6wHWb72g",
        "colab_type": "text"
      },
      "source": [
        "That's a pretty simple activation function to understand. But what value does it provide?\n",
        "\n",
        "This function can be useful, especially in your output layer, if you want your model to product large or negative values. Many of the activation functions that we'll see greatly restrict the range of values that they output. The linear activation function does restrict it's output range at all. Any real number can be produced by a node with this activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov3n-R3HdgGZ",
        "colab_type": "text"
      },
      "source": [
        "## Rectified Linear Units (ReLU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLysbFqRdlWZ",
        "colab_type": "text"
      },
      "source": [
        "There is another linear activation function that turns out to be quite useful, the [Rectified Lienar Unit (ReLU)](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu).\n",
        "\n",
        "ReLU simply returns the input value unless that value is less than zero. In that case it returns zero.\n",
        "\n",
        "$$a = \\begin{cases}\n",
        "x \\ , &x \\geq 0 \\\\\n",
        "0 \\ , &x < 0 \\\\\n",
        "\\end{cases}$$\n",
        "\n",
        "Let's take a look at ReLU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8-miZ45dp1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def relu(x):\n",
        "  if x < 0:\n",
        "    return 0\n",
        "  return x\n",
        "\n",
        "inputs = np.linspace(-10, 10, 100, .1)\n",
        "outputs = [relu(x) for x in inputs]\n",
        "_ = plt.plot(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da8UGSJoeyWR",
        "colab_type": "text"
      },
      "source": [
        "This is also a quite simple activation, but it turns out to be quite useful in practice. Many powerful neural networks utilize ReLU activation, at least in part. It has the advantage of making training very fast; however, nodes using ReLU do run the risk of \"dying\" during the training process. The nodes die when they get to a state were they always produce a zero output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaD3-xNhgGl1",
        "colab_type": "text"
      },
      "source": [
        "Let's also think about the use of a ReLU node in a network. If the output layer consists of ReLU values, then the output of the network will be from `0` to infinity.\n",
        "\n",
        "This works fine for models that are predicting positive values, but what if your model is predicting celsius temperatures in Antartica or some other potentially negative value?\n",
        "\n",
        "In this case you would need to adjust the target training data to all be positive, say by adding `100` to it, and then do the reverse to the output of the model, subtract `100` from each value.\n",
        "\n",
        "You'll find that you'll need to do this type of adjustment quite often when building models. Understanding your activation functions, espeically in your output layer, is critically important. When you know the range of values that your model can produce you can adjust your training data to fall within that range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXuakqxriZJQ",
        "colab_type": "text"
      },
      "source": [
        "## Leaky ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HW8fiPSibHV",
        "colab_type": "text"
      },
      "source": [
        "We talked about dead nodes when discussing the ReLU activation function. One strategy that helps mitigate the dead node issue is a \"leaky\" ReLU.  Leaky ReLUs are ReLU functions that pass through any value zero or greater. For values less than zero they apply an alpha value to them and return the result.\n",
        "\n",
        "$$a = \\begin{cases}\n",
        "x \\ , &x \\geq 0 \\\\\n",
        "x * \\alpha \\ , &x < 0 \\\\\n",
        "\\end{cases}$$\n",
        "\n",
        "TensorFlow Keras doen't make a distinction between ReLU and Leaky ReLU, it simply provides an alpha parameter to [relu](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa3RVEFkjq_X",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 1: Leaky ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "recLkGjzjtD2",
        "colab_type": "text"
      },
      "source": [
        "Write a `leaky_relu` function that passes through any value zero or greater and applys an alpha of `0.1` to values less than zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaPaIGF2j6Zf",
        "colab_type": "text"
      },
      "source": [
        "**Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kclhS0QjeOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def leaky_relu(x):\n",
        "  pass # Your code goes here\n",
        "\n",
        "inputs = np.linspace(-10, 10, 100, .1)\n",
        "outputs = [leaky_relu(x) for x in inputs]\n",
        "_ = plt.plot(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ6CXhn_kIBn",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtjzVKkukErP",
        "colab_type": "text"
      },
      "source": [
        "#### Answer Key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMACAKr6kHIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def leaky_relu(x):\n",
        "  if x < 0:\n",
        "    return x * 0.1\n",
        "  return x\n",
        "\n",
        "inputs = np.linspace(-10, 10, 100, .1)\n",
        "outputs = [leaky_relu(x) for x in inputs]\n",
        "_ = plt.plot(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwHHyjk4kAyQ",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UCbhUpukY1K",
        "colab_type": "text"
      },
      "source": [
        "## Binary Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZpDhyYmkbq-",
        "colab_type": "text"
      },
      "source": [
        "The binary step activation function serves as an on/off switch for a node. This function returns zero if it's input is on one side of a threshold and one if it is on the other.\n",
        "\n",
        "$$a = \\begin{cases}\n",
        "1 \\ , &x \\geq 0 \\\\\n",
        "0 \\ , &x < 0 \\\\\n",
        "\\end{cases}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfHXM-Sjmk_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def binary_step(x):\n",
        "  if x < 0:\n",
        "    return 0\n",
        "  return 1\n",
        "\n",
        "inputs = np.linspace(-10, 10, 100, .1)\n",
        "outputs = [binary_step(x) for x in inputs]\n",
        "_ = plt.plot(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rk7qA21oVG0",
        "colab_type": "text"
      },
      "source": [
        "At the output layer this function can be useful when you need to make a yes/no decision and don't care about the confidene of the model in that decision. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOw5sUIfpmmI",
        "colab_type": "text"
      },
      "source": [
        "## Sigmoid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39hbHBRKppPE",
        "colab_type": "text"
      },
      "source": [
        "Activation functions can also be non-linear. The [sigmoid](https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid) function works using a logistic curve.\n",
        "\n",
        "$$a=\\frac{1}{1+e^{-x}}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ7bDksKp8Zl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "inputs = np.linspace(-10, 10, 100, .1)\n",
        "outputs = [sigmoid(x) for x in inputs]\n",
        "_ = plt.plot(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJohf_AlqRLp",
        "colab_type": "text"
      },
      "source": [
        "You'll notice that the sigmoid function restricts it's output range to $(0.0, 1.0)$. This is typically not a concern in hidden layers, but needs to be considered in the output layer. You'll likely need to scale your training targets down to this range and expand your predictions back to your actual data range.\n",
        "\n",
        "Sigmoids in the output layer can be very useful for predicting continuous values. They can also be useful we making binary classification decisions. You can build a model that outputs values from $(0.0, 1.0)$ and treat the output as a confidence in a decision where values closer to `0.0` show no confidence and  values closer to `1.0` show extreme confidence. You then experiment and set a threshold where you make your binary decision.\n",
        "\n",
        "For example, if you were making a classifier to determine if an image contained a cat you might find that any time the model returned a value over `0.85` there was typically a cat in the image. Before making this decision you'd need to experiment, find the precision and recall for different thresholds, and choose the one that fit your use case the best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gR1zW4Tt_59",
        "colab_type": "text"
      },
      "source": [
        "## Hyperbolic Tangent (tanh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzZpEmsippKH",
        "colab_type": "text"
      },
      "source": [
        "Similar to sigmoid, the hyperbolic tanget, [tanh](https://www.tensorflow.org/api_docs/python/tf/keras/activations/tanh) is a non-linear activation function that can be used in your models. The biggest difference between sigmod and tanh is that tanh has an output range of $(-1.0, 1.0)$\n",
        "\n",
        "$$a=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMtF5swxu3pM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def tanh(x):\n",
        "  return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "\n",
        "inputs = np.linspace(-10, 10, 100, .1)\n",
        "outputs = [tanh(x) for x in inputs]\n",
        "_ = plt.plot(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Vi3O347bIWz"
      },
      "source": [
        "The tanh function is generally useful in hidden layers and can be especially useful in output layers where you need to produce negative numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLKmhdd5vvk-",
        "colab_type": "text"
      },
      "source": [
        "## Softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-ZpEmI2wBvI",
        "colab_type": "text"
      },
      "source": [
        "So far all of the activation functions that we have seen operate without knowing anything about other nodes in their layer. Each node accepts input from the layer before it and passes output to the next layer in the model. The node is unaware of any other node in it's own layer and activation functions on the nodes work independently.\n",
        "\n",
        "[Softmax](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax) is a different type of activation function. Softmax is aware of nodes in the same layer and adjusts their outputs in relation to each other.\n",
        "\n",
        "Softmax outputs values in the range of $[0.0, 1.0]$. If you were to sum the outputs of every node in a layer, the sum would always equal `1.0`, or something very very close to `1.0`.\n",
        "\n",
        "Let's say that we had a model that tried to determine if an image contained an apple, orange, or grapefruit. If given a picture of a bright red apple, it might output `[1.0, 0.0, 0.0]` to show that it was highly confident that the image contained an apply. If given a picture of a yellow apply it might be a little less confident and output `[0.8, 0.15, 0.05]`, indicating a little less confidence. If given a picture of a large orage it might output `[0.05, 0.55, 0.4]`, showing that it was having a tough time making a decision.\n",
        "\n",
        "It is worth noting that softmax is typically not used in hidden layers of a model. Most of the time you will see it used on the output layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CYZEXNK1VDIJ"
      },
      "source": [
        "## Exercise 2: Which Activation Function?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE4hZMXqz8hv",
        "colab_type": "text"
      },
      "source": [
        "In this exercise we will describe a model that we are building and you will answer with the best activation function to use in the output layer and why. Be sure to talk about what your output data represents and how it will be interpreted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmACNxgV0xhN",
        "colab_type": "text"
      },
      "source": [
        "1. We are building a model that predicts the stock price for a stock. Which activation function should we use in the output layer and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXsKVb_u0ZkH",
        "colab_type": "text"
      },
      "source": [
        "> *Your answer goes here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiH_-6uz00WX",
        "colab_type": "text"
      },
      "source": [
        "2. We are building a model that classifies a lung scan image as having pneumonia or not. Which activation function should we use in the output layer and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usK5HzJe1PXb",
        "colab_type": "text"
      },
      "source": [
        "> *Your answer goes here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxhefN5D1U7p",
        "colab_type": "text"
      },
      "source": [
        "3. We are building a model that determines if an image of a number is 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9. Which activation function should we use in the output layer and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqmOXCTe1jZB",
        "colab_type": "text"
      },
      "source": [
        "> *Your answer goes here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX5GCEVy1l_Q",
        "colab_type": "text"
      },
      "source": [
        "4. We are building a model that predicts the daily change in temperature at a location. Which activation function should we use in the output layer and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHN-Y5yN2Oqm",
        "colab_type": "text"
      },
      "source": [
        "> *Your answer goes here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsKsUW7t3IIK",
        "colab_type": "text"
      },
      "source": [
        "5. We are building a model that attempts to predict which single Unicode character is depected in an image. Which activation function should we use in the output layer and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMcVaflf3VC_",
        "colab_type": "text"
      },
      "source": [
        "> *Your answer goes here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uby3SnIl2R9U",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-1-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etpWQdNT2Vvb",
        "colab_type": "text"
      },
      "source": [
        "1. In this case a pass-through, ReLU, or Leaky ReLU would all be candidates. The ReLU and Leaky ReLU would be better choices due to their emperical performance in other models.\n",
        "2. The Binary Step or Sigmoid would both be reasonable candidates. The Sigmoid is likely more preferable so that we get more visiblity into the confidence of the predictions. Though softmax could be used, it is typically only necessary when there are three or more possible cases.\n",
        "3. Softmax is likely the best candidate in this case since we are trying to classify between ten distinct cases. It would be possible to use another function, say sigmoid, and multiply the output by 10 and truncate to the nearest integer, but that is more creative of a model than typically seen in practice.\n",
        "4. The best activation function in this case is the Hyperbolic Tangent since we need to predict positive and negative values. It could be argued that pass-through or even leaky-relu might be okay, but Hyperbolic Tanget probably is the better choice. Since pass-through is just a linear regression, it isn't the best candidate.\n",
        "5. This is a bit of a tricky question because it really matters on how we want to interpret the output. Softmax is definitely not a good choice because there are too many Unicode characters and the layer would be too wide. If we set up the output to be the Unicode code point ReLU or Leaky ReLU might work with the right amount of output post-processing since codepoints are positive numbers. If we set up the output so that each node is a bit in some specific encoding, say UTF-32, then we could use Binary Step or Sigmoid and classify each bit as on or off."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSKt9pn62VK9",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    }
  ]
}