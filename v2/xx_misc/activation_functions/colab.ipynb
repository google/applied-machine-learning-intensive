{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Activation Functions",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "copyright",
        "Swt2fxm-fG_B",
        "iWq38ASlb2aY",
        "CYZEXNK1VDIJ",
        "exercise-1-key-1"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/v2/xx_misc/activation_functions/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7PLP9Q30PKtv",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f5W9rkuBmBu9"
      },
      "source": [
        "# Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zIykBQbYXrXA"
      },
      "source": [
        "Activation functions are core components of neural networks. These functions are used in every node of network to reduce a vector of inputs into an output value.\n",
        "\n",
        "Learning when to apply specific activation functions is a critical skill for any buiding deep learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G8u2lYRWbE37"
      },
      "source": [
        "## What is an activation function?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhT2IYeaX7H8",
        "colab_type": "text"
      },
      "source": [
        "Picture yourself as a node in a neural network. On one side of you there are multiple input streams passing data from the prior layer. On the other side there are multiple output streams that we use to pass data to every node in the next layer.\n",
        "\n",
        "We expect the data from our input layer to contain many different values since we are getting data from different nodes. On the output-side we'll give everh node in the next layer the same value. Distilling the multiple diverse inputs into a single value that we can hand to the next layer is the job of an activation function.\n",
        "\n",
        "In mathmatical terms it looks something like this:\n",
        "\n",
        "> $y = activation(\\sum_{i=0}^{n}{x_i} + bias)$\n",
        "\n",
        "We sum our inputs from prior nodes, $x$, and our bias. We then pass that summation through an activation function in order to get our output value, $y$, that we then pass to every node in the next layer of the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxY-NIdqaw-a",
        "colab_type": "text"
      },
      "source": [
        "## Pass-through Activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9QLdlAEa8bh",
        "colab_type": "text"
      },
      "source": [
        "The most basic activation function is the [linear](https://www.tensorflow.org/api_docs/python/tf/keras/activations/linear) activation function. This function take the sum of inputs and bias, doest nothing to it, and hands the result to the next layer of the network.\n",
        "\n",
        "Let's plot the linear activation function in the code block below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkNEfo1RbURY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def linear(x):\n",
        "  return x\n",
        "\n",
        "inputs = np.linspace(-10, 10, 10)\n",
        "outputs = [linear(x) for x in inputs]\n",
        "_ = plt.plot(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfQC6wHWb72g",
        "colab_type": "text"
      },
      "source": [
        "That's a pretty simple activation function to understand. But what value does it provide?\n",
        "\n",
        "This function can be useful, especially in your output layer, if you want your model to product large or negative values. Many of the activation functions that we'll see greatly restrict the range of values that they output. The linear activation function does restrict it's output range at all. Any real number can be produced by a node with this activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov3n-R3HdgGZ",
        "colab_type": "text"
      },
      "source": [
        "## Rectified Linear Units (ReLU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLysbFqRdlWZ",
        "colab_type": "text"
      },
      "source": [
        "There is another linear activation function that turns out to be quite useful, the [Rectified Lienar Unit (ReLU)](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu).\n",
        "\n",
        "ReLU simply returns the input value unless that value is less than zero. In that case it returns zero.\n",
        "\n",
        "$$a = \\begin{cases}\n",
        "x \\ , &x \\geq 0 \\\\\n",
        "0 \\ , &x < 0 \\\\\n",
        "\\end{cases}$$\n",
        "\n",
        "Let's take a look at ReLU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8-miZ45dp1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def relu(x):\n",
        "  if x < 0:\n",
        "    return 0\n",
        "  return x\n",
        "\n",
        "inputs = np.linspace(-10, 10, 100, .1)\n",
        "outputs = [relu(x) for x in inputs]\n",
        "_ = plt.plot(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da8UGSJoeyWR",
        "colab_type": "text"
      },
      "source": [
        "This is also a quite simple activation, but it turns out to be quite useful in practice. Many powerful neural networks utilize ReLU activation, at least in part. It has the advantage of making training very fast; however, nodes using ReLU do run the risk of \"dying\" during the training process. The nodes die when they get to a state were they always produce a zero output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaD3-xNhgGl1",
        "colab_type": "text"
      },
      "source": [
        "Let's also think about the use of a ReLU node in a network. If the output layer consists of ReLU values, then the output of the network will be from `0` to infinity.\n",
        "\n",
        "This works fine for models that are predicting positive values, but what if your model is predicting celsius temperatures in Antartica or some other potentially negative value?\n",
        "\n",
        "In this case you would need to adjust the target training data to all be positive, say by adding `100` to it, and then do the reverse to the output of the model, subtract `100` from each value.\n",
        "\n",
        "You'll find that you'll need to do this type of adjustment quite often when building models. Understanding your activation functions, espeically in your output layer, is critically important. When you know the range of values that your model can produce you can adjust your training data to fall within that range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Vi3O347bIWz"
      },
      "source": [
        "<img src=\"https://i.imgur.com/0CIHbg7.png\" width=\"250\">\n",
        "\n",
        "Given some input $x$ either from previous layers, or some input data, an activation takes the input and returns a decision to activate.  In the diagram above:\n",
        "\n",
        "- inputs $x_1,x_2,x_3$ are passed to a hidden layer\n",
        "- activation function $a_1$ outputs to the final layer\n",
        "- activation function $a_2$ returns a value $\\hat{y}$, a prediction\n",
        "\n",
        "There are many types of activation functions, each with their own strengths in both effectiveness and computational cost. In practice, a sigmoid activation might be used in a for a binary classifier, while a [softmax](https://en.wikipedia.org/wiki/Softmax_function) would be used for a multiclass problem, and ReLu is commonly used in image classification with convolutional neural net (CNN) architecture.\n",
        "\n",
        "Many activations are derivatives of the following three functions:\n",
        "\n",
        "- Sigmoid (logistic function)\n",
        "$$a=\\frac{1}{1+e^{-x}}$$\n",
        "\n",
        "- Tanh (hyperbolic tangent)\n",
        "$$a=tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$$\n",
        "\n",
        "- ReLU (Rectified Linear Unit)\n",
        "$$a = \\begin{cases}\n",
        "x \\ , &x \\geq 0 \\\\\n",
        "0 \\ , &x < 0 \\\\\n",
        "\\end{cases}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iWq38ASlb2aX"
      },
      "source": [
        "For each activation function listed above (sigmoid, tanh, ReLu), let's implement the function on some data and plot the results to visualize the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2gXzsn0kyF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEQBKaloRyDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sigmoid\n",
        "\n",
        "def sigmoid(x):\n",
        "  return [1 / (1 + np.exp(-item)) for item in x]\n",
        "   \n",
        "# Create sample data\n",
        "x = np.arange(-10., 10., 0.2)\n",
        "sig = sigmoid(x)\n",
        "fig, ax = plt.subplots(1,1)\n",
        "plt.plot(x,sig, marker = \"o\")\n",
        "\n",
        "plt.axhline(0, c='k')\n",
        "plt.axvline(0, c='k')\n",
        "\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "plt.xlabel(\"X\") \n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Sigmoid Function\")\n",
        "plt.text(3, 0.8, r'$a=\\frac{1}{1+e^{-x}}$', fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pjfr55R0UHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tanh\n",
        "\n",
        "in_array = np.linspace(-np.pi, np.pi, 12) \n",
        "out_array = np.tanh(in_array) \n",
        "\n",
        "plt.plot(in_array, out_array, color = 'red', marker = \"o\") \n",
        "plt.title('Tanh Function') \n",
        "plt.xlabel(\"X\") \n",
        "plt.ylabel(\"Y\")\n",
        "plt.axhline(0, c='k')\n",
        "plt.axvline(0, c='k')\n",
        "plt.text(.2, -.5, r'$a=tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$', fontsize=16)\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKiatFhq0Rbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ReLU\n",
        "\n",
        "def ReLU(x):\n",
        "  return np.maximum(0.0, x)\n",
        "\n",
        "X = np.linspace(-5, 5, 100)\n",
        "plt.plot(X, ReLU(X),'b', marker = \"o\")\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('ReLU Function')\n",
        "plt.axvline(0, c='k')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B8ISQldzFQO",
        "colab_type": "text"
      },
      "source": [
        "The \"leaky ReLU\" function is a derivative of the ReLU function, and it commonly used in neural nets.\n",
        "\n",
        "$$a = \\begin{cases}\n",
        "x \\ , &x \\geq 0 \\\\\n",
        "0.01x \\ , &x < 0 \\\\\n",
        "\\end{cases}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi7zt8YjztxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Leaky ReLU\n",
        "\n",
        "def LeakyReLU(x):\n",
        "  return x if x >= 0 else 0.01*x\n",
        "\n",
        "X = np.linspace(-5, 5, 100)\n",
        "plt.plot(X, ReLU(X),'b', marker = \"o\")\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Leaky ReLU Function')\n",
        "plt.axvline(0, c='k')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksueK0B-Wcqa",
        "colab_type": "text"
      },
      "source": [
        "## Activations Functions in Convolutional Neural Nets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn828tzCWi9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pnp_AZXVYsdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tcsdvOsYvb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape data to fit model.\n",
        "X_train = X_train.reshape(60000,28,28,1)\n",
        "X_test = X_test.reshape(10000,28,28,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC3KD7dtY1uk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "# one-hot encode target column\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEHf59IabDjV",
        "colab_type": "text"
      },
      "source": [
        "### Create the model architecture "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrRuvTutY4m-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, Conv2D, Flatten\n",
        "# Create model.\n",
        "model = Sequential()\n",
        "# Add model layers.\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIU5uveCZrg8",
        "colab_type": "text"
      },
      "source": [
        "As we can see, this model has 2 convolutional layers. Each of the Conv2D layers have ReLu activations to identify interesting parts of the digits image, and a final dense layer with a softmax to classify the image as one of the 9 digits. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APFJjk3IcvHZ",
        "colab_type": "text"
      },
      "source": [
        "### Visualize the model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cho51iYJcNjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
        "\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True,\n",
        "           show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSVg9K8y0LDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(filename='model_plot.png') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGpltg5KbT8I",
        "colab_type": "text"
      },
      "source": [
        "### Compile the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAWSpE-DbWl5",
        "colab_type": "text"
      },
      "source": [
        "Compiling allows you to tune the model hyperparameters, like `optimizer`, `loss` and performance `metrics`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUBtQd71a_Rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxzAoUYFfR39",
        "colab_type": "text"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB30lZ9lfbKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(X_train, y_train,\n",
        "                    batch_size=128,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CBw8pUOueYs",
        "colab_type": "text"
      },
      "source": [
        "Using CNN architecture with ReLu activation functions, we were able to train a model with ~98% accuracy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsz_ap8blRBP",
        "colab_type": "text"
      },
      "source": [
        "### Get Confusion Matrix from predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_u5wr0esneN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApZkwwH9lDaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJrjAxmnsxfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzPfoCEhs6xJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(cm, [x for x in range(10)],\n",
        "                  normalize=False,\n",
        "                  title='Confusion matrix',\n",
        "                  cmap=plt.cm.Greens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqpua61u4d4E",
        "colab_type": "text"
      },
      "source": [
        "### Show activation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zj2dugh4yVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.models import Model\n",
        "\n",
        "def display_activation(activations, col_size, row_size, act_index): \n",
        "    activation = activations[act_index]\n",
        "    activation_index=0\n",
        "    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*2.5,col_size*1.5))\n",
        "    for row in range(0,row_size):\n",
        "        for col in range(0,col_size):\n",
        "            ax[row][col].imshow(activation[0, :, :, activation_index], cmap='bone')\n",
        "            activation_index += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UpNI6w748VS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = np.random.randint(len(X_test))\n",
        "plt.imshow(X_test[n][:,:,0]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zK3i0DZ5zXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
        "activations = activation_model.predict(X_test[n].reshape(1,28,28,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMpk_OUV5aB4",
        "colab_type": "text"
      },
      "source": [
        "### First layer activation maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSD1W1yj4zdw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_activation(activations, 8, 8, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCi_JXpf5ekF",
        "colab_type": "text"
      },
      "source": [
        "### 2nd layer activation maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMN1YG3Y5h6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_activation(activations, 5, 6, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5WFLU7hWbO8_"
      },
      "source": [
        "# Resources\n",
        "\n",
        "* [Comparison of activation functions](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions)\n",
        "* [deeplearning.ai](https://www.coursera.org/lecture/neural-networks-deep-learning/activation-functions-4dDC1)\n",
        "\n",
        "* [Discriminative Localization](http://cnnlocalization.csail.mit.edu/)\n",
        "\n",
        "* [CNN with Keras](https://www.kaggle.com/amarjeet007/visualize-cnn-with-keras)\n",
        "\n",
        "* [Disadvantages of ReLu](https://www.quora.com/What-are-the-disadvantages-of-using-the-ReLu-when-using-Neural-Networks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Swt2fxm-fG_B"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJxLcFfyAh2j",
        "colab_type": "text"
      },
      "source": [
        "Implement ReLu on another image dataset that you have worked on before, or a new one, and plot the activations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CYZEXNK1VDIJ"
      },
      "source": [
        "### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgC_BJhG8Kl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your Code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-1-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-1-solution-1"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AKb-7oqrcfox",
        "colab": {}
      },
      "source": [
        "# Put the recommended solution here; if there is more than one \"good\" solution\n",
        "# that you think students should know put those solutions in subsequent code\n",
        "# boxes with \"# Solution\" in the first line."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}