---

marp: true

---

<style>
img[alt~="center"] {
  display: block;
  margin: 0 auto;
}
</style>

# Machine Learning Fairness

---

# Read & Discuss

<!--
In this session we'll focus our attention on machine learning fairness. We'll start by engaging with some examples from various domains, showing the impact of bias on end users.

*There are four potential articles in the facilitator guide. Choose your favorite three and have
students count off 1, 2, 3. Assign each group an article to read.*

*Articles:
https://seejane.org/wp-content/uploads/gdiq-reel-truth-women-arent-seen-or-heard-automated-analysis.pdf
http://www.openculture.com/2018/07/color-film-was-designed-to-take-pictures-of-white-people-not-people-of-color.html
https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing
https://www.wired.com/story/ai-text-generator-too-dangerous-to-make-public/*

*After everyone has finished reading (~10 mins), gather in mixed clusters of three (one student from each
article) to explain the main takeaway of their article to others in the newly-formed group.*

What bias was uncovered?
How would you describe its impact on people?
What do you find interesting about this article?*
-->

---

# Types of Bias

* Reporting
* Automation
* Selection (coverage, non-response, sampling)
* Group attribution (in-group, out-group)
* Implicit (confirmation, experimenters)

<!--
Biases can be found throughout the design and development of ML systems. Stay in your same groups of three. Now we'll do a
two-part activity to identify different types of bias.

*For part one, ask students to use cards to match nine descriptive examples with nine different types of bias (bias card
matching). For part two, ask students to think-pair-share or volunteer answers to the whole group: what instances of these bias types have they encountered in their own experience?*

*If you have a concrete example of bias in ML systems from your own experience, share how it was detected
and handled.*
-->

---

# Ethical Storyboarding

* What ML fairness risks exist?
* How would a “fair” model behave?
* How can we mitigate risks?
  * model
  * dataset
  * context

<!--
It’s important to remember ML algorithms and systems are built, trained, and evaluated by people, and they are affected by human cognitive limitations and biases. To create systems that work for everyone, we have to intentionally work to mitigate those issues. It’s also important to note that fairness is subjective. Not all biases should be approached the same way.

*In your same groups of three, have students work on ethical storyboarding activity for some ML example (chosen from the bias cards or other). Discuss what ML fairness risks might exist for their specific product and what research/feedback mechanisms could help mitigate those issues.*

*After ~25 minutes, debrief as a whole class.*
What do you perceive as main takeaways from this exercise?
What does this mean to you and your role as up-and-coming professionals in Machine Learning?
What questions do you still have?
-->
