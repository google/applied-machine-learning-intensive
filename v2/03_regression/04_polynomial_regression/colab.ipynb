{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Polynomial Regression and Overfitting",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "copyright",
        "exercise-1-key-1",
        "exercise-2-key-1",
        "exercise-3-key-1"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/v2/03_regression/04_polynomial_regression/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "za20dNxkuKgG",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PWoQYCYUuRhq"
      },
      "source": [
        "# Polynomial Regression and Overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mtccx8anFhLG"
      },
      "source": [
        "So far in this course, we have dealt exclusively with linear models. These have all been \"straight-line\" models where we attempt to draw a straight line that fits a regression.\n",
        "\n",
        "Today we will start building curved-lined models based on [polynomial equations](https://en.wikipedia.org/wiki/Polynomial)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PknJfIfBwzu5"
      },
      "source": [
        "## Generating Sample Data\n",
        "\n",
        "Let's start by generating some data based on a second degree polynomial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dgmmzEFXw-qi",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_items = 100\n",
        "\n",
        "np.random.seed(seed=420)\n",
        "X = np.random.randn(num_items, 1)\n",
        "\n",
        "# These coefficients are chosen arbitrarily.\n",
        "y = 0.6*(X**2) - 0.4*X + 1.3\n",
        "\n",
        "plt.plot(X, y, 'b.')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LPrjFFtr0-Ml"
      },
      "source": [
        "Let's add some randomness to create a more realistic dataset and re-plot the randomized data points and the fit line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wi5xcUqLqVyr",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_items = 100\n",
        "\n",
        "np.random.seed(seed=420)\n",
        "X = np.random.randn(num_items, 1)\n",
        "\n",
        "# Create some randomness.\n",
        "randomness = np.random.randn(num_items, 1) / 2\n",
        "\n",
        "# This is the same equation as the plot above, with added randomness.\n",
        "y = 0.6*(X**2) - 0.4*X + 1.3 + randomness\n",
        "\n",
        "X_line = np.linspace(X.min(), X.max(), num=num_items)\n",
        "y_line = 0.6*(X_line**2) - 0.4*X_line + 1.3\n",
        "\n",
        "plt.plot(X, y, 'b.')\n",
        "plt.plot(X_line, y_line, 'r-')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0_ts9GCoF4rW"
      },
      "source": [
        "That looks much better! Now we can see that a 2-degree polynomial function fits this data reasonably well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "57bc0JsX1-ly"
      },
      "source": [
        "## Polynomial Fitting\n",
        "\n",
        "We can now see a pretty obvious 2-degree polynomial that fits the scatterplot.\n",
        "\n",
        "Scikit-learn offers a `PolynomialFeatures` class that handles polynomial combinations for a linear model. In this case, we know that a 2-degree polynomial is a good fit since the data was generated from a polynomial curve. Let's see if the model works.\n",
        "\n",
        "We begin by creating a `PolynomialFeatures` instance of degree 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pYj3qjd9q8OA",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "pf = PolynomialFeatures(degree=2)\n",
        "X_poly = pf.fit_transform(X)\n",
        "\n",
        "X.shape, X_poly.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjlcA2idpxaE",
        "colab_type": "text"
      },
      "source": [
        "You might be wondering what the `include_bias` parameter is. By default, it is `True`, in which case it forces the first exponent to be 0.\n",
        "\n",
        "This adds a constant bias term to the equation. When we ask for no bias we start our exponents at 1 instead of 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hhgV7MVq3EGL"
      },
      "source": [
        "This preprocessor generates a new feature matrix consisting of all polynomial combinations of the features. Notice that the input shape of `(100, 1)` becomes `(100, 2)` after transformation.\n",
        "\n",
        "In this simple case, we doubled the number of features since we asked for a 2-degree polynomial and had one input feature. The number of generated features grows exponentially as the number of features and polynomial degrees increases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R1q9mXXXHYLr"
      },
      "source": [
        "## Model Fitting\n",
        "\n",
        "We can now fit the model by passing our polynomial preprocessing data to the linear regressor.\n",
        "\n",
        "How close did the intercept and coefficient match the values in the function we used to generate our data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xwc2Bdh4rSVQ",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_poly, y)\n",
        "\n",
        "lin_reg.intercept_, lin_reg.coef_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QyFK6PhcIBdE"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "We can plot our fitted line against the equation we used to generate the data. The fitted line is green, and the actual curve is red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eEd9rGnFr_mc",
        "colab": {}
      },
      "source": [
        "np.random.seed(seed=420)\n",
        "\n",
        "X_line_fitted = np.linspace(X.min(), X.max(), num=100)\n",
        "y_line_fitted = lin_reg.coef_[0][1] * X_line_fitted**2 + lin_reg.coef_[0][0] * X_line_fitted + lin_reg.intercept_\n",
        "\n",
        "plt.plot(X_line_fitted, y_line_fitted, 'g-')\n",
        "plt.plot(X_line, y_line, 'r-')\n",
        "plt.plot(X, y, 'b.')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pLfOr-euIJ4v"
      },
      "source": [
        "# Overfitting\n",
        "\n",
        "When using polynomial regression, it can be easy to *overfit* the data so that it performs well on the training data but doesn't perform well in the real world.\n",
        "\n",
        "To understand overfitting we will create a fake dataset generated off of a linear equation, but we will use a polynomial regression as the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q4FHYtfrwVEr",
        "colab": {}
      },
      "source": [
        "np.random.seed(seed=420)\n",
        "\n",
        "# Create 50 points from a linear dataset with randomness.\n",
        "num_items = 50\n",
        "X = 6 * np.random.rand(num_items, 1)\n",
        "y = X + 2 + np.random.randn(num_items, 1)\n",
        "\n",
        "X_line = np.array([X.min(), X.max()])\n",
        "y_line = X_line + 2\n",
        "\n",
        "plt.plot(X_line, y_line, 'r-')\n",
        "plt.plot(X, y, 'b.')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-axAp_pzxHQe"
      },
      "source": [
        "Let's now create a 10 degree polynomial to fit the linear data and fit the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4lflXXT24pRN",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "np.random.seed(seed=420)\n",
        "\n",
        "poly_features = PolynomialFeatures(degree=10, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "regression = LinearRegression()\n",
        "regression.fit(X_poly, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QPXZ46X8xXFB"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Let's draw the polynomial line that we fit to the data. To draw the line, we need to execute the 10 degree polynomial equation.\n",
        "\n",
        "$$\n",
        "y = k_0 + k_1x^1 + k_2x^2 + k_3x^3 + ... + k_9x^9 + k_{10}x^{10}\n",
        "$$\n",
        "\n",
        "Coding the above equation by hand is tedious and error-prone. It also makes it difficult to change the degree of the polynomial we are fitting.\n",
        "\n",
        "Let's see if there is a way to write the code more dynamically, using the `PolynomialFeatures` and `LinearRegression` functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y-XxBVnPzB9o"
      },
      "source": [
        "The `PolynomialFeatures` class provides us with a list of exponents that we can use for each portion of the polynomial equation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LIatzCwvy8QD",
        "colab": {}
      },
      "source": [
        "poly_features.powers_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3-GRE5mgzYmI"
      },
      "source": [
        "The `LinearRegression` class provides us with a list of coefficients that correspond to the powers provided by `PolynomialFeatures`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e-RmO7-KzSMe",
        "colab": {}
      },
      "source": [
        "regression.coef_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w1nb49h7zjeR"
      },
      "source": [
        "It also provides an intercept."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "14UXMC2nzgzO",
        "colab": {}
      },
      "source": [
        "regression.intercept_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jM1pOywgzm8d"
      },
      "source": [
        "Having this information, we can take a set of $X$ values (in the code below we use 100), then run our equation on those values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PjZem434xSZf",
        "colab": {}
      },
      "source": [
        "np.random.seed(seed=420)\n",
        "\n",
        "# Create 100 even-spaced x-values.\n",
        "X_line_fitted = np.linspace(X.min(), X.max(), num=100)\n",
        "\n",
        "# Start our equation with the intercept.\n",
        "y_line_fitted = regression.intercept_\n",
        "\n",
        "# For each exponent, raise the X value to that exponent and multiply it by the\n",
        "# appropriate coefficient\n",
        "for i in range(len(poly_features.powers_)):\n",
        "  exponent = poly_features.powers_[i][0]\n",
        "  y_line_fitted = y_line_fitted + \\\n",
        "    regression.coef_[0][i] * (X_line_fitted**exponent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H98LMnMp0JC0"
      },
      "source": [
        "We can now plot the data points, the actual line used to generate them, and our fitted model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PH8z0hQt0PQf",
        "colab": {}
      },
      "source": [
        "plt.plot(X_line, y_line, 'r-')\n",
        "plt.plot(X_line_fitted, y_line_fitted, 'g-')\n",
        "plt.plot(X, y, 'b.')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lt6mmUD80VXn"
      },
      "source": [
        "Notice how our line is very wavy, and it spikes up and down to pass through specific data points. (This is especially true for the lowest and highest $x$-values, where the curve passes through them exactly.) This is a sign of overfitting. The line fits the training data reasonably well, but it may not be as useful on new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aXaa2K9h0pCX"
      },
      "source": [
        "## Using a Simpler Model\n",
        "\n",
        "The most obvious way to prevent overfitting in this example is to simply reduce the degree of the polynomial.\n",
        "\n",
        "The code below uses a 2-degree polynomial and seems to fit the data much better. A linear model would work well too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9khOTN-y09qO",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "regression = LinearRegression()\n",
        "regression.fit(X_poly, y)\n",
        "\n",
        "X_line_fitted = np.linspace(X.min(), X.max(), num=100)\n",
        "y_line_fitted = regression.intercept_\n",
        "for i in range(len(poly_features.powers_)):\n",
        "  exponent = poly_features.powers_[i][0]\n",
        "  y_line_fitted = y_line_fitted + \\\n",
        "    regression.coef_[0][i] * (X_line_fitted**exponent)\n",
        "\n",
        "plt.plot(X_line, y_line, 'r-')\n",
        "plt.plot(X_line_fitted, y_line_fitted, 'g-')\n",
        "plt.plot(X, y, 'b.')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WqqCYcOt1iUF"
      },
      "source": [
        "## Lasso Regularization\n",
        "\n",
        "It is not always so clear what the \"simpler\" model choice is. Often, you will have to rely on regularization methods. A **regularization** is a method that penalizes large coefficients, with the aim of shrinking unnecessary coefficients to zero.\n",
        "\n",
        "Least Absolute Shrinkage and Selection Operator (Lasso) regularization, also called L1 regularization, is a regularization method that adds the sum of the absolute values of the coefficients as a penalty in a cost function.\n",
        "\n",
        "In scikit-learn, we can use the [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) model, which performs a linear regression with an L1 regression penalty.\n",
        "\n",
        "In the resultant graph, you can see that the regression smooths out our polynomial curve quite a bit despite the polynomial being a degree 10 polynomial. Note that Lasso regression can make the impact of less important features completely disappear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "16_BAxX_mAVt",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "poly_features = PolynomialFeatures(degree=10, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "lasso_reg = Lasso(alpha=5.0)\n",
        "lasso_reg.fit(X_poly, y)\n",
        "\n",
        "X_line_fitted = np.linspace(X.min(), X.max(), num=100)\n",
        "y_line_fitted = lasso_reg.intercept_\n",
        "for i in range(len(poly_features.powers_)):\n",
        "  exponent = poly_features.powers_[i][0]\n",
        "  y_line_fitted = y_line_fitted + lasso_reg.coef_[i] * (X_line_fitted**exponent)\n",
        "\n",
        "plt.plot(X_line, y_line, 'r-')\n",
        "plt.plot(X_line_fitted, y_line_fitted, 'g-')\n",
        "plt.plot(X, y, 'b.')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PV1Geq_n3N47"
      },
      "source": [
        "## Ridge Regularization\n",
        "\n",
        "Similar to Lasso regularization, [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) regularization adds a penalty to the cost function of a model. In the case of Ridge, also called L2 regularization, the penalty is the sum of squares of the coefficients.\n",
        "\n",
        "Again, we can see that the regression smooths out the curve of our 10-degree polynomial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WlS8-f-ynEEy",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "poly_features = PolynomialFeatures(degree=10, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "ridge_reg = Ridge(alpha=0.5)\n",
        "ridge_reg.fit(X_poly, y)\n",
        "\n",
        "X_line_fitted = np.linspace(X.min(), X.max(), num=100)\n",
        "y_line_fitted = ridge_reg.intercept_\n",
        "for i in range(len(poly_features.powers_)):\n",
        "  exponent = poly_features.powers_[i][0]\n",
        "  y_line_fitted = y_line_fitted + ridge_reg.coef_[0][i] * (X_line_fitted**exponent)\n",
        "\n",
        "plt.plot(X_line, y_line, 'r-')\n",
        "plt.plot(X_line_fitted, y_line_fitted, 'g-')\n",
        "plt.plot(X, y, 'b.')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vkoP4T6W4Vjl"
      },
      "source": [
        "## ElasticNet Regularization\n",
        "\n",
        "Another common form of regularization is [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) regularization. This regularization method combines the concepts of L1 and L2 regularization by applying a penalty containing both a squared value and an absolute value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "phYvL4o1oPKv",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "poly_features = PolynomialFeatures(degree=10, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "elastic_reg = ElasticNet(alpha=2.0, l1_ratio=0.5)\n",
        "elastic_reg.fit(X_poly, y)\n",
        "\n",
        "X_line_fitted = np.linspace(X.min(), X.max(), num=100)\n",
        "y_line_fitted = elastic_reg.intercept_\n",
        "for i in range(len(poly_features.powers_)):\n",
        "  exponent = poly_features.powers_[i][0]\n",
        "  y_line_fitted = y_line_fitted + \\\n",
        "    elastic_reg.coef_[i] * (X_line_fitted**exponent)\n",
        "\n",
        "plt.plot(X_line, y_line, 'r-')\n",
        "plt.plot(X_line_fitted, y_line_fitted, 'g-')\n",
        "plt.plot(X, y, 'b.')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0AfrlEyb-Dlu"
      },
      "source": [
        "## Other Strategies\n",
        "\n",
        "Aside from regularization, there are other strategies that can be used to prevent overfitting. These include:\n",
        "\n",
        "* [Early stopping](https://en.wikipedia.org/wiki/Early_stopping)\n",
        "* [Cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))\n",
        "* [Ensemble methods](https://en.wikipedia.org/wiki/Ensemble_learning)\n",
        "* Simplifying your model\n",
        "* Removing features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZztAQi3YPptV"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PsCX81qd-0v3"
      },
      "source": [
        "For these exercises we will work with the [diabetes dataset](https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset) that comes with scikit-learn. The data contains the following features:\n",
        "\n",
        "1. age\n",
        "1. sex\n",
        "1. body mass index (bmi)\n",
        "1. average blood pressure (bp)\n",
        "\n",
        "It also contains six measures of blood serum, `s1` through `s6`. The target is a numeric assessment of the progression of the disease over the course of a year.\n",
        "\n",
        "The data has been standardized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOHF9pSD0Q8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = load_diabetes()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['progression'] = data.target\n",
        "\n",
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_OwePHmc5jl",
        "colab_type": "text"
      },
      "source": [
        "Let's plot how body mass index relates to blood pressure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vfqqluBM_mVl",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(df['bmi'], df['bp'], 'b.')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PGZdi0rqClcW"
      },
      "source": [
        "## Exercise 1: Polynomial Regression "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wGvum6-lnTnL"
      },
      "source": [
        "Let's create a model to see if we can map body mass index to blood pressure.\n",
        "\n",
        "1. Create a 10-degree polynomial preprocessor for our regression\n",
        "1. Create a linear regression model\n",
        "1. Fit and transform the `bmi` values with the polynomial features preprocessor\n",
        "1. Fit the transformed data using the linear regression\n",
        "1. Plot the fitted line over a scatterplot of the data points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2at-qnRUP9yK"
      },
      "source": [
        "**Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eXs9gz-3QAH-",
        "colab": {}
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "541BajOTJyms",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-1-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vxBY7sF4Aw2u",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "data = load_diabetes()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['progression'] = data.target\n",
        "\n",
        "X = df[['bmi']]\n",
        "y = df['bp']\n",
        "\n",
        "poly_features = PolynomialFeatures(degree=10, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "regression = LinearRegression()\n",
        "regression.fit(X_poly, y)\n",
        "\n",
        "X_line_fitted = np.linspace(X.min(), X.max(), num=100)\n",
        "y_line_fitted = regression.intercept_\n",
        "\n",
        "for i in range(len(poly_features.powers_)):\n",
        "  exponent = poly_features.powers_[i][0]\n",
        "  y_line_fitted = y_line_fitted + \\\n",
        "    regression.coef_[i] * (X_line_fitted**exponent)\n",
        "\n",
        "plt.plot(X_line_fitted, y_line_fitted, 'g-')\n",
        "plt.plot(X, y, 'b.')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtkiIBrwJxU8",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1tV79QxhDP_i"
      },
      "source": [
        "## Exercise 2: Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EZyLCzEanVoD"
      },
      "source": [
        "Your model from exercise one likely looked like it overfit. Experiment with the Lasso, Ridge, and/or ElasticNet classes in the place of the `LinearRegression`. Adjust the parameters for whichever regularization class you use until you create a line that doesn't look to be under- or over-fitted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wilt9sLuQI50"
      },
      "source": [
        "**Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "owQ0vctpQP_6",
        "colab": {}
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_27ehIdZJtw8",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-2-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NGljYKcVB8AL",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "data = load_diabetes()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['progression'] = data.target\n",
        "\n",
        "X = df[['bmi']]\n",
        "y = df['bp']\n",
        "\n",
        "poly_features = PolynomialFeatures(degree=10, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "regression = Ridge()\n",
        "regression.fit(X_poly, y)\n",
        "\n",
        "X_line_fitted = np.linspace(X.min(), X.max(), num=100)\n",
        "y_line_fitted = regression.intercept_\n",
        "\n",
        "for i in range(len(poly_features.powers_)):\n",
        "  exponent = poly_features.powers_[i][0]\n",
        "  y_line_fitted = y_line_fitted + \\\n",
        "    regression.coef_[i] * (X_line_fitted**exponent)\n",
        "\n",
        "plt.plot(X_line_fitted, y_line_fitted, 'g-')\n",
        "plt.plot(X, y, 'b.')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2oH9HFNJvbv",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nso7fOpVEWJb"
      },
      "source": [
        "## Exercise 3: Other Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5jBqG09r615k"
      },
      "source": [
        "Experiment with the [BayesianRidge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html). Does its fit line look better or worse than your other models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a5DJdaaw6n7a"
      },
      "source": [
        "**Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DHhJ1nUZEuV7",
        "colab": {}
      },
      "source": [
        "# Your code goes here."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZtjFc68fiCP",
        "colab_type": "text"
      },
      "source": [
        "Does your fit line look better or worse than your other models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xquJndh1fnGT",
        "colab_type": "text"
      },
      "source": [
        "> *Your Answer Goes Here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM_uN8EcJrNy",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-3-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-3-solution-1"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BQOGNKFb6r38",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "data = load_diabetes()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['progression'] = data.target\n",
        "\n",
        "X = df[['bmi']]\n",
        "y = df['bp']\n",
        "\n",
        "poly_features = PolynomialFeatures(degree=10, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "regression = BayesianRidge()\n",
        "regression.fit(X_poly, y)\n",
        "\n",
        "X_line_fitted = np.linspace(X.min(), X.max(), num=100)\n",
        "y_line_fitted = regression.intercept_\n",
        "\n",
        "for i in range(len(poly_features.powers_)):\n",
        "  exponent = poly_features.powers_[i][0]\n",
        "  y_line_fitted = y_line_fitted + \\\n",
        "    regression.coef_[i] * (X_line_fitted**exponent)\n",
        "\n",
        "plt.plot(X_line_fitted, y_line_fitted, 'g-')\n",
        "plt.plot(X, y, 'b.')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrHF7fXtfrTg",
        "colab_type": "text"
      },
      "source": [
        "Does your fit line look better or worse than your other models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRTatCvdfuLO",
        "colab_type": "text"
      },
      "source": [
        "> The answers are somewhat open here, depending on how the students tuned the models. In our case, when we applied ridge regression, we seemed to underfit a bit compared to Bayesian modelling. However, our Bayesian model might have been a little overfit for high BMI values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrtWM0JeJqaA",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    }
  ]
}
