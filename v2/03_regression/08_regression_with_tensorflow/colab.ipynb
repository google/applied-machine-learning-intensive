{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regression with TensorFlow",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "copyright",
        "5fsbamUAVuCP",
        "0o3jHPj9llA8",
        "s9hGTw7O6Mtu"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/v2/03_regression/08_regression_with_tensorflow/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "hMqWDc_m6rUC",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c2hPzRb6j_CA"
      },
      "source": [
        "# Regression with TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9x88D_U-4oTH"
      },
      "source": [
        "We have trained a linear regression model in TensorFlow and used it to predict housing prices. However, the model didn't perform as well as we would have liked it to. In this lab, we will build a neural network to try to tackle the same regression problem and see if we can get better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AaBjI9hmqnT",
        "colab_type": "text"
      },
      "source": [
        "## Loading and Preparing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4f3CKqFUqL2-"
      },
      "source": [
        "The dataset we'll use for this Colab contains California housing information taken from the 1990 census data. We explored this data in a previous lab, so we won't do an analysis here. As a reminder, the documentation for the dataset can be found [on Kaggle](https://www.kaggle.com/camnugent/california-housing-prices)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MxiIKhP4E2Zr"
      },
      "source": [
        "Upload your `kaggle.json` file and run the code block below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkRBt4c813MP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! chmod 600 kaggle.json && (ls ~/.kaggle 2>/dev/null || mkdir ~/.kaggle) && mv kaggle.json ~/.kaggle/ && echo 'Done'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdltN0vY18H_",
        "colab_type": "text"
      },
      "source": [
        "Once you are done, use the `kaggle` command to download the file into the lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt4pCnny2Am2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle datasets download camnugent/california-housing-prices\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aJqOuak2FCs",
        "colab_type": "text"
      },
      "source": [
        "We now have a file called `california-housing-prices.zip` that we can load into a `DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9ivCDWnwE2Zx",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "housing_df = pd.read_csv('california-housing-prices.zip')\n",
        "\n",
        "housing_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI9ElSbPje6j",
        "colab_type": "text"
      },
      "source": [
        "Next we can define which columns are features and which is the target.\n",
        "\n",
        "We'll also make a separate list of our numeric columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTjb8Gn-92v2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_column = 'median_house_value'\n",
        "feature_columns = [c for c in housing_df.columns if c != target_column]\n",
        "numeric_feature_columns = [c for c in feature_columns if c != 'ocean_proximity']\n",
        "\n",
        "target_column, feature_columns, numeric_feature_columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrOyGuN_k74i",
        "colab_type": "text"
      },
      "source": [
        "We also reduced the value of our targets by a factor in the previous lab. This reduction in magnitude was done to help the model train faster. Let's do that again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RkwE7QGlAjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TARGET_FACTOR = 100000\n",
        "\n",
        "housing_df[target_column] /= TARGET_FACTOR\n",
        "\n",
        "housing_df[target_column].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzTdkcVS2ZCk",
        "colab_type": "text"
      },
      "source": [
        "And we filled in some missing `total_bedrooms` values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LByQBpsy2b--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "has_all_data = housing_df[~housing_df['total_bedrooms'].isna()]\n",
        "\n",
        "sums = has_all_data[['total_bedrooms', 'total_rooms']].sum().tolist()\n",
        "\n",
        "bedrooms_to_total_rooms_ratio = sums[0] / sums[1]\n",
        "\n",
        "missing_total_bedrooms_idx = housing_df['total_bedrooms'].isna()\n",
        "\n",
        "housing_df.loc[missing_total_bedrooms_idx, 'total_bedrooms'] = housing_df[\n",
        "    missing_total_bedrooms_idx]['total_rooms'] * bedrooms_to_total_rooms_ratio\n",
        "\n",
        "housing_df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYI2A_97jlD-",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 1: Standardization\n",
        "\n",
        "Previously when we worked with this dataset, we normalized the feature data in order to get it ready for the model. Normalization was the process of making all of the data fit between 0.0 and 1.0 by subtracting the minimum of each column from each data point in that column and then dividing by the delta between the maximum and minimum values.\n",
        "\n",
        "In this exercise you will need to standardize all of the feature columns. Standardization is performed by subtracting the mean value of each column from each data point in that column and then dividing by the standard deviation.\n",
        "\n",
        "> *Hint: When you are done call `describe()` and ensure that the standard deviation for every feature column is 1.0`*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvbESp4tkTeO",
        "colab_type": "text"
      },
      "source": [
        "#### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PMD-9e8kVDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your Code Goes Here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tsH6xLZkXFL",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVPyB6vhkYJ7",
        "colab_type": "text"
      },
      "source": [
        "#### Answer Key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD6mR-pbkXt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle datasets download camnugent/california-housing-prices\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "housing_df = pd.read_csv('california-housing-prices.zip')\n",
        "\n",
        "target_column = 'median_house_value'\n",
        "feature_columns = [c for c in housing_df.columns if c != target_column]\n",
        "numeric_feature_columns = [c for c in feature_columns if c != 'ocean_proximity']\n",
        "\n",
        "TARGET_FACTOR = 100000\n",
        "\n",
        "housing_df[target_column] /= TARGET_FACTOR\n",
        "\n",
        "has_all_data = housing_df[~housing_df['total_bedrooms'].isna()]\n",
        "sums = has_all_data[['total_bedrooms', 'total_rooms']].sum().tolist()\n",
        "bedrooms_to_total_rooms_ratio = sums[0] / sums[1]\n",
        "missing_total_bedrooms_idx = housing_df['total_bedrooms'].isna()\n",
        "housing_df.loc[missing_total_bedrooms_idx, 'total_bedrooms'] = housing_df[\n",
        "    missing_total_bedrooms_idx]['total_rooms'] * bedrooms_to_total_rooms_ratio\n",
        "\n",
        "# Solution Starts Here\n",
        "\n",
        "housing_df.loc[:, numeric_feature_columns] = (\n",
        "    housing_df[numeric_feature_columns] - \n",
        "      housing_df[numeric_feature_columns].mean()) / housing_df[numeric_feature_columns].std()\n",
        "\n",
        "housing_df[numeric_feature_columns].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fif0juEwkZtE",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt-DiypL6HZ9",
        "colab_type": "text"
      },
      "source": [
        "### One-Hot Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPpALMfd6KbK",
        "colab_type": "text"
      },
      "source": [
        "The `ocean_proximity` column will not work with the neural network model that we are planning to build. Neural networks expect numeric values, but `ocean_proximity` contains string values.\n",
        "\n",
        "Let's remind ourselves which values it contains:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYL4unBF6NeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sorted(housing_df['ocean_proximity'].unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moQf0ZjC6Rso",
        "colab_type": "text"
      },
      "source": [
        "There are five string values. In our linear regression Colab we told TensorFlow to treat these values as a categorical column. Each string was converted to a whole number that represented their position in a vocabulary list: `0`, `1`, `2`, `3`, or `4`.\n",
        "\n",
        "For neural networks it is common to see another strategy called one-hot encoding. One-hot encoding is the process of taking a column with a fixed list of string values and turning it into multiple columns containing only zeros and ones.\n",
        "\n",
        "For instance the column `ocean_proximity` containing five strings would be converted to five columns containing ones and zeros:\n",
        "\n",
        "op_sub_hr | op_inland | op_island | op_near_bay | op_near_ocean\n",
        "----------|-----------|-----------|-------------|--------------\n",
        " 0        | 0         | 0         | 1           | 0\n",
        " 0        | 1         | 0         | 0           | 0\n",
        " 0        | 1         | 0         | 0           | 0\n",
        " 1        | 0         | 0         | 0           | 0\n",
        " 0        | 0         | 1         | 0           | 0\n",
        " 0        | 0         | 0         | 0           | 1\n",
        " 0        | 0         | 1         | 0           | 0\n",
        "\n",
        " Notice that in each row, only one column has a value of `1`. The rest are all `0`. This is the \"one-hot\" in one-hot encoding.\n",
        "\n",
        "As you can imagine, it doesn't scale well for columns with many distinct values. In our case, `5` is perfectly reasonable.\n",
        "\n",
        "Let's manually one-hot encode our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC2E1ItX6Q5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for op in sorted(housing_df['ocean_proximity'].unique()):\n",
        "  op_col = op.lower().replace(' ', '_').replace('<', '')\n",
        "  housing_df[op_col] = (housing_df['ocean_proximity'] == op).astype(int)\n",
        "  feature_columns.append(op_col)\n",
        "\n",
        "feature_columns.remove('ocean_proximity')\n",
        "\n",
        "housing_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6RtwkjglSwN",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2: Split the Data\n",
        "\n",
        "We want to hold out some of the data for validation. Using standard Python or a library, split the data. Put 20% of the data in a `DataFrame` called `testing_df` and the other 80% in a `DataFrame` called `training_df`. Be sure to shuffle the data before splitting. Print the number of records in `testing_df` and `training_df` in order to check your work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30T4hQ4bmCM-",
        "colab_type": "text"
      },
      "source": [
        "#### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GqS7LRJmFsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your Code Goes Here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIOTjR4mmL_X",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo7CwFX-mNEz",
        "colab_type": "text"
      },
      "source": [
        "#### Answer Key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "welCyCX-mPqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle datasets download camnugent/california-housing-prices\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "housing_df = pd.read_csv('california-housing-prices.zip')\n",
        "\n",
        "target_column = 'median_house_value'\n",
        "feature_columns = [c for c in housing_df.columns if c != target_column]\n",
        "numeric_feature_columns = [c for c in feature_columns if c != 'ocean_proximity']\n",
        "\n",
        "TARGET_FACTOR = 100000\n",
        "\n",
        "housing_df[target_column] /= TARGET_FACTOR\n",
        "\n",
        "housing_df.loc[:, numeric_feature_columns] = (\n",
        "    housing_df[numeric_feature_columns] - \n",
        "      housing_df[numeric_feature_columns].mean()) / housing_df[numeric_feature_columns].std()\n",
        "\n",
        "has_all_data = housing_df[~housing_df['total_bedrooms'].isna()]\n",
        "sums = has_all_data[['total_bedrooms', 'total_rooms']].sum().tolist()\n",
        "bedrooms_to_total_rooms_ratio = sums[0] / sums[1]\n",
        "missing_total_bedrooms_idx = housing_df['total_bedrooms'].isna()\n",
        "housing_df.loc[missing_total_bedrooms_idx, 'total_bedrooms'] = housing_df[\n",
        "    missing_total_bedrooms_idx]['total_rooms'] * bedrooms_to_total_rooms_ratio\n",
        "\n",
        "for op in sorted(housing_df['ocean_proximity'].unique()):\n",
        "  op_col = op.lower().replace(' ', '_').replace('<', '')\n",
        "  housing_df[op_col] = (housing_df['ocean_proximity'] == op).astype(int)\n",
        "  feature_columns.append(op_col)\n",
        "feature_columns.remove('ocean_proximity')\n",
        "\n",
        "# Solution Starts Here\n",
        "\n",
        "housing_df = housing_df.sample(frac=1)\n",
        "\n",
        "test_set_size = int(len(housing_df) * 0.2)\n",
        "\n",
        "testing_df = housing_df[:test_set_size]\n",
        "training_df = housing_df[test_set_size:]\n",
        "\n",
        "print(len(training_df), len(testing_df))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyPkAN3mmQW7",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyCUUMkpn0Wi",
        "colab_type": "text"
      },
      "source": [
        "## Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv4GnnTyohQe",
        "colab_type": "text"
      },
      "source": [
        "We will build the model using TensorFlow 2. Let's enable it and go ahead and load up TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u6detBun2pN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4Pfxar2o5O2",
        "colab_type": "text"
      },
      "source": [
        "When we built a TensorFlow `LinearRegressor` in a previous lab, we were using a pre-configured model. For our neural network regressor, we will build the model ourselves using the [Keras](https://www.tensorflow.org/guide/keras) API of TensorFlow.\n",
        "\n",
        "We'll build a **sequential** model where one layer feeds into the next. Each layer will be **densely connected**, which means every node in one layer connects to every node in the next layer.\n",
        "\n",
        "A few things are required for our network. We need to have 13 input nodes since that is the number of features that we have (8 original numerical columns, plus the 5 one-hot encoded ocean proximity columns that we added). We also need to have one output node since we are trying to predict a single price value.\n",
        "\n",
        "Let's see what that would look like:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkkNFjzop3I2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Create the Sequential model.\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Determine the \"input shape\", which is the number\n",
        "# of features that we will feed into the model.\n",
        "input_shape = len(feature_columns)\n",
        "\n",
        "# Create a layer that accepts our features and outputs\n",
        "# a single value, the predicted median home price.\n",
        "layer = layers.Dense(1, input_shape=[input_shape])\n",
        "\n",
        "# Add the layer to our model.\n",
        "model.add(layer)\n",
        "\n",
        "# Print out a model summary.\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdu82rLhrFXM",
        "colab_type": "text"
      },
      "source": [
        "Above we have basically recreated our linear regression from an earlier lab. We have all of our inputs directly mapping to a single output. We didn't choose an activation function, and the default activation function for a [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer is a linear function $f(x) = x$.\n",
        "\n",
        "Note that the way we built this model was pretty verbose. You typically see simple models like this built in a more compact manner:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTCo5CpVLV2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential(layers=[\n",
        "    layers.Dense(1, input_shape=[len(feature_columns)])\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEPw3kIhMtiW",
        "colab_type": "text"
      },
      "source": [
        "Also notice that the layers are named `dense_1`, `dense_2`, etc.\n",
        "\n",
        "If you don't supply a name for a layer, TensorFlow will provide a name for you. In small models, this isn't a problem, but you might want to have a meaningful layer name in larger models.\n",
        "\n",
        "Even in simple models, is `dense_2` a good name for the first layer in a model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25xCx2aCNLcc",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 3: Name Your Layers\n",
        "\n",
        "The default naming scheme for layers can start to become confusing, especially if you repeatedly run a cell block to iterate on your model design.\n",
        "\n",
        "In this exercise consult the [`Dense` documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) and find the argument that allows you to name your layer. Use that argument in the code below to name your layer 'the_only_layer'. Note that you might have to consult the documentation for the parent classes of `Dense`.\n",
        "\n",
        "Also, don't forget to answer the question below the code block!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3SrSWLvViFn",
        "colab_type": "text"
      },
      "source": [
        "#### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjbI2IPsVKlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential(layers=[\n",
        "    layers.Dense(\n",
        "        1,\n",
        "        input_shape=[len(feature_columns)],\n",
        "        # Name your layer here\n",
        "    )\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E3jXNquVhCn",
        "colab_type": "text"
      },
      "source": [
        "Which class did the parameter that you used originate from?\n",
        "\n",
        "> *Your answer goes here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdvLFUFQVsVf",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fsbamUAVuCP",
        "colab_type": "text"
      },
      "source": [
        "##### Answer Key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYJlLkpQVv7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential(layers=[\n",
        "    layers.Dense(\n",
        "        1,\n",
        "        input_shape=[len(feature_columns)],\n",
        "        name='the_only_layer'\n",
        "    )\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jncjAEG8V0yt",
        "colab_type": "text"
      },
      "source": [
        "Which class did the parameter that you used originate from?\n",
        "\n",
        "> The closest parent that has the `name` parameter is [`Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer). The next-level parent, [`Module`](https://www.tensorflow.org/api_docs/python/tf/Module) also has a name parameter, so either would do. If you noticed both, awesome!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f45NeAsBWNfa",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXiiJF3fWReI",
        "colab_type": "text"
      },
      "source": [
        "#### Making a Deep Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef2RnBnKLKjM",
        "colab_type": "text"
      },
      "source": [
        "Where neural networks really get powerful is when you add **hidden layers**. These hidden layers can find complex patterns in your data.\n",
        "\n",
        "Let's create a model with a few hidden layers. We'll add two layers with sixty-four nodes each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIz2AyIPr6Qt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "feature_count = len(feature_columns)\n",
        "\n",
        "model = keras.Sequential([\n",
        "  layers.Dense(64, input_shape=[feature_count]),\n",
        "  layers.Dense(64),\n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BYvMgqWtblx",
        "colab_type": "text"
      },
      "source": [
        "We now have a deep neural network model. The model has 13 input nodes. These nodes feed into our first hidden layer of 64 nodes.\n",
        "\n",
        "The first line of our model summary tells us that we have 64 nodes and 896 parameters. The node count in 'Output Shape' makes sense, but what about the 'Param #' of 896?\n",
        "\n",
        "Remember that we have 13 input nodes feeding into 64 nodes in our first hidden layer. The layers are densely connected, so each of the 13 input nodes connects to each of the 64 nodes in the next layer. `13 * 64 = 832` connections. Add another 64 for the number of nodes in the layer, and you get the 896 number.\n",
        "\n",
        "This pattern repeats for the next layer. 64 nodes connecting to 64 nodes: `64 * 64 + 64 = 4160`.\n",
        "\n",
        "And finally 64 nodes connect to the final output node: `64 * 1 + 1 = 65`.\n",
        "\n",
        "This makes for a total of 5121 parameters in the model. Even a very small neural network like this can have a lot of trainable parameters inside of it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktQvZT_jS0UY",
        "colab_type": "text"
      },
      "source": [
        "Before we start training it, we need to tell TensorFlow how and what to optimize the model for using the [`compile` method](https://keras.io/models/model/#compile). In our example below, we are optimizing for mean squared error using the [Adam optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam).  We'll calculate and report the mean squared error and mean absolute error along the way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58EnP9hwuCxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "  loss='mse',\n",
        "  optimizer='Adam',\n",
        "  metrics=['mae', 'mse'],\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwa9xoVwulvh",
        "colab_type": "text"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-8Vaoryu9rv",
        "colab_type": "text"
      },
      "source": [
        "We can now train the model using the [`fit()` method](https://keras.io/models/model/#fit). Training is performed for a specified number of **epochs**. An epoch is a full pass over the training data. In this case, we are asking to train over the full dataset 50 times.\n",
        "\n",
        "In order to get the data into the model, we don't have to write an input function like we did with the `Estimator` API. The Keras API provides for a much more direct format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suWnhYbUuoYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "model.fit(\n",
        "  training_df[feature_columns],\n",
        "  training_df[target_column],\n",
        "  epochs=EPOCHS,\n",
        "  validation_split=0.2,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuTsElUdv4uU",
        "colab_type": "text"
      },
      "source": [
        "## Validating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFLnpozdwYbL",
        "colab_type": "text"
      },
      "source": [
        "We can now see how well our model performs on our validation test set. In order to get the model to make predictions, we use the [`predict` method](https://keras.io/models/model/#predict)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzC4Q6kuxy-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(testing_df[feature_columns])\n",
        "\n",
        "predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nAGc2VQyIGu",
        "colab_type": "text"
      },
      "source": [
        "Notice that the predictions are lists of lists. This is because neural networks can return more than one prediction per input. We set this network up to have a single final node, but could have had more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH_6-meyzBIj",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 4: Calculating RMSE\n",
        "\n",
        "At this point we have the predicted values from our test features and the actual values. In this exercise you are tasked with computing the root-mean squared error of those predictions. Given the predictions stored in `predictions` above, write code that computes the root mean squared error of those predictions vs. the truth found in `testing_df`. Print the root mean squared error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuY3HZAZl4z7",
        "colab_type": "text"
      },
      "source": [
        "#### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_flON0FHc_yW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your Code Goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DncNYnRdBQ6",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnsgI-budCzh",
        "colab_type": "text"
      },
      "source": [
        "#### Answer Key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkgH_Ae_v8xm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle datasets download camnugent/california-housing-prices\n",
        "\n",
        "import pandas as pd\n",
        "import math \n",
        "\n",
        "from sklearn import metrics\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "housing_df = pd.read_csv('california-housing-prices.zip')\n",
        "\n",
        "target_column = 'median_house_value'\n",
        "feature_columns = [c for c in housing_df.columns if c != target_column]\n",
        "numeric_feature_columns = [c for c in feature_columns if c != 'ocean_proximity']\n",
        "\n",
        "TARGET_FACTOR = 100000\n",
        "\n",
        "housing_df[target_column] /= TARGET_FACTOR\n",
        "\n",
        "housing_df.loc[:, numeric_feature_columns] = (\n",
        "    housing_df[numeric_feature_columns] - \n",
        "      housing_df[numeric_feature_columns].mean()) / housing_df[numeric_feature_columns].std()\n",
        "\n",
        "has_all_data = housing_df[~housing_df['total_bedrooms'].isna()]\n",
        "sums = has_all_data[['total_bedrooms', 'total_rooms']].sum().tolist()\n",
        "bedrooms_to_total_rooms_ratio = sums[0] / sums[1]\n",
        "missing_total_bedrooms_idx = housing_df['total_bedrooms'].isna()\n",
        "housing_df.loc[missing_total_bedrooms_idx, 'total_bedrooms'] = housing_df[\n",
        "    missing_total_bedrooms_idx]['total_rooms'] * bedrooms_to_total_rooms_ratio\n",
        "\n",
        "for op in sorted(housing_df['ocean_proximity'].unique()):\n",
        "  op_col = op.lower().replace(' ', '_').replace('<', '')\n",
        "  housing_df[op_col] = (housing_df['ocean_proximity'] == op).astype(int)\n",
        "  feature_columns.append(op_col)\n",
        "feature_columns.remove('ocean_proximity')\n",
        "\n",
        "housing_df = housing_df.sample(frac=1)\n",
        "\n",
        "test_set_size = int(len(housing_df) * 0.2)\n",
        "\n",
        "testing_df = housing_df[:test_set_size]\n",
        "training_df = housing_df[test_set_size:]\n",
        "\n",
        "feature_count = len(feature_columns)\n",
        "\n",
        "model = keras.Sequential([\n",
        "  layers.Dense(64, input_shape=[feature_count]),\n",
        "  layers.Dense(64),\n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  loss='mse',\n",
        "  optimizer='Adam',\n",
        "  metrics=['mae', 'mse'],\n",
        ")\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "model.fit(\n",
        "  training_df[feature_columns],\n",
        "  training_df[target_column],\n",
        "  epochs=EPOCHS,\n",
        "  validation_split=0.2,\n",
        ")\n",
        "\n",
        "predictions = model.predict(testing_df[feature_columns])\n",
        "\n",
        "# Solution Starts Here\n",
        "\n",
        "root_mean_squared_error = math.sqrt(\n",
        "    metrics.mean_squared_error(\n",
        "      predictions * TARGET_FACTOR,\n",
        "      testing_df[target_column] * TARGET_FACTOR\n",
        "))\n",
        "\n",
        "print(\"Root Mean Squared Error (on training data): %0.3f\" % \n",
        "  root_mean_squared_error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4gknbbpdu2U",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33SMPRXPeLmv",
        "colab_type": "text"
      },
      "source": [
        "## Improving the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAtfBRAXeSYW",
        "colab_type": "text"
      },
      "source": [
        "In the exercise above, you likely got a root mean squared error very close to the error we got in the linear regression lab. What's going on? I thought deep learning models were supposed to be really, really good!\n",
        "\n",
        "Deep learning models can be really good, but they often require a bit of hyperparameter tuning. Aside from the breadth and depth of the hidden layers, the activation function for the model can have a big impact on how a model performs.\n",
        "\n",
        "Earlier we mentioned that the default activation function for `Dense` layers is the linear function $f(x) = x$. It turns out that if you stack layers of linear functions, you just get a single linear function, so the network that we built is basically just one big linear regression.\n",
        "\n",
        "We can change the activation function layer by layer for our model. In order to do that, we just pass an `activation` argument to our `Dense` class. Keras has [many built-in activations](https://www.tensorflow.org/api_docs/python/tf/keras/activations) that you can reference by name like:\n",
        "\n",
        "```python\n",
        "  layers.Dense(64, activation='sigmoid')\n",
        "```\n",
        "\n",
        "For activations that aren't built into Keras, you can use the full path to their class:\n",
        "\n",
        "```python\n",
        "  layers.Dense(64, activation=tf.nn.swish)\n",
        "```\n",
        "\n",
        "The [`tf.nn` namespace](https://www.tensorflow.org/api_docs/python/tf/nn) is a little crowded, but there are activations functions in there including [`swish`](https://www.tensorflow.org/api_docs/python/tf/nn/swish), [`leaky_relu`](https://www.tensorflow.org/api_docs/python/tf/nn/leaky_relu), and more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-BNO2fAeOZ2",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 5: A Better Activation Function\n",
        "\n",
        "Experiment with different activation functions and find one that performs better than the linear activation that we used above. You can set the activation function on any or all of the layers in the network. The functions don't have to be the same.\n",
        "\n",
        "Print out the root mean squared error once you find an acceptable activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMgsYuE0ld9g",
        "colab_type": "text"
      },
      "source": [
        "#### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuWQBoAslfo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your Code Goes Here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2csiDfLlgtT",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o3jHPj9llA8",
        "colab_type": "text"
      },
      "source": [
        "#### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KITtzYN3mSy_",
        "colab_type": "text"
      },
      "source": [
        "There are many activation functions that work better than the linear activation. `relu` is a proven activation function that tends to perform very well in practice. We used it here to get a RMSE that was nearly $20,000 better than our earlier model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfSvH5_glqEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle datasets download camnugent/california-housing-prices\n",
        "\n",
        "import pandas as pd\n",
        "import math \n",
        "\n",
        "from sklearn import metrics\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "housing_df = pd.read_csv('california-housing-prices.zip')\n",
        "\n",
        "target_column = 'median_house_value'\n",
        "feature_columns = [c for c in housing_df.columns if c != target_column]\n",
        "numeric_feature_columns = [c for c in feature_columns if c != 'ocean_proximity']\n",
        "\n",
        "TARGET_FACTOR = 100000\n",
        "\n",
        "housing_df[target_column] /= TARGET_FACTOR\n",
        "\n",
        "housing_df.loc[:, numeric_feature_columns] = (\n",
        "    housing_df[numeric_feature_columns] - \n",
        "      housing_df[numeric_feature_columns].mean()) / housing_df[numeric_feature_columns].std()\n",
        "\n",
        "has_all_data = housing_df[~housing_df['total_bedrooms'].isna()]\n",
        "sums = has_all_data[['total_bedrooms', 'total_rooms']].sum().tolist()\n",
        "bedrooms_to_total_rooms_ratio = sums[0] / sums[1]\n",
        "missing_total_bedrooms_idx = housing_df['total_bedrooms'].isna()\n",
        "housing_df.loc[missing_total_bedrooms_idx, 'total_bedrooms'] = housing_df[\n",
        "    missing_total_bedrooms_idx]['total_rooms'] * bedrooms_to_total_rooms_ratio\n",
        "\n",
        "for op in sorted(housing_df['ocean_proximity'].unique()):\n",
        "  op_col = op.lower().replace(' ', '_').replace('<', '')\n",
        "  housing_df[op_col] = (housing_df['ocean_proximity'] == op).astype(int)\n",
        "  feature_columns.append(op_col)\n",
        "feature_columns.remove('ocean_proximity')\n",
        "\n",
        "housing_df = housing_df.sample(frac=1)\n",
        "\n",
        "test_set_size = int(len(housing_df) * 0.2)\n",
        "\n",
        "testing_df = housing_df[:test_set_size]\n",
        "training_df = housing_df[test_set_size:]\n",
        "\n",
        "feature_count = len(feature_columns)\n",
        "\n",
        "# Solution Starts Here\n",
        "\n",
        "model = keras.Sequential([\n",
        "  layers.Dense(64, input_shape=[feature_count], activation='relu'),\n",
        "  layers.Dense(64, activation='relu'),\n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Solution Ends Here\n",
        "\n",
        "model.compile(\n",
        "  loss='mse',\n",
        "  optimizer='Adam',\n",
        "  metrics=['mae', 'mse'],\n",
        ")\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "model.fit(\n",
        "  training_df[feature_columns],\n",
        "  training_df[target_column],\n",
        "  epochs=EPOCHS,\n",
        "  validation_split=0.2,\n",
        ")\n",
        "\n",
        "predictions = model.predict(testing_df[feature_columns])\n",
        "\n",
        "root_mean_squared_error = math.sqrt(\n",
        "    metrics.mean_squared_error(\n",
        "      predictions * TARGET_FACTOR,\n",
        "      testing_df[target_column] * TARGET_FACTOR\n",
        "))\n",
        "\n",
        "print(\"Root Mean Squared Error (on training data): %0.3f\" % \n",
        "  root_mean_squared_error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUWvhcnrlorU",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E63WyKpmleN",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixm_bQDGuYbr",
        "colab_type": "text"
      },
      "source": [
        "At this point, we have a pretty solid neural network regression model. It performs better than our linear regression model, though it does take a while to train.\n",
        "\n",
        "Training time is largely a product of two factors:\n",
        "\n",
        "1. The size of the model\n",
        "1. The number of epochs\n",
        "\n",
        "Larger models take longer to train. That shouldn't come as a surprise. Remember from above that we calculated the number of parameters in our model. Every layer that is densely connected adds many more parameters that need to be adjusted during training.\n",
        "\n",
        "Our goal is to find a model that is big enough, but not too big. This, it turns out, is very much an area where experimentation is required.\n",
        "\n",
        "The second determination of model training time is the number of epochs. We can choose an arbitrary number of epochs from one to infinity. How many do we need?\n",
        "\n",
        "It turns out that we can be much more scientific about this parameter. As a model begins to converge, there is less and less benefit for each subsequent epoch.\n",
        "\n",
        "More training does not necessarily mean a better model.\n",
        "\n",
        "There are a few ways to determine the appropriate number of epochs. One is to plot the error and see when it flattens out.\n",
        "\n",
        "It turns out that our model actually returns the error values when you fit the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbhsGp9_0LOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([\n",
        "  layers.Dense(64, input_shape=[feature_count]),\n",
        "  layers.Dense(64),\n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  loss='mse',\n",
        "  optimizer='Adam',\n",
        "  metrics=['mae', 'mse'],\n",
        ")\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "history = model.fit(\n",
        "  training_df[feature_columns],\n",
        "  training_df[target_column],\n",
        "  epochs=EPOCHS,\n",
        "  verbose=0,                     # New parameter to make model training silent\n",
        "  validation_split=0.2,\n",
        ")\n",
        "\n",
        "history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqUttCc121Mo",
        "colab_type": "text"
      },
      "source": [
        "Notice that the `history.history` contains our model's loss (`loss`), mean absolute error (`mae`), mean squared error (`mse`), validation loss (`val_loss`), validation mean absolute error (`val_mae`), and validation mean squared error (`val_mse`) at each epoch.\n",
        "\n",
        "It would be useful to plot the error over time. In the next exercise, you will create a visualization that will help us determine when to stop training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_Pn1ZFe3mEp",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 6: Plotting Error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HERuu5h332oV",
        "colab_type": "text"
      },
      "source": [
        "Use `matplotlib.pyplot` or `seaborn` to create a line plot that shows the mean squared error and the validation mean squared error per epoch.\n",
        "\n",
        "In the code block below, we save the errors per epoch in the variable `history`. Inspect the variable and plot a line plot which has the epoch on the x-axis and the mean squared error on the y-axis. There should be two lines on the visualization: mean absolute error and validation mean absolute error.\n",
        "\n",
        "Note that we created the model with the default activation function. Use the activation function that you found to be more useful in exercise 5.\n",
        "\n",
        "The result should be a line plot of epoch and error with two lines similar to:\n",
        "\n",
        "![alt text](https://i.imgur.com/0YDzVhq.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK0v5ehj58nx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([\n",
        "  layers.Dense(64, input_shape=[feature_count]),\n",
        "  layers.Dense(64),\n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  loss='mse',\n",
        "  optimizer='Adam',\n",
        "  metrics=['mae', 'mse'],\n",
        ")\n",
        "\n",
        "EPOCHS = 100\n",
        "\n",
        "history = model.fit(\n",
        "  training_df[feature_columns],\n",
        "  training_df[target_column],\n",
        "  epochs=EPOCHS,\n",
        "  verbose=0,\n",
        "  validation_split=0.2,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H743wjkE56AV",
        "colab_type": "text"
      },
      "source": [
        "#### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rN-BXBX6KkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your Code Goes Here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6XBdKqj6L7Z",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9hGTw7O6Mtu",
        "colab_type": "text"
      },
      "source": [
        "#### Answer Key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzn8VGEh4KzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle datasets download camnugent/california-housing-prices\n",
        "\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import metrics\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "housing_df = pd.read_csv('california-housing-prices.zip')\n",
        "\n",
        "target_column = 'median_house_value'\n",
        "feature_columns = [c for c in housing_df.columns if c != target_column]\n",
        "numeric_feature_columns = [c for c in feature_columns if c != 'ocean_proximity']\n",
        "\n",
        "TARGET_FACTOR = 100000\n",
        "\n",
        "housing_df[target_column] /= TARGET_FACTOR\n",
        "\n",
        "housing_df.loc[:, numeric_feature_columns] = (\n",
        "    housing_df[numeric_feature_columns] - \n",
        "      housing_df[numeric_feature_columns].mean()) / housing_df[numeric_feature_columns].std()\n",
        "\n",
        "has_all_data = housing_df[~housing_df['total_bedrooms'].isna()]\n",
        "sums = has_all_data[['total_bedrooms', 'total_rooms']].sum().tolist()\n",
        "bedrooms_to_total_rooms_ratio = sums[0] / sums[1]\n",
        "missing_total_bedrooms_idx = housing_df['total_bedrooms'].isna()\n",
        "housing_df.loc[missing_total_bedrooms_idx, 'total_bedrooms'] = housing_df[\n",
        "    missing_total_bedrooms_idx]['total_rooms'] * bedrooms_to_total_rooms_ratio\n",
        "\n",
        "for op in sorted(housing_df['ocean_proximity'].unique()):\n",
        "  op_col = op.lower().replace(' ', '_').replace('<', '')\n",
        "  housing_df[op_col] = (housing_df['ocean_proximity'] == op).astype(int)\n",
        "  feature_columns.append(op_col)\n",
        "feature_columns.remove('ocean_proximity')\n",
        "\n",
        "housing_df = housing_df.sample(frac=1)\n",
        "\n",
        "test_set_size = int(len(housing_df) * 0.2)\n",
        "\n",
        "testing_df = housing_df[:test_set_size]\n",
        "training_df = housing_df[test_set_size:]\n",
        "\n",
        "feature_count = len(feature_columns)\n",
        "\n",
        "model = keras.Sequential([\n",
        "  layers.Dense(64, input_shape=[feature_count], activation='relu'),\n",
        "  layers.Dense(64, activation='relu'),\n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  loss='mse',\n",
        "  optimizer='Adam',\n",
        "  metrics=['mae', 'mse'],\n",
        ")\n",
        "\n",
        "EPOCHS = 100\n",
        "\n",
        "history = model.fit(\n",
        "  training_df[feature_columns],\n",
        "  training_df[target_column],\n",
        "  epochs=EPOCHS,\n",
        "  validation_split=0.2,\n",
        ")\n",
        "\n",
        "# Solution Starts Here\n",
        "\n",
        "errors = history.history['mse']\n",
        "validation_errors = history.history['val_mse']\n",
        "epochs = np.arange(0, len(errors))\n",
        "plt.figure(figsize=(14,10))\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE')\n",
        "sns.lineplot(epochs, errors)\n",
        "sns.lineplot(epochs, validation_errors)\n",
        "_ = plt.legend(['Mean Squared Error', 'Validation Mean Squared Error'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57I3QgoD6xO7",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqp93VIH7eEJ",
        "colab_type": "text"
      },
      "source": [
        "### Interpreting Loss Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B4Cwtbs7ifl",
        "colab_type": "text"
      },
      "source": [
        "We have now created a visualization that should look something like this:\n",
        "\n",
        "![alt text](https://i.imgur.com/0YDzVhq.png)\n",
        "\n",
        "But how do we interpret this visualization?\n",
        "\n",
        "The blue line is the mean squared error for the training data. You can see it plummeting fast as the model quickly learns.\n",
        "\n",
        "The orange line is the validation data. This is a holdout set of data that the model checks after each epoch. You can see it dropping pretty quickly, too, but then it seems to stabilize somewhat by 20 epochs.\n",
        "\n",
        "Toward the right side of the graph, you can see that our validation set says volatile but relatively flat, while our training data set keeps getting better and better.\n",
        "\n",
        "Should we train more or less?\n",
        "\n",
        "The constantly reducing blue line is actually a signal of overfitting on the training data.\n",
        "\n",
        "The flat(ish) orange line signals this model is as good as we can get.\n",
        "\n",
        "For this model we could possibly stop training after even 25 epochs and get similar performance.\n",
        "\n",
        "But how do you know when to stop?\n",
        "\n",
        "Luckily there is an *early stopping* algorithm that allows a model to stop training when validation data isn't improving.\n",
        "\n",
        "In the example below, we set up a model to train for 1000 epochs; however, we add an early stopping callback. Early stopping stops training when the model isn't progressing upon validation.\n",
        "\n",
        "If you run the code block below, you'll see far fewer than 1000 epochs run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgCZjgOT7JqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([\n",
        "  layers.Dense(64, input_shape=[feature_count]),\n",
        "  layers.Dense(64),\n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  loss='mse',\n",
        "  optimizer='Adam',\n",
        "  metrics=['mae', 'mse'],\n",
        ")\n",
        "\n",
        "EPOCHS = 1000\n",
        "\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "history = model.fit(\n",
        "  training_df[feature_columns],\n",
        "  training_df[target_column],\n",
        "  epochs=EPOCHS,\n",
        "  validation_split=0.2,\n",
        "  callbacks=[early_stop],\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehTtTmPT61oH",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21IdNfUG64H6",
        "colab_type": "text"
      },
      "source": [
        "We have now learned how to build a deep neural network to solve a regression problem. We have visualized our loss in order to determine when we might stop training, and we have utilized early stopping to avoid wasting time training a model.\n",
        "\n",
        "Welcome to deep neural networks. They are deceptively simple to build, but they are very complex to master. When you can build a model to fit a domain, you can create amazing predictions that rival human experts."
      ]
    }
  ]
}
