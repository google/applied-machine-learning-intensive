{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression with TensorFlow",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "copyright",
        "bI4aY3FWp-xh",
        "DkrSkr1kxPU0",
        "EBDrp9lVP6VL",
        "exercise-2-key-1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "hMqWDc_m6rUC",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c2hPzRb6j_CA"
      },
      "source": [
        "# Linear Regression with TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9x88D_U-4oTH"
      },
      "source": [
        "In previous units we learned about regression and about how to build and apply a regression model using [scikit-learn](https://scikit-learn.org/stable/). For many regression cases `scikit-learn` is more than adequate. However, there are times when more powerful tools are needed. [TensorFlow](https://www.tensorflow.org/) is one of those tools. It is a computational toolkit built to perform machine learning and data science tasks at scale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bW7AVurPpb0u"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1GF_PMg4plQ-"
      },
      "source": [
        "### Estimated Duration\n",
        "\n",
        "120 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l6Kb0efckmq",
        "colab_type": "text"
      },
      "source": [
        "## Problem Framing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hknYP8ou1cPK"
      },
      "source": [
        "Machine learning is one of a variety of solutions that might work for solving a problem. It is always important to understand the problem space before diving in and starting to clean data and code.\n",
        "\n",
        "In this lab we would like to be able to **predict the price of a house**.\n",
        "\n",
        "Questions we should ask ourselves might include:\n",
        "\n",
        "* Predict the price when? Now? In the past? In the future? For what range?\n",
        "* Where are we predicting prices for? One market? One state? One country?\n",
        "* What is our tolerance for being wrong?\n",
        "* Are we okay with a few huge outliers if the overall model is better?\n",
        "* What metrics are we using to define success and what are the acceptable values?\n",
        "* Is there an non-ML way to solve this problem?\n",
        "* What data is available to solve the problem?\n",
        "\n",
        "The list of questions is boundless. Eventually you'll need to move on, but understanding the problem and the solution space is vital.\n",
        "\n",
        "---\n",
        "\n",
        "For this problem we'll further define the problem by saying:\n",
        "\n",
        ">  We want to create a system that predicts the prices of houses in California in 1990. We have census data from 1990 available to build and test the system. We will accept a system with a root mean squared error of 200,000 or better.\n",
        "\n",
        "Since this is a contrived example we'll short-cut and say that our analysis has led us to believe that we want to use a linear regression model to serve as our prediction system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AaBjI9hmqnT",
        "colab_type": "text"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4f3CKqFUqL2-"
      },
      "source": [
        "The dataset we'll use for this Colab contains California housing data taken from the 1990 census data. This is a popular dataset for experimenting with machine learning models.\n",
        "\n",
        "As with any data science project it is a good idea to take some time and review the [data schema and description](https://developers.google.com/machine-learning/crash-course/california-housing-data-description). Ask yourself:\n",
        "\n",
        "* What data is available? What are the columns?\n",
        "* What do those columns mean?\n",
        "* What data types are those columns?\n",
        "* What is the granularity of the data? In this particular case, what is a \"block\"?\n",
        "* How many rows of data are there?\n",
        "* Roughly how big is the data? Kilobytes? Megabytes? Gigabytes? Terabytes? More?\n",
        "* Are any of the columns highly correlated?\n",
        "* What bias is contained in the data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV1IN4SEmvSV",
        "colab_type": "text"
      },
      "source": [
        "### Load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MxiIKhP4E2Zr"
      },
      "source": [
        "Now that we have an understanding of the data that we are going to use in our model, let's load it into this Colab and examine it more closely.\n",
        "\n",
        "We'll rely on Pandas to read a CSV version of the data from the internet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9ivCDWnwE2Zx",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = ('https://download.mlcc.google.com/mledu-datasets/' + \n",
        "       'california_housing_train.csv')\n",
        "\n",
        "housing_df = pd.read_csv(url)\n",
        "\n",
        "housing_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkAz5fKTmyIu",
        "colab_type": "text"
      },
      "source": [
        "### Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6mYFU9r0Vfcx"
      },
      "source": [
        "You should always look at your data and statistics about that data before you begin modelling it. First though, let's see the columns and data types that we have available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgRbsFWWm-bW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "housing_df.dtypes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWqdbWXAnSI8",
        "colab_type": "text"
      },
      "source": [
        "Eight floating point features and a floating point target, `median_house_value`. This is what we expect based on the [data documentation](https://developers.google.com/machine-learning/crash-course/california-housing-data-description)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbAA98gBnhcG",
        "colab_type": "text"
      },
      "source": [
        "It is a good idea to also describe the data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_uC_781pU-K",
        "colab_type": "text"
      },
      "source": [
        "#### Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "gzb10yoVrydW",
        "colab": {}
      },
      "source": [
        "housing_df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tcms-0dziNlO"
      },
      "source": [
        "In this case we can see that all of the column counts are the same. That lets us know that every data point has a value. This can sometimes give you a false sense of security because many datasets have default values instead of empty values.\n",
        "\n",
        "Looking at the min and max can be helpful too. Does a 1.0 value for a minimum number of rooms for a block match your mental model of what a block is? What about that max of 37,937.0 rooms? In cases like this it can be useful to [research your topic area](https://www2.census.gov/geo/pdfs/reference/GARM/Ch11GARM.pdf).\n",
        "\n",
        "In this particular case those numbers are might be okay as long as the dense block is in an urban area with very dense and tall buildings on the block. As you probe a dataset you should ask yourself questions like this. When something doesn't look right, investigate it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOORlGOFpghq",
        "colab_type": "text"
      },
      "source": [
        "##### Exercise 1: Sanity Check\n",
        "\n",
        "Use Pandas to find the row of data that contains the census block with the largest number of rooms. Search for the latitude and longitude for that location and answer the questions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YoH2f9npzJM",
        "colab_type": "text"
      },
      "source": [
        "**Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkowOrlup0zW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your Code Goes Here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpvOIDvtp2Q9",
        "colab_type": "text"
      },
      "source": [
        "1. What city is the block located in?\n",
        "> *Your solution goes here*\n",
        "1. Are 37937.0 total rooms reasonable? Why?\n",
        "> *Your solution goes here*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60nqtytFqAq2",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI4aY3FWp-xh",
        "colab_type": "text"
      },
      "source": [
        "###### Answer Key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dwuir_3qBx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = ('https://download.mlcc.google.com/mledu-datasets/' + \n",
        "       'california_housing_train.csv')\n",
        "\n",
        "housing_df = pd.read_csv(url)\n",
        "\n",
        "housing_df.sort_values(by='total_rooms').tail(4)[[\n",
        "  'total_rooms','latitude', 'longitude']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoEMzn71spI2",
        "colab_type": "text"
      },
      "source": [
        "1. What city is the block located in?\n",
        "> Yorba Linda, CA\n",
        "1. Are 37937.0 total rooms reasonable? Why?\n",
        "> That's a tough one without knowing the dimensions of the census block, but the answer most likely that the data is not reasonable. The entire town of Yorba Linda has just over 20,000 residents and likely there are more than one census blocks in the town. If there were a military base or prison nearby, then it might make sense, but there doesn't seem to be. This does look like bad data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JiRbzawqCL3",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNzV5-mrtgMq",
        "colab_type": "text"
      },
      "source": [
        "#### Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iFY9F9PPjKcC"
      },
      "source": [
        "It is also a good idea to take a look a the actual data. We can use Panda's `head()`, `tail()`, and/or `sample()` methods to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DyJQrnogjSNc",
        "colab": {}
      },
      "source": [
        "housing_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JmHCrw9cjbpr"
      },
      "source": [
        "Did you gain any insight from peeking at the actual data? Is the data sorted in a manner that might lead to a bad model?\n",
        "\n",
        "In this case the data seems to be sorted ascending by longitude and possibly secondarily descending by latitude. We need to consider this when sampling or splitting the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlLhD4Bqundm",
        "colab_type": "text"
      },
      "source": [
        "#### Correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBSYWGXDwAC0",
        "colab_type": "text"
      },
      "source": [
        "It is important to understand how columns relate to one another. Every feature that you add to a your training set increases the amount of work that must be done to train your model. If you can find columns with a high degree of correlation, you can potentially not use one of the columns in your training and still get a model that performs well.\n",
        "\n",
        "Let's create a correlation matrix heatmap for our data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3javxCbhusYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12,10))\n",
        "_ = sns.heatmap(housing_df.corr(), cmap='coolwarm', annot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ_kyCHQwg3c",
        "colab_type": "text"
      },
      "source": [
        "##### Exercise 2: Correlated Columns\n",
        "\n",
        "Answer the following questions about the correlation between columns in our data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHFuBiX1wypg",
        "colab_type": "text"
      },
      "source": [
        "**Student Solution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsHfpMg-w5CJ",
        "colab_type": "text"
      },
      "source": [
        "1. What columns are the most highly correlated?\n",
        "> *Your solution goes here.*\n",
        "1. Which column is most strongly correlated with `median_house_value`?\n",
        "> *Your solution goes here.*\n",
        "1. Which columns have the strongest negative correlation?\n",
        "> *Your solution goes here.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSmvBtqhxOgk",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkrSkr1kxPU0",
        "colab_type": "text"
      },
      "source": [
        "###### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaZadWLSxRAl",
        "colab_type": "text"
      },
      "source": [
        "1. What columns are the most highly correlated?\n",
        "> totoal_bedrooms and households\n",
        "1. Which column is most strongly correlated with `median_house_value`?\n",
        "> median_income\n",
        "1. Which columns have the strongest negative correlation?\n",
        "> latitude and longitude\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggt58MTWxbF0",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_jewMyK0del",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XApHK9X10kJs",
        "colab_type": "text"
      },
      "source": [
        "Now is the stage where we would perform model-independent preprocessing to the data to repair any missing data. Since there isn't really any missing data, we don't have much pre-procesisng to do.\n",
        "\n",
        "Let's look at those room counts again though. The values seem a little odd.\n",
        "\n",
        "First we'll plot the room counts in ascending order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrX1gOWP0cZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rooms = housing_df['total_rooms'].sort_values().reset_index(drop=True)\n",
        "\n",
        "plt.figure(figsize=(12,10))\n",
        "_ = sns.lineplot(x=rooms.index.values, y=rooms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZWeBpvt2WtI",
        "colab_type": "text"
      },
      "source": [
        "That's quite a spike there at the end!\n",
        "\n",
        "Looking at the chart let's pick a point where the number of rooms really starts to extremely slope upward, say 10,000. If we chose to drop the rows with really large values, what would that do to our data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OT5hQgj1154",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "many_rooms = rooms[rooms > 10000].size\n",
        "\n",
        "percent = (many_rooms / rooms.size) * 100\n",
        "\n",
        "print(f'{many_rooms} blocks have more than 10000 rooms ' +\n",
        "      f'which is {percent:0.2f}% of our data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRXdRK6O3Lih",
        "colab_type": "text"
      },
      "source": [
        "So we'd knock out over 1% of our data by trying to remove what we think are outliers. That's not horrible, but is probably not something we would want to do on a hunch.\n",
        "\n",
        "At this point we'll choose not to make any model-independent changes to our data and will continue building the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrGexe8F3qux",
        "colab_type": "text"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pAZcUet3tPG",
        "colab_type": "text"
      },
      "source": [
        "It is time to actually build our model. In this case we know we are going to build a linear regression model using TensorFlow. We could buld the model by hand, but luckily we don't have too. TensorFlow provides many pre-built models in their [estimator](https://www.tensorflow.org/api_docs/python/tf/estimator) library. We are going to use the [`tensorflow.estimator.LinearRegressor`](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor) model.\n",
        "\n",
        "The [`Estimator`](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator) class is the base for TensorFlow estimators. It's methods define the API for estimators. In the remainder of this lab we will create an instance of `LinearRegressor` and use the `Estimator` API to train the model and make test predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vVk_qlG6U80j"
      },
      "source": [
        "### Prepare the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1ugcRBM6iDL",
        "colab_type": "text"
      },
      "source": [
        "Earlier we considered pre-processing the data. That pre-processing was intended to be more generic preprocessing that needed to be done to correct errors with the data set.\n",
        "\n",
        "Now that we have chosen a model, we need to to specific processing related to the type of model that we'll be building and how we are going to test and train the model.\n",
        "\n",
        "Initially we'll be using the `LinearRegressor` with default options. For measuring model quality we will perform hold-out testing with 20% of the data being held out for test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p1--v928V01",
        "colab_type": "text"
      },
      "source": [
        "#### Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAURgg8H9Rwi",
        "colab_type": "text"
      },
      "source": [
        "The scale and range of data in each column of our dataset varies widely. In many models, larger values with be over-considered in training. In order to combat this we can *normalize* our data.\n",
        "\n",
        "Note that we only want to normalize the feature data so let's first create variables to hold our feature and target column names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTjb8Gn-92v2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_column = 'median_house_value'\n",
        "feature_columns = [c for c in housing_df.columns if c != target_column]\n",
        "\n",
        "target_column, feature_columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8nypshl_CIt",
        "colab_type": "text"
      },
      "source": [
        "To normalize we subtract the minimum value from each column and then divide by the delta between the min and max. This should make all of our feature values fall into the range of 0.0 to 1.0. You can see in the `describe()` output that we now have a min values of 0.0 and max values of 1.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGiqSei4-N6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "housing_df.loc[:, feature_columns] = (\n",
        "    housing_df[feature_columns] - \n",
        "      housing_df[feature_columns].min()) / (\n",
        "          housing_df[feature_columns].max() -\n",
        "            housing_df[feature_columns].min())\n",
        "\n",
        "housing_df[feature_columns].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNrZW7VfcSJh",
        "colab_type": "text"
      },
      "source": [
        "Another option would be to *standardize* the data. Standardization is the process of subtracting the mean from each column and then dividing by the standard deviation. We chose not to do that in this case because that creates negative values, which don't work well with this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veSSyPI5ctrK",
        "colab_type": "text"
      },
      "source": [
        "Should we modify the target in any way?\n",
        "\n",
        "Let's take a look at the values again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJhhKY6_d2lw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "housing_df[target_column].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iOvgkdMd9-v",
        "colab_type": "text"
      },
      "source": [
        "Those are some pretty big values. It does look like there is a ceiling of 500,001 applied to the data and a minimum value of 14,999.\n",
        "\n",
        "Given enough time, our model could train to predict these big of values. However, we are going to be using a pretty small learning rate by default with the [`Ftrl` optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Ftrl): 0.0001. In order to speed things up we can shrink the values in the target column by some constant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1AeytQWbNXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TARGET_FACTOR = 100000\n",
        "\n",
        "housing_df[target_column] = housing_df[target_column] / TARGET_FACTOR\n",
        "\n",
        "housing_df[target_column].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbc5GXvxfQoI",
        "colab_type": "text"
      },
      "source": [
        "We've reduced the values from the range of 14,999-500,001 to 0.14999-5.0. This should allow the model to converge faster. Of course, now our predictions will need to be multiplied by 100,000 in order to reflect real dollar values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xuz9tZs5irjC"
      },
      "source": [
        "#### Train/Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctYZoXqt_aMi",
        "colab_type": "text"
      },
      "source": [
        "We want to go ahead and divide our data into testing and training splits. For this example we'll hold out 20% of the data for testing.\n",
        "\n",
        "One easy way to do that is just to slice the data; however, our data is sorted by latitude and longitude so we need to shuffle it first so that we aren't testing with data from just one location in California."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6Bz5_x-FjJX9",
        "colab": {}
      },
      "source": [
        "# Shuffle\n",
        "housing_df = housing_df.sample(frac=1)\n",
        "\n",
        "# Calculate test set size\n",
        "test_set_size = int(len(housing_df) * 0.2)\n",
        "\n",
        "# Split the data\n",
        "testing_df = housing_df[:test_set_size]\n",
        "training_df = housing_df[test_set_size:]\n",
        "\n",
        "print(f'Holding out {len(testing_df)} records for testing. ')\n",
        "print(f'Using {len(training_df)} records for training.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiMaAnXwACKa",
        "colab_type": "text"
      },
      "source": [
        "### Load TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEqvGadvAEPP",
        "colab_type": "text"
      },
      "source": [
        "We'll next load the [TensorFlow](http://www.tensorflow.org) library.\n",
        "\n",
        "TensorFlow released version 2.0 in late 2019. As of the writing of the lab, Colab supports both versions 1 and 2, but it defaults to version 1. In order to tell Colab to use TensorFlow 2 you need to run the magic in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Px986j_Vklo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgYChzI1A6Ec",
        "colab_type": "text"
      },
      "source": [
        "Next we'll load TensorFlow and check to make sure that we are running version 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8GO4OPNWAE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b12uHZlKOZvf",
        "colab_type": "text"
      },
      "source": [
        "Finallly, we set some global settings for TensorFlow. In this case we want to ensure that any time there is a question about the size of a floating point value that it is processed as a 64-bit number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrjXtlV6OhvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.set_floatx('float64')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "odbXASMkjn2d"
      },
      "source": [
        "### TensorFlow Data Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U9TuQquBq4J",
        "colab_type": "text"
      },
      "source": [
        "`DataFrame` is a container for a dataset in Pandas. To process the data with TensorFlow we need to get the data in the `DataFrame` into a TensorFlow [Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n",
        "\n",
        "Since our housing data fits in memory, we can use the `from_tensor_slices` class method to create our `Dataset`. There are a few different data formats that we could pass the method, but our model expects a feature map and a list of labels.\n",
        "\n",
        "A feature map is a Python dictionary with feature names for keys and an iterable of column values as the value. Labels are just an interable of our target values.\n",
        "\n",
        "Below we create the test and training `DataSet` objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDiQDV-yGUru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing_ds = tf.data.Dataset.from_tensor_slices((\n",
        "    {c: testing_df[c] for c in feature_columns},  # feature map\n",
        "    testing_df[target_column]                     # labels\n",
        "))\n",
        "\n",
        "training_ds = tf.data.Dataset.from_tensor_slices((\n",
        "    {c: training_df[c] for c in feature_columns},  # feature map\n",
        "    training_df[target_column]                     # labels\n",
        "))\n",
        "\n",
        "testing_ds, training_ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZLd86oNWmfJp"
      },
      "source": [
        "The code above runs and displays two `TensorSliceDataset` objects that seem to have the correct columns. However, we can't tell how many rows of data each contains.\n",
        "\n",
        "Intuitively you'd think this would be as simple as asking for the length of the data sets from Python:\n",
        "\n",
        "```\n",
        " len(testing_ds)\n",
        " len(training_ds)\n",
        "```\n",
        "\n",
        "This won't work though. TensorFlow data set objects can represent in-memory data, like what we have now. They can also represent data in multiple sources stored in different locations. They can even represent a stream of data that is never-ending. For this reason having a standard `len` is impossible.\n",
        "\n",
        "Because of this we need to do a little more work to get a count of the data in a TensorFlow data set. To get a count we'll use the `reduce` operation. This operation takes an initial value, in our case 0, and then performs some function over and over for each row in the dataset. In this case we just add one for each value. The reduction returns values for each row and feeds it to the next. The final row simply returns the value to the runtime.\n",
        "\n",
        "We can see below that the `reduce` operation counts the number of rows for the testing and training dataset and they both match the values we saw above in the Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qiNjkMGrm1B0",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "testing_ds_count = testing_ds.reduce(np.int64(0), lambda x, _: x + 1)\n",
        "training_ds_count = training_ds.reduce(np.int64(0), lambda x, _: x + 1)\n",
        "\n",
        "print(testing_ds_count.numpy())\n",
        "print(training_ds_count.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v7Plfl3K6xEj"
      },
      "source": [
        "### LinearRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIpAcoJ2KNOY",
        "colab_type": "text"
      },
      "source": [
        "The model that we'll use is the [LinearRegressor](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor). This class complies with the  TensorFlow [Estimator](https://www.tensorflow.org/get_started/estimator) API. This API takes care of a lot of the low-level model plumbing, and exposes convenient methods for performing model training, evaluation, and inference.\n",
        "\n",
        "Though the `LinearRegressor` has many configuration options, [only feature columns have to be specified when the regressor is created](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor#__init__).\n",
        "\n",
        "We provide the regressor [feature columns](https://www.tensorflow.org/guide/feature_columns) as a list of columns that we'd like the model to use for training and prediction. For now that will be every one of our features. These columns are all floating put numbers so we use a list expansion to create a list of `float64` `numeric_column` objects.\n",
        "\n",
        "A warning will be issued if you don't specify a `model_dir`. For now that is fine since we don't plan on saving our model and plan to train it completely now. If we do specify a model directory state will be saved that can cause issues as you iterate on the design of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2XBeRgQh7cZ9",
        "colab": {}
      },
      "source": [
        "housing_features = [\n",
        "    tf.feature_column.numeric_column(c, dtype=tf.dtypes.float64) \n",
        "      for c in feature_columns\n",
        "]\n",
        "\n",
        "linear_regressor = tf.estimator.LinearRegressor(\n",
        "    feature_columns=housing_features,\n",
        ")\n",
        "\n",
        "linear_regressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOrG0PM_RR3e",
        "colab_type": "text"
      },
      "source": [
        "If we had multiple workers, we could distribute the training and evaluation of the model by using a distribution strategy. In the example below you can see that we are using a [`MirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy) to spread out the work.\n",
        "\n",
        "More information on distrubuting `Estimator` work can be found [in the TensorFlow documentation](https://www.tensorflow.org/guide/distributed_training#using_tfdistributestrategy_with_estimator_limited_support)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjuPqN0oRDkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "housing_features = [\n",
        "    tf.feature_column.numeric_column(c, dtype=tf.dtypes.float64) \n",
        "      for c in feature_columns\n",
        "]\n",
        "\n",
        "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
        "config = tf.estimator.RunConfig(\n",
        "    train_distribute=mirrored_strategy,\n",
        "    eval_distribute=mirrored_strategy,\n",
        ")\n",
        "\n",
        "linear_regressor = tf.estimator.LinearRegressor(\n",
        "    feature_columns=housing_features,\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "linear_regressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2IY422qVBPhy"
      },
      "source": [
        "### Training Input Function\n",
        "\n",
        "The LinearRegressor that we just created is still not trained. To train the model we need to call the [train](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor#train) method and pass it an input function provides a `DataSet` to extract data from.\n",
        "\n",
        "We saw how to create a `DataSet` earlier. It would be nice if we could reuse that `DataSet`, but TensorFlow requires that you create the `DataSet` in your function so we'll use the same `DataSet` creation code from above.\n",
        "\n",
        "We also need to change a few attributes of the dataset. Our training data only has 13600 records, which isn't a lot of data. We can choose to repeat the data so that it is fed to the model multiple times. In this case we chose to repeat it 10 times. Hopefully this will give the optimizer enough data to find a good solution.\n",
        "\n",
        "Since we are repeating the same data over-and-over, we also are going to shuffle it in-between repeats. This will give add some varibility to the training data.\n",
        "\n",
        "Finally, we choose to process the data in batches of 100. These mini-batches of 100 are used for a single optimization step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "05R0-6Da3axE",
        "colab": {}
      },
      "source": [
        "def training_input():\n",
        "  ds = tf.data.Dataset.from_tensor_slices((\n",
        "    {c: training_df[c] for c in feature_columns},  # feature map\n",
        "    training_df[target_column]                     # labels\n",
        "  ))\n",
        "  ds = ds.repeat(100)\n",
        "  ds = ds.shuffle(buffer_size=10000)\n",
        "  ds = ds.batch(100)\n",
        "  return ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3WczWK_rFnHV"
      },
      "source": [
        "### Training\n",
        "\n",
        "We can now call the `train` method on the regressor, passing it the input function that we defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m32hOayrFuYQ",
        "colab": {}
      },
      "source": [
        "linear_regressor.train(input_fn=training_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "89rdMHaJ_zC3"
      },
      "source": [
        "We can see in the above output how TensorFlow's LinearRegressor will tell us, as it's training, what the loss is as the model improves. This output can be useful when, later on, we'll tweak the learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot197scbO3kG",
        "colab_type": "text"
      },
      "source": [
        "### Testing Input Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RzhWiGIzHQAw"
      },
      "source": [
        "In order to evaluate the quality of our model we need to make predictions and see how close they are to reality. To do this we rely on the [`predict()`](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor#predict) method.\n",
        "\n",
        "Similar to `train`, this method expects an input function. We'll create one similar to the one we created for train, only we won't repeat or shuffle the data and will process the data in batches of 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5bfSUNuPxw0",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 3: Create a Testing Input Function\n",
        "\n",
        "Create a testing input function called `testing_input`. The function should accept no arguments and should return a `DataSet`. The `DataSet` should not repeats, not shuffle, and should have batches of size 1. Also, target/label values aren't needed for testing input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZphAewXsHgKp",
        "colab": {}
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pesEo_YNQADB",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBDrp9lVP6VL",
        "colab_type": "text"
      },
      "source": [
        "##### Answer Key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1z6GtbHP1jk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testing_input():\n",
        "  ds = tf.data.Dataset.from_tensor_slices((\n",
        "    {c: testing_df[c] for c in feature_columns},  # feature map\n",
        "    testing_df[target_column]                     # labels\n",
        "  ))\n",
        "  ds = ds.batch(1)\n",
        "  return ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwBusV7CP2go",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7qke3gvShNkX"
      },
      "source": [
        "### Make Predictions\n",
        "\n",
        "Now we need to make predictions using our test features. To do that we pass our testing input function to the `predict` method on our trained linear regressor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EGds4R4fhSZL",
        "colab": {}
      },
      "source": [
        "predictions = linear_regressor.predict(input_fn=testing_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oi6Hv0mhhcsx"
      },
      "source": [
        "That runs pretty fast... almost suspiciously fast. The reason is that the model isn't actually making predictions at this point. We have just built the graph to make predictions. TensorFlow is using lazy execution. The predictions won't be made until we ask for them.\n",
        "\n",
        "Let's go ahead and get the predictions and put them in a NumPy array so that we can calculate our error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "twhy8Q7hhv9T",
        "colab": {}
      },
      "source": [
        "predicted_median_values = [item['predictions'][0] for item in predictions]\n",
        "print(\"Our predictions: \", predicted_median_values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PMlmcvIdh1iE"
      },
      "source": [
        "### Evaluate Model\n",
        "\n",
        "Now that we have predictions we can compare them to our actual values and evaluate the quality of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jKUugsQTiBeH",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "mean_squared_error = metrics.mean_squared_error(\n",
        "    np.array(predicted_median_values) * TARGET_FACTOR,\n",
        "    testing_df[target_column] * TARGET_FACTOR\n",
        ")\n",
        "print(\"Mean Squared Error (on training data): %0.3f\" % mean_squared_error)\n",
        "\n",
        "root_mean_squared_error = math.sqrt(mean_squared_error)\n",
        "print(\"Root Mean Squared Error (on training data): %0.3f\" % root_mean_squared_error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDg7cEjwgpoF",
        "colab_type": "text"
      },
      "source": [
        "What is this telling us? The mean square error is somewhat hard to think about. However, whenever you take the root you get the units of the target column. In our test run we were `71020.204` dollars off on our predictions.\n",
        "\n",
        "Is that good?\n",
        "\n",
        "Let's see what the mean price is in our test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOFO6tqhhENB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing_df[target_column].mean() * TARGET_FACTOR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CkwSgWUhP4r",
        "colab_type": "text"
      },
      "source": [
        "About 210,000 dollars. 71,000 is about 35% of 210,000 so our model is off by a mean of 34% of the actual price. I probably wouldn't make many bets using the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1tRI52Vxihh0"
      },
      "source": [
        "## Exercise 4: Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5PpoW4JOm3Xa"
      },
      "source": [
        "There are a few hyperparameters that we can adjust in order to try to improve our model. In the code cell below you'll find most of the code that we've used so far in this lab. There are three `TODO` markers in the code. Find them and:\n",
        "\n",
        "1. Have the model use the [Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)\n",
        "1. Configure the training `DataSet`. Experiment with different batch sizes. Leave the batch size that performs the best in the code.\n",
        "1. Configure the testing `DataSet`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4T1w-bNP9av2"
      },
      "source": [
        "**Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3XxK53QSjE_V",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "# Load the data\n",
        "url = ('https://download.mlcc.google.com/mledu-datasets/' + \n",
        "       'california_housing_train.csv')\n",
        "\n",
        "housing_df = pd.read_csv(url)\n",
        "\n",
        "# Create lists of column names\n",
        "target_column = 'median_house_value'\n",
        "feature_columns = [c for c in housing_df.columns if c != target_column]\n",
        "\n",
        "# Normalize the feature columns\n",
        "housing_df.loc[:, feature_columns] = (\n",
        "    housing_df[feature_columns] - \n",
        "      housing_df[feature_columns].min()) / (\n",
        "          housing_df[feature_columns].max() -\n",
        "            housing_df[feature_columns].min())\n",
        "\n",
        "# Scale the target column\n",
        "TARGET_FACTOR = 100000\n",
        "housing_df[target_column] = housing_df[target_column] / TARGET_FACTOR\n",
        "\n",
        "# Test/Train split\n",
        "housing_df = housing_df.sample(frac=1)\n",
        "test_set_size = int(len(housing_df) * 0.2)\n",
        "testing_df = housing_df[:test_set_size]\n",
        "training_df = housing_df[test_set_size:]\n",
        "\n",
        "# Create TensorFlow features\n",
        "housing_features = [\n",
        "    tf.feature_column.numeric_column(c, dtype=tf.dtypes.float64) \n",
        "      for c in feature_columns\n",
        "]\n",
        "\n",
        "# Create model\n",
        "linear_regressor = tf.estimator.LinearRegressor(\n",
        "    feature_columns=housing_features,\n",
        "    # TODO: Set Optimizer\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "def training_input():\n",
        "  ds = tf.data.Dataset.from_tensor_slices((\n",
        "    {c: training_df[c] for c in feature_columns},  # feature map\n",
        "    training_df[target_column]                     # labels\n",
        "  ))\n",
        "  # TODO: Configure DataSet\n",
        "  return ds\n",
        "\n",
        "linear_regressor.train(\n",
        " input_fn=training_input\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "def testing_input():\n",
        "  ds = tf.data.Dataset.from_tensor_slices((\n",
        "    {c: testing_df[c] for c in feature_columns},  # feature map\n",
        "    testing_df[target_column]                     # labels\n",
        "  ))\n",
        "  # TODO: Configure DataSet\n",
        "  return ds\n",
        "\n",
        "predictions_node = linear_regressor.predict(\n",
        "  input_fn=testing_input,\n",
        ")\n",
        "\n",
        "# Convert the predctions to a NumPy array\n",
        "predicted_median_values = np.array(\n",
        "    [item['predictions'][0] for item in predictions_node])\n",
        "\n",
        "# Find the RMSE\n",
        "root_mean_squared_error = math.sqrt(\n",
        "    metrics.mean_squared_error(\n",
        "      predicted_median_values * TARGET_FACTOR,\n",
        "      testing_df[target_column] * TARGET_FACTOR\n",
        "))\n",
        "\n",
        "print(\"Root Mean Squared Error (on training data): %0.3f\" % \n",
        "root_mean_squared_error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Y_c__4i9jk",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-1-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L8zRmh7z9eRw",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "# Load the data\n",
        "url = ('https://download.mlcc.google.com/mledu-datasets/' + \n",
        "       'california_housing_train.csv')\n",
        "\n",
        "housing_df = pd.read_csv(url)\n",
        "\n",
        "# Create lists of column names\n",
        "target_column = 'median_house_value'\n",
        "feature_columns = [c for c in housing_df.columns if c != target_column]\n",
        "\n",
        "# Normalize the feature columns\n",
        "housing_df.loc[:, feature_columns] = (\n",
        "    housing_df[feature_columns] - \n",
        "      housing_df[feature_columns].min()) / (\n",
        "          housing_df[feature_columns].max() -\n",
        "            housing_df[feature_columns].min())\n",
        "\n",
        "# Scale the target column\n",
        "TARGET_FACTOR = 100000\n",
        "housing_df[target_column] = housing_df[target_column] / TARGET_FACTOR\n",
        "\n",
        "# Test/Train split\n",
        "housing_df = housing_df.sample(frac=1)\n",
        "test_set_size = int(len(housing_df) * 0.2)\n",
        "testing_df = housing_df[:test_set_size]\n",
        "training_df = housing_df[test_set_size:]\n",
        "\n",
        "# Create TensorFlow features\n",
        "housing_features = [\n",
        "    tf.feature_column.numeric_column(c, dtype=tf.dtypes.float64) \n",
        "      for c in feature_columns\n",
        "]\n",
        "\n",
        "# Create model\n",
        "adam_optimizer = tf.keras.optimizers.Adam()\n",
        "linear_regressor = tf.estimator.LinearRegressor(\n",
        "    feature_columns=housing_features,\n",
        "    optimizer=adam_optimizer,\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "def training_input():\n",
        "  ds = tf.data.Dataset.from_tensor_slices((\n",
        "    {c: training_df[c] for c in feature_columns},  # feature map\n",
        "    training_df[target_column]                     # labels\n",
        "  ))\n",
        "  ds = ds.repeat(100)\n",
        "  ds = ds.shuffle(buffer_size=10000)\n",
        "  ds = ds.batch(10)\n",
        "  return ds\n",
        "\n",
        "linear_regressor.train(\n",
        " input_fn=training_input\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "def testing_input():\n",
        "  ds = tf.data.Dataset.from_tensor_slices((\n",
        "    {c: testing_df[c] for c in feature_columns},  # feature map\n",
        "    testing_df[target_column]                     # labels\n",
        "  ))\n",
        "  ds = ds.batch(1)\n",
        "  return ds\n",
        "\n",
        "predictions_node = linear_regressor.predict(\n",
        "  input_fn=testing_input,\n",
        ")\n",
        "\n",
        "# Convert the predctions to a NumPy array\n",
        "predicted_median_values = np.array(\n",
        "    [item['predictions'][0] for item in predictions_node])\n",
        "\n",
        "# Find the RMSE\n",
        "root_mean_squared_error = math.sqrt(\n",
        "    metrics.mean_squared_error(\n",
        "      predicted_median_values * TARGET_FACTOR,\n",
        "      testing_df[target_column] * TARGET_FACTOR\n",
        "))\n",
        "\n",
        "print(\"Root Mean Squared Error (on training data): %0.3f\" % \n",
        "root_mean_squared_error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-1-solution-1"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wSaybPnhlnSx"
      },
      "source": [
        "## Exercise 5: Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gpVHdK9Qm-SQ"
      },
      "source": [
        "The `LinearRegressor` builds a linear model with weights for each feature. Use the `get_variable_names` and `get_variable_value` methods to find the weights. Print the weights in a format similar to that shown below:\n",
        "\n",
        "```\n",
        "bias_weights 3.170546\n",
        "population -12.792054\n",
        "median_income 5.906482\n",
        "total_bedrooms 5.3723865\n",
        "households 4.3297663\n",
        "longitude -3.7551448\n",
        "latitude -3.533678\n",
        "total_rooms -2.850763\n",
        "housing_median_age 0.66154426\n",
        "```\n",
        "\n",
        "The columns are sorted by the relative impact to the formula (absolute value). Notice the bias weights in the list. This is the constant bias and should go first in the list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bLmealg79jUx"
      },
      "source": [
        "**Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZvBCdvNDovB3",
        "colab": {}
      },
      "source": [
        "# Your Code Goes Here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDwJQyTtI-f-",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-2-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KVVbRXIq9my7",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "# Load the data\n",
        "url = ('https://download.mlcc.google.com/mledu-datasets/' + \n",
        "       'california_housing_train.csv')\n",
        "\n",
        "housing_df = pd.read_csv(url)\n",
        "\n",
        "# Create lists of column names\n",
        "target_column = 'median_house_value'\n",
        "feature_columns = [c for c in housing_df.columns if c != target_column]\n",
        "\n",
        "# Normalize the feature columns\n",
        "housing_df.loc[:, feature_columns] = (\n",
        "    housing_df[feature_columns] - \n",
        "      housing_df[feature_columns].min()) / (\n",
        "          housing_df[feature_columns].max() -\n",
        "            housing_df[feature_columns].min())\n",
        "\n",
        "# Scale the target column\n",
        "TARGET_FACTOR = 100000\n",
        "housing_df[target_column] = housing_df[target_column] / TARGET_FACTOR\n",
        "\n",
        "# Test/Train split\n",
        "housing_df = housing_df.sample(frac=1)\n",
        "test_set_size = int(len(housing_df) * 0.2)\n",
        "testing_df = housing_df[:test_set_size]\n",
        "training_df = housing_df[test_set_size:]\n",
        "\n",
        "# Create TensorFlow features\n",
        "housing_features = [\n",
        "    tf.feature_column.numeric_column(c, dtype=tf.dtypes.float64) \n",
        "      for c in feature_columns\n",
        "]\n",
        "\n",
        "# Create model\n",
        "adam_optimizer = tf.keras.optimizers.Adam()\n",
        "linear_regressor = tf.estimator.LinearRegressor(\n",
        "    feature_columns=housing_features,\n",
        "    optimizer=adam_optimizer,\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "def training_input():\n",
        "  ds = tf.data.Dataset.from_tensor_slices((\n",
        "    {c: training_df[c] for c in feature_columns},  # feature map\n",
        "    training_df[target_column]                     # labels\n",
        "  ))\n",
        "  ds = ds.repeat(100)\n",
        "  ds = ds.shuffle(buffer_size=10000)\n",
        "  ds = ds.batch(10)\n",
        "  return ds\n",
        "\n",
        "linear_regressor.train(\n",
        " input_fn=training_input\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "def testing_input():\n",
        "  ds = tf.data.Dataset.from_tensor_slices((\n",
        "    {c: testing_df[c] for c in feature_columns},  # feature map\n",
        "    testing_df[target_column]                     # labels\n",
        "  ))\n",
        "  ds = ds.batch(1)\n",
        "  return ds\n",
        "\n",
        "predictions_node = linear_regressor.predict(\n",
        "  input_fn=testing_input,\n",
        ")\n",
        "\n",
        "# Convert the predctions to a NumPy array\n",
        "predicted_median_values = np.array(\n",
        "    [item['predictions'][0] for item in predictions_node])\n",
        "\n",
        "# Find the RMSE\n",
        "root_mean_squared_error = math.sqrt(\n",
        "    metrics.mean_squared_error(\n",
        "      predicted_median_values * TARGET_FACTOR,\n",
        "      testing_df[target_column] * TARGET_FACTOR\n",
        "))\n",
        "\n",
        "print(\"Root Mean Squared Error (on training data): %0.3f\" % \n",
        "root_mean_squared_error)\n",
        "\n",
        "w = {}\n",
        "for v in linear_regressor.get_variable_names():\n",
        "  if v.startswith('linear/linear_model/') and v.endswith('/weights'):\n",
        "    _, _, name, _ = v.split('/')   \n",
        "    w[name] = linear_regressor.get_variable_value(v)[0][0]\n",
        "  elif v.startswith('linear/linear_model/'):\n",
        "    _, _, name = v.split('/')\n",
        "    print(name, linear_regressor.get_variable_value(v)[0])\n",
        "\n",
        "for k, v in sorted(w.items(), key=lambda i: abs(i[1]), reverse=True):\n",
        "  print(k, v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2rT20XcM3lF",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    }
  ]
}