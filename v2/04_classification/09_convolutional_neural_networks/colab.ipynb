{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional Neural Networks",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "axhMVhTdjZ-7",
        "exercise-9-key-1",
        "a85YjVkkkOep"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/v2/04_classification/09_convolutional_neural_networks/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axhMVhTdjZ-7",
        "colab_type": "text"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpiN4SUKjbOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-ivEsaENhCB",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqPpkBh3Nnct",
        "colab_type": "text"
      },
      "source": [
        "Convolutional Neural Networks (CNN) are deep neural networks with the addition of two very special types of layers: **convolutional layers** and **pooling layers**. In this lesson, we will take a look at both."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBKkcm6ROLLn",
        "colab_type": "text"
      },
      "source": [
        "## Convolutional Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "537m9PSIONyR",
        "colab_type": "text"
      },
      "source": [
        "Convolutional layers are layers in a neural network that only partially connect to their input layers. The layer is divided into receptive fields that only look at a portion of the input layer and apply filters to it.\n",
        "\n",
        "Let's see this in action. First, we will create a 100 x 100 x 3 image that contains red vertical stripes centered every 10 pixels on the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxJ9X6YaH6ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create an image that is completely black.\n",
        "vertical_stripes = np.zeros((100, 100, 3))\n",
        "\n",
        "# Loop over the image 10 pixels at a time, turning the center line of vertial\n",
        "# pixels red.\n",
        "for x in range(4, 101, 10):\n",
        "  vertical_stripes[:, x:x+2, 0] = 1.0\n",
        "\n",
        "_ = plt.imshow(vertical_stripes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0uXETOLP6Jc",
        "colab_type": "text"
      },
      "source": [
        "We will now create a filter that we'll apply using TensorFlow's [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) function.\n",
        "\n",
        "For illustrative purposes, we'll create a filter to extract the red out of the image we just created. The filter will be 10 x 10 x 3. (Remember that our vertical red lines are centered every 10 pixels and that our image has RGB values.) The final number in the filter (1) is the number of output channels we'd like the filter to produce. These output channels are called \"feature maps\". You get one feature map per filter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGs7yR7GLFz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "receptor_height, receptor_width = 10, 10\n",
        "input_color_channels, output_color_channels = 3, 1\n",
        "\n",
        "filters = np.zeros(shape=(receptor_height, receptor_width, input_color_channels,\n",
        "                          output_color_channels), dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-APXHrDtRNRy",
        "colab_type": "text"
      },
      "source": [
        "We created our filter and set it to all zeros. We now need to indicate what portion of the receptor field we want to extract data from. In this case we are trying to extract the vertical red line which we know is centered every ten pixels (pixels 5 and 6). To capture the red line we'll tell the filter that we only care about the 5th and 6th pixel in every row of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL9L8RlsRMho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filters[:, 5:7, :, 0] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze9FVE1VSBt_",
        "colab_type": "text"
      },
      "source": [
        "Let's now get our image ready to pass to our convolutional layer. To do that we package the 3-dimensional image in yet another array to create a dataset for TensorFlow. TensorFlow's convolutional function expects a 4-dimensional dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxp8DjcxSKon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = np.array([vertical_stripes], dtype=np.float32)\n",
        "image_count, image_height, image_width, color_channels = dataset.shape\n",
        "\n",
        "image_count, image_height, image_width, color_channels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS8Vmt-5TAgG",
        "colab_type": "text"
      },
      "source": [
        "To get the image into TensorFlow we need to convert it into a Tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y19a0mrfS-cy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "X = tf.convert_to_tensor(dataset, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rnO7icvTkYm",
        "colab_type": "text"
      },
      "source": [
        "To create our convolutional layer we use [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). The arguments we are passing it are:\n",
        "\n",
        "*  The image that we are processing.\n",
        "*  The filters that we want to apply to the data. In this case we are passing in the filter that will capture the middle vertical pixels in a 10x10 receptor.\n",
        "*  The strides we want the layer to take when operating on the data. In this case we want the input data to be processed for every image and every color channel. The 10s cause the receptor to shift by 10 pixels every vertical and horizontal step through the image. This is exactly our filter size and allows us to stay centered on the red vertical lines. In practice you'd likely want some overlap.\n",
        "*  A padding argument that we input as \"SAME\" which causes TensorFlow to pad the image if necessary (equal padding on each size) in order to make the filter process the entire image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAVxgudyTjtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "convolution = tf.nn.conv2d(X, filters, strides=[1, 10, 10, 1], padding=\"SAME\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbdSrKvCUmob",
        "colab_type": "text"
      },
      "source": [
        "We can now run our convolutional layer using a TensorFlow session.\n",
        "\n",
        "Notice that our output shape reduces the input image to a 10 x 10 x 1 matrix from a 100 x 100 x 3 matrix. This is because we processed the image using a 10 x 10 single-channel-output filter and stepped 10 pixels each time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwRTfa2WLham",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = convolution.numpy()\n",
        "output.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnS92oLHWK2u",
        "colab_type": "text"
      },
      "source": [
        "Looking at the image isn't very telling. It simply looks like a single-color image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a16GKcd9M4_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(output[0, :, :, 0 ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynnmYDmDWTdQ",
        "colab_type": "text"
      },
      "source": [
        "When we look at the data, we can see that the values are uniformly 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8Q-xZ0_WSad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.unique(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anfmmmNtYAWQ",
        "colab_type": "text"
      },
      "source": [
        "What happens if we include some black pixels by increasing our vertical filter to capture the four vertical pixels in the center? Our output number changes to 20.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LEydsmQXepS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filters = np.zeros(shape=(receptor_height, receptor_width, input_color_channels,\n",
        "                          output_color_channels), dtype=np.float32)\n",
        "filters[:, 4:8, :, :] = 1\n",
        "\n",
        "X = tf.convert_to_tensor(dataset)\n",
        "convolution = tf.nn.conv2d(X, filters, strides=[1,10,10,1], padding=\"SAME\")\n",
        "\n",
        "output = convolution.numpy()\n",
        "\n",
        "np.unique(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wqz32nZMYk7-",
        "colab_type": "text"
      },
      "source": [
        "If we move our filter to only capture black pixels our output becomes 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH3g9ZFiXyDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filters = np.zeros(shape=(receptor_height, receptor_width, input_color_channels,\n",
        "                          output_color_channels), dtype=np.float32)\n",
        "filters[:, :2, :, :] = 1\n",
        "\n",
        "X = tf.convert_to_tensor(dataset)\n",
        "convolution = tf.nn.conv2d(X, filters, strides=[1,10,10,1], padding=\"SAME\")\n",
        "\n",
        "output = convolution.numpy()\n",
        "\n",
        "np.unique(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDDOOkvsZLRG",
        "colab_type": "text"
      },
      "source": [
        "Let's look at a convolutional layer on a real image. We'll load a sample image from scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoIJALbzZP48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_sample_image\n",
        "\n",
        "china = load_sample_image('china.jpg')\n",
        "\n",
        "plt.imshow(china)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsVJwfWcZWFz",
        "colab_type": "text"
      },
      "source": [
        "We will package the image in a 4-dimensional matrix for processing by TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqzNI8cwZb4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = np.array([china], dtype=np.float32)\n",
        "image_count, image_height, image_width, color_channels = dataset.shape\n",
        "\n",
        "image_count, image_height, image_width, color_channels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdL-m32vZh5a",
        "colab_type": "text"
      },
      "source": [
        "Let's re-create our vertical line filter and apply it to the image to see the convolutional layer in action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiOxub1oY_Pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "receptor_height, receptor_width = 10, 10\n",
        "input_color_channels, output_color_channels = 3, 1\n",
        "filters = np.zeros(shape=(receptor_height, receptor_width, input_color_channels,\n",
        "                          output_color_channels), dtype=np.float32)\n",
        "filters[:, 5:7, :, :] = 1\n",
        "\n",
        "image_count, image_height, image_width, color_channels = dataset.shape\n",
        "X = tf.convert_to_tensor(dataset)\n",
        "\n",
        "convolution = tf.nn.conv2d(X, filters, strides=[1,4,4,1], padding=\"SAME\")\n",
        "\n",
        "output = convolution.numpy()\n",
        "\n",
        "plt.imshow(output[0, :, :, 0], cmap=\"gray\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh-uhsWqcfUD",
        "colab_type": "text"
      },
      "source": [
        "Typically you won't define your own filters though. You can let TensorFlow discover them by using [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d) instead of [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d).\n",
        "\n",
        "In this example we ask for three features with a 5x5 visual receptor stepping two pixels at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufZGmFneaTfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_count, image_height, image_width, color_channels = dataset.shape\n",
        "X = tf.convert_to_tensor(dataset)\n",
        "\n",
        "convolution = tf.keras.layers.Conv2D(filters=3, kernel_size=5, strides=[2,2],\n",
        "                               padding=\"SAME\")\n",
        "\n",
        "output = convolution(X)\n",
        "output = output.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVRW0OmFjJds",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the first feature map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVy5abuBjIIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(output[0, :, :, 0])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiKLRKzui1Qz",
        "colab_type": "text"
      },
      "source": [
        "Here is the second feature map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD7LNVBRiw4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(output[0, :, :, 1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN0iB6JNi4Ox",
        "colab_type": "text"
      },
      "source": [
        "And the third."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoLh4DO4iy74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(output[0, :, :, 2])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc3X_R2ac_g9",
        "colab_type": "text"
      },
      "source": [
        "## Pooling Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri1rpacidD3k",
        "colab_type": "text"
      },
      "source": [
        "Pooling layers are used to shrink the data from their input layer by sampling the data per receptor. Let's look at an example. We'll first load a sample image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJiUonz2dmnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flower = load_sample_image('flower.jpg')\n",
        "\n",
        "plt.imshow(flower)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsFrv0BCdrPX",
        "colab_type": "text"
      },
      "source": [
        "We can package this image in a 4-dimensional matrix and pass it to the [tf.nn.max_pool](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool) function. This function extracts the maximum value from each receptor field.\n",
        "\n",
        "In the example below we create a 2 x 2 receptor and move it around the image shifting 2 pixels each time. This reduces the height and width of the image by half, effectively reducing our dataset size by 75%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_8YirYZswEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = np.array([flower], dtype=np.float32)\n",
        "\n",
        "X = tf.convert_to_tensor(dataset)\n",
        "max_pool = tf.nn.max_pool(X, ksize=[1,2,2,1], strides=[1,2,2,1],\n",
        "                          padding=\"VALID\")\n",
        "\n",
        "output = max_pool.numpy()\n",
        "\n",
        "plt.imshow(output[0].astype(np.uint8))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWwPocAWentg",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 1: Manual Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD9H1pj9vK9z",
        "colab_type": "text"
      },
      "source": [
        "Use [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) to apply a stack of filters to the scikit-learn built-in flower image mentioned earlier in this colab.\n",
        "\n",
        "* Create a (7, 7, 3, 2) filter set. The `2` at the end indicates that we'll create two filters and get two output channels (feature maps).\n",
        "* Make the first filter be a vertical line filter on the middle pixel of each row.\n",
        "* Make the second filter be a horizontal line filter on the middle pixel of each row.\n",
        "* Pass the flower image and filters to [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d), step 3 pixels vertically and horizontally.\n",
        "* Display the first feature map as an image.\n",
        "* Display the second feature map as an image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QK1tz626LjZU"
      },
      "source": [
        "**Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlBg4KCAgumu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create your filters and apply them to the flower image using TensorFlow here.\n",
        "\n",
        "# Use PyPlot to output the first feature map here.\n",
        "\n",
        "# Use PyPlot to output the second feature map here."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-9-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N-RroA16LsCm",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "flower = load_sample_image('flower.jpg')\n",
        "\n",
        "receptor_height, receptor_width = 7, 7\n",
        "input_color_channels, output_color_channels = 3, 2\n",
        "filters = np.zeros(shape=(receptor_height, receptor_width, input_color_channels,\n",
        "                          output_color_channels), dtype=np.float32)\n",
        "filters[:, 3:4, :, 0] = 1\n",
        "filters[3:4, :, :, 1] = 1\n",
        "\n",
        "X = tf.convert_to_tensor([flower], dtype=tf.float32)\n",
        "\n",
        "convolution = tf.nn.conv2d(X, filters, strides=[1,3,3,1], padding=\"SAME\")\n",
        "\n",
        "output = convolution.numpy()\n",
        "\n",
        "plt.imshow(output[0, :, :, 0], cmap=\"gray\")\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(output[0, :, :, 1], cmap=\"gray\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFy6iuhLuh8j",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3WIUe0E9449",
        "colab_type": "text"
      },
      "source": [
        "## Buliding a CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmstsPo997_3",
        "colab_type": "text"
      },
      "source": [
        "Now that we have learned about the component parts of a convolutional neural network, let's actually build one.\n",
        "\n",
        "In this section we will use the [Diabetic Retinopathy 224x224 Gaussian Filtered\n",
        "](https://www.kaggle.com/sovitrath/diabetic-retinopathy-224x224-gaussian-filtered) dataset that is hosted on Kaggle.\n",
        "\n",
        "Upload your `kaggle.json` file and run the code below to download the file with the Kaggle API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgG0xYRd_VcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! chmod 600 kaggle.json && (ls ~/.kaggle 2>/dev/null || mkdir ~/.kaggle) && mv kaggle.json ~/.kaggle/ && echo 'Done'\n",
        "! kaggle datasets download sovitrath/diabetic-retinopathy-224x224-gaussian-filtered\n",
        "! ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REwLrScD_p5M",
        "colab_type": "text"
      },
      "source": [
        "The image file is `diabetic-retinopathy-224x224-gaussian-filtered.zip`. Let's unzip it and inspect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZe_vKWN_tKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "zipfile.ZipFile('diabetic-retinopathy-224x224-gaussian-filtered.zip').extractall()\n",
        "os.listdir('./gaussian_filtered_images/gaussian_filtered_images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4WFEhm0AuhS",
        "colab_type": "text"
      },
      "source": [
        "We have listed the unzipped directory. Inside there are five folders relating to the five classes of diabetic retinopathy in the dataset:\n",
        "\n",
        "* No_DR\n",
        "* Mild\n",
        "* Moderate\n",
        "* Severe\n",
        "* Proliferate_DR\n",
        "\n",
        "The images have already been passed through a filter and resized to `224` by `224` pixels, which makes oure task a bit easier.\n",
        "\n",
        "Let' see how many images exist in each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzCcISG_Atzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        " \n",
        "root = './gaussian_filtered_images/gaussian_filtered_images/'\n",
        "for directory, _, files in os.walk(root):\n",
        "  cnt = 0\n",
        "  for file_name in files:\n",
        "    if file_name.endswith('png'):\n",
        "      cnt += 1\n",
        "  if cnt > 0:\n",
        "    print(directory, cnt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6F81UJE_W7Z",
        "colab_type": "text"
      },
      "source": [
        "As you can see, we have about `1800` images of cases with no issues and roughly the same amount spread over the different stages of the disease.\n",
        "\n",
        "Let's look at one of the images in each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMvIGXoDE0wM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "samples = {}\n",
        "root = './gaussian_filtered_images/gaussian_filtered_images/'\n",
        "for directory, _, files in os.walk(root):\n",
        "  cnt = 0\n",
        "  for file_name in files:\n",
        "    if file_name.endswith('png'):\n",
        "      cnt += 1\n",
        "  if cnt > 0:\n",
        "    samples[directory] = file_name\n",
        "\n",
        "fig, axs = plt.subplots(len(samples))\n",
        "fig.set_figheight(25)\n",
        "fig.set_figwidth(5)\n",
        "\n",
        "subplot = 0\n",
        "for directory, image_path in samples.items():\n",
        "  img = cv.imread(os.path.join(directory, image_path))\n",
        "  axs[subplot].title.set_text(directory)\n",
        "  axs[subplot].imshow(img)\n",
        "  subplot += 1\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1WXz_S8Hd-Z",
        "colab_type": "text"
      },
      "source": [
        "You can likely see the difference in the different images. We didn't order these images, but if you follow the disease progression:\n",
        "\n",
        "* No_DR\n",
        "* Mild\n",
        "* Moderate\n",
        "* Severe\n",
        "* Proliferate_DR\n",
        "\n",
        "You should see an increasing amount of discoloration in the scans."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0-JhkdKH3GU",
        "colab_type": "text"
      },
      "source": [
        "We now need to find a way to get the images into the model. TensorFlow Keras has a class called [`DirectoryIterator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/DirectoryIterator) that can help with that.\n",
        "\n",
        "The iterator pulls images from a directory and passes them to our model in batches. There are many settings that we can change. In our example we set the `target_size` to the size of our input images. Notice that we don't provide a third dimension even though these are RGB files. This is because the default `color_mode` is `'rgb'`, which implies three values.\n",
        "\n",
        "We also set `image_data_generator` to `None`. If we would have wanted, we could have passed an [`ImageDataGenerator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) to augment the image (and increase the size of our dataset). We'll save this for an exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyPRFN8_H5o-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_dir = 'gaussian_filtered_images/gaussian_filtered_images'\n",
        "\n",
        "train_image_iterator = tf.keras.preprocessing.image.DirectoryIterator(\n",
        "    target_size=(224, 224),\n",
        "    directory=train_dir,\n",
        "    image_data_generator=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_vxVJVnPiMs",
        "colab_type": "text"
      },
      "source": [
        "The output for the code above notes that `3662` images were found across `5`. Classes. These classes are the directories that were in our root folder. They are sorted, so the actual value of the classes are:\n",
        "\n",
        "* 0 - Mild\n",
        "* 1 - Moderate\n",
        "* 2 - No_DR\n",
        "* 3 - Proliferate_DR\n",
        "* 4 - Severe\n",
        "\n",
        "We can validate that using the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEYbHTgEZVov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_image_iterator.filepaths[np.where(train_image_iterator.labels == 0)[0][0]])\n",
        "print(train_image_iterator.filepaths[np.where(train_image_iterator.labels == 1)[0][0]])\n",
        "print(train_image_iterator.filepaths[np.where(train_image_iterator.labels == 2)[0][0]])\n",
        "print(train_image_iterator.filepaths[np.where(train_image_iterator.labels == 3)[0][0]])\n",
        "print(train_image_iterator.filepaths[np.where(train_image_iterator.labels == 4)[0][0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "353M9kccQZiY",
        "colab_type": "text"
      },
      "source": [
        "Let's build our model now. We'll use the [`Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) and [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) classes that we've used in many previous labs, as well as a few new classes new to this lab:\n",
        "\n",
        "* [`Conv2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) which creates a convolutional layer.\n",
        "* [`MaxPool2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPooling2D) which creates a pooling layer.\n",
        "* [`Flatten`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) which creates a layer that converts a multidimensional tensor down to a flat tensor.\n",
        "\n",
        "You can see the entire model below. We input our images into a convolutional layer followed by a pooling layer. After stacking a few convolutional layers and pooling layers we flatten the final pooling output and finish with some traditional dense layers. The final dense layer is 5 nodes wide and is activated by softmax. This layer represents our classification predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyiyMgh5MSRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu',\n",
        "                           input_shape=(224, 224 ,3)),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUnosERsR5iC",
        "colab_type": "text"
      },
      "source": [
        "Now let's start training. Let one or two epochs run but then **!!!! STOP THE CELL FROM RUNNING !!!!**\n",
        "\n",
        "How long was each epoch taking? Ours was taking about `4` minutes. Let's do the math. If each epoch took `4` minutes and we ran `100` epochs then we'd be training for `400` minutes. That's just under `7` hours of training!\n",
        "\n",
        "Luckily, there is a better way. In the menu click on 'Runtime' and then 'Change runtime type'. In the modal that appears there is an option called 'Hardware accelerator' that is set to 'None'. Change this to 'GPU' and save your settings.\n",
        "\n",
        "You're runtime will change, so you'll need to go back to the start of this section and run all of the cells from the start. Don't forget to upload your `kaggle.json` again.\n",
        "\n",
        "When you get back to this cell a second time and start it running you should notice a big improvement in training time. We were getting `9` seconds per epoch, which is about `900` seconds total. This totals `15` minutes, which is much better. Let the cell run to completion (hopefully about `15` mintues). You should see it progressing as it is running."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMidmU_EMUTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(\n",
        "    train_image_iterator,\n",
        "    epochs=100,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a57P5HKHUK_r",
        "colab_type": "text"
      },
      "source": [
        "You might have noticed that each epoch only processed `115` items. These are batches, not images. The [`DirectoryIterator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/DirectoryIterator) defaults to a batch size of `32`. We have `3662` images. `3662 / 32 = 114.4375`, which explains the `115` number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNzOc52MUo4L",
        "colab_type": "text"
      },
      "source": [
        "Now let's plot our training accuracy over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL0IZZjYSl6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(list(range(len(history.history['accuracy']))),\n",
        "         history.history['accuracy'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-HlbpSdUskN",
        "colab_type": "text"
      },
      "source": [
        "And our loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ova5HkZhSnYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(list(range(len(history.history['loss']))), history.history['loss'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOpyloTfUvEV",
        "colab_type": "text"
      },
      "source": [
        "And finally we can use our model to make predictions. Let's reuse the `samples` that we had earlier in order to get a glimpse at each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJYaW1x3U9vf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "classes = ['Mild', 'Moderate', 'No_DR', 'Proliferate_DR', 'Severe']\n",
        "\n",
        "for directory, file in samples.items():\n",
        "  path = os.path.join(directory, file)\n",
        "  img = cv.imread(path)\n",
        "  img_tensor = tf.convert_to_tensor([img])\n",
        "  prediction = classes[np.argmax(model.predict(img_tensor))]\n",
        "  actual = directory[directory.rindex('/') + 1:]\n",
        "  correct = prediction==actual\n",
        "  print(f'prediction: {prediction}; actual: {actual}, correct?: {correct}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WriJWpQeX6gF",
        "colab_type": "text"
      },
      "source": [
        "We got `2` out of `5` correct. Your results may vary.\n",
        "\n",
        "Overall the model seemed to train well, but still missed some of our predictions. We'll try to address this in the excercise below by augmenting our images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc6jl5bWaneu",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2: `ImageDataGenerator`\n",
        "\n",
        "Re-create the model above using an [`ImageDataGenerator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) to augment the training data set. When running `fit` be sure to pay attention to the `steps_per_epoch` parameter. It defaults to unbounded and the generator just keeps on generating if you don't set it.\n",
        "\n",
        "Notice that we didn't split off a validation hold-out set of data. This is a bad practice. We skipped it here just because we've done it so many times in the course. Typically the way to do this with images is to split some of the images into a separate directory structure.\n",
        "\n",
        "In our case we'd have:\n",
        "\n",
        "```\n",
        "training/\n",
        "  |--> Mild/\n",
        "  |--> Moderate/\n",
        "  |--> No_DR/\n",
        "  |--> Proliferate_DR/\n",
        "  |--> Severe/\n",
        "validation/\n",
        "  |--> Mild/\n",
        "  |--> Moderate/\n",
        "  |--> No_DR/\n",
        "  |--> Proliferate_DR/\n",
        "  |--> Severe/\n",
        "```\n",
        "\n",
        "And would randomly put some images in the `validation` substructure.\n",
        "\n",
        "Then we'd then create two [`DirectoryIterator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/DirectoryIterator) objects. One for training and one for validation.\n",
        "\n",
        "If we were performing augmentation we'd set the `subset` parameter to `'validation'` for the validation iterator to ensure that the iterator didn't perform augmentation on the validation data and to `'training'` for the other. Alternatively, you can use the [`ImageDataGenerator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) directly for the training data and use a `DirectoryIterator` like we did above for the validation data.\n",
        "\n",
        "For this exercise, hold `10%` of the data out as a validation set.\n",
        "\n",
        "When you have finished training your model visualize your training loss. \n",
        "\n",
        "Next, use the model to make predictions and then calculate the F1 score of your validation results.\n",
        "\n",
        "Explain your work.\n",
        "\n",
        "*Use as many code blocks and text blocks as necessary below.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teJ-Y617kRlg",
        "colab_type": "text"
      },
      "source": [
        "**Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0FfIkINkUJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW_OcgWukNEw",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a85YjVkkkOep",
        "colab_type": "text"
      },
      "source": [
        "#### Answer Key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_ufh2yKkanw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "! chmod 600 kaggle.json && (ls ~/.kaggle 2>/dev/null || mkdir ~/.kaggle) && mv kaggle.json ~/.kaggle/ && echo 'Done'\n",
        "! kaggle datasets download sovitrath/diabetic-retinopathy-224x224-gaussian-filtered\n",
        "\n",
        "zipfile.ZipFile('diabetic-retinopathy-224x224-gaussian-filtered.zip').extractall()\n",
        "os.listdir('./gaussian_filtered_images/gaussian_filtered_images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE3aDCdcky7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        " \n",
        "root = './gaussian_filtered_images/gaussian_filtered_images/'\n",
        "\n",
        "# Make training and validation directories\n",
        "training_dir = './training'\n",
        "if os.path.exists(training_dir):\n",
        "  shutil.rmtree(training_dir)\n",
        "os.mkdir(training_dir)\n",
        "\n",
        "validation_dir = './validation'\n",
        "if os.path.exists(validation_dir):\n",
        "  shutil.rmtree(validation_dir)\n",
        "os.mkdir(validation_dir)\n",
        "\n",
        "# Copy the files over into the new directories\n",
        "for directory, _, files in os.walk(root):\n",
        "  png_files = []\n",
        "  for file_name in files:\n",
        "    if file_name.endswith('png'):\n",
        "      png_files.append(file_name)\n",
        "  if cnt > 0:\n",
        "    random.shuffle(png_files)\n",
        "    validation_cnt = int(len(png_files) * 0.1)\n",
        "\n",
        "    base, dir_name = (directory[:directory.rindex('/')], \n",
        "                      directory[directory.rindex('/') + 1:])\n",
        "\n",
        "    for file in png_files[:validation_cnt]:\n",
        "      dest = os.path.join(validation_dir, dir_name)\n",
        "      if not os.path.exists(dest):\n",
        "        os.mkdir(dest)\n",
        "      dest = os.path.join(dest, file)\n",
        "      source = os.path.join(directory, file)\n",
        "      shutil.copyfile(source, dest)\n",
        "    \n",
        "    for file in png_files[validation_cnt:]:\n",
        "      dest = os.path.join(training_dir, dir_name)\n",
        "      if not os.path.exists(dest):\n",
        "        os.mkdir(dest)\n",
        "      dest = os.path.join(dest, file)\n",
        "      source = os.path.join(directory, file)\n",
        "      shutil.copyfile(source, dest)\n",
        "\n",
        "# Make sure that the count of images in each folder look correct\n",
        "for root in (validation_dir, training_dir):\n",
        "  for directory, _, files in os.walk(root):\n",
        "    cnt = 0\n",
        "    for file_name in files:\n",
        "      if file_name.endswith('png'):\n",
        "        cnt += 1\n",
        "    if cnt > 0:\n",
        "      print(directory, cnt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTQgKoxPqeRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=(224, 224 ,3)),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4Ro-5_Bqqg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_data_gen = ImageDataGenerator().flow_from_directory(\n",
        "    directory=training_dir,\n",
        "    target_size=(224, 224))\n",
        "\n",
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    epochs=200,\n",
        "    steps_per_epoch=100\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWERWrUBsnb-",
        "colab_type": "text"
      },
      "source": [
        "Once training is complete, we can plot accuracy across time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLPhYhULsrn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(list(range(len(history.history['accuracy']))),\n",
        "         history.history['accuracy'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIVYcdRAsqzN",
        "colab_type": "text"
      },
      "source": [
        "And loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jvgkJ52syGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(list(range(len(history.history['loss']))),\n",
        "         history.history['loss'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzbXHvajs1TW",
        "colab_type": "text"
      },
      "source": [
        "Finally, we want to make predictions using our validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypK8YAYDtARx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_image_iterator = tf.keras.preprocessing.image.DirectoryIterator(\n",
        "    target_size=(224, 224),\n",
        "    directory=validation_dir,\n",
        "    image_data_generator=None)\n",
        "\n",
        "predictions = model.predict(validation_image_iterator)\n",
        "\n",
        "predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObrwHgQnzfYX",
        "colab_type": "text"
      },
      "source": [
        "And print the F1 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHc1eewfxwCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "classes = ['Mild', 'Moderate', 'No_DR', 'Proliferate_DR', 'Severe']\n",
        "\n",
        "actual_classes = validation_image_iterator.classes\n",
        "predicted_classes = [np.argmax(prediction) for prediction in predictions]\n",
        "\n",
        "f1_score(actual_classes, predicted_classes, average='micro')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7ESAHYekXCD",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    }
  ]
}