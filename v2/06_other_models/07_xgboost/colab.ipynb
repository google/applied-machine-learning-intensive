{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XGBoost",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "0_pfCiSlWvE4",
        "VAeNiodRWKII"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/v2/06_other_models/07_xgboost/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_pfCiSlWvE4",
        "colab_type": "text"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5rgB9h6Wtor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6KkIPwzW2xw",
        "colab_type": "text"
      },
      "source": [
        "# XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enHv5jR5b77s",
        "colab_type": "text"
      },
      "source": [
        "[XGBoost](https://github.com/dmlc/xgboost) is a powerful toolkit for performing machine learning tasks. Though much of the current excitement in the machine learning sphere comes from deep learning, XGBoost is a new non-deep-learning algorithm that has been winning data science competitions and getting a lot of attention. Instead of a deep neural network, XGBoost uses a collection of decision trees (random forest) arranged and optimized by a boosting scheme and unique penalization of trees in the forest.\n",
        "\n",
        "XGBoost started becoming popular as it started to perform as well as, if not better than, modern deep learning approaches, while also using less computing resources to train."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkYifsXHVZEo",
        "colab_type": "text"
      },
      "source": [
        "## The Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmdXSpRUdXlJ",
        "colab_type": "text"
      },
      "source": [
        "To demonstrate XGBoost, we will use a [subset of data](https://www.kaggle.com/joshmcadams/sdss-16) from the [SDSS data release 16](https://www.sdss.org/dr16/).\n",
        "This dataset contains data observed about objects in space, as well as the type of object that was observed. We'll see if we can train a model that can classify objects based on these observations.\n",
        "\n",
        "To get started, upload your `kaggle.json` file and run the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U--f_mHTbZvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! chmod 600 kaggle.json && (ls ~/.kaggle 2>/dev/null || mkdir ~/.kaggle) && mv kaggle.json ~/.kaggle/ && echo 'Done'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4HQm-nNd6Ti",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll download the `sdss-16` dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYm2woKTbex7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! kaggle datasets download joshmcadams/sdss-16\n",
        "! ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-ZTTBrXd-A2",
        "colab_type": "text"
      },
      "source": [
        "And then load it up and get some information about it. Notice that we use `.info()` to examine the `DataFrame`. The `.info()` method returns information about the data, including column counts and even memory usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLLBqvIwblVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('sdss-16.zip')\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8G2hEw0oktT",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z95utBqToodM",
        "colab_type": "text"
      },
      "source": [
        "We can tell from our `.info()` call that we have `732,977` non-null columns. First we should double-check that every column is non-null."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXTjVKKqoke4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E12hG5aGo2bU",
        "colab_type": "text"
      },
      "source": [
        "Good, no null data. Let's examine the columns that we are working with.\n",
        "\n",
        "Column | Description\n",
        "-------|-------------\n",
        "`object_id`            | Unique SDSS identifier composed from [skyVersion,rerun,run,camcol,field,obj].\n",
        "`right_ascension`      | J2000 right ascension (r')\n",
        "`declination`          | J2000 declination (r')\n",
        "`u_magnitude`          | Model magnitude u-band\n",
        "`g_magnitude`          | Model magnitude g-band\n",
        "`r_magnitude`          | Model magnitude r-band\n",
        "`i_magnitude`          | Model magnitude i-band\n",
        "`z_magnitude`          | Model magnitude z-band\n",
        "`obs_run_number`       | Run number\n",
        "`rerun_number`         | Re-run number\n",
        "`camera_column`        | Camera column\n",
        "`field_number`         | Field number\n",
        "`spectro_object_id`    | Unique ID\n",
        "`class`                | Type of object (target)\n",
        "`redshift`             | Final redshift\n",
        "`plate_id`             | Plate ID\n",
        "`observation_date`     | Date of observation\n",
        "`fiber_id`             | Fiber ID\n",
        "\n",
        "The column that we'll be trying to predict is `class`.\n",
        "\n",
        "There are quite a few columns that seem to be more metadata than observation data, so we'll not include them in our features. Good candidates for this catagory include:\n",
        "\n",
        "* `object_id`\n",
        "* `obs_run_number`\n",
        "* `rerun_number`\n",
        "* `observation_date`\n",
        "* `spectro_object_id`\n",
        "\n",
        "Then there are columns that we need to understand better before we can decide whether we should include them or not. These include:\n",
        "\n",
        "* `camera_column`\n",
        "* `field_number`\n",
        "* `plate_id`\n",
        "* `fiber_id`\n",
        "\n",
        "Looking at the [SDSS DR16 Glossary](https://www.sdss.org/dr16/help/glossary/), these fields should likely not be used for classification either.\n",
        "\n",
        "That leaves us with the following potential feature columns:\n",
        "\n",
        "* `right_ascension`\n",
        "* `declination`\n",
        "* `u_magnitude`\n",
        "* `g_magnitude`\n",
        "* `r_magnitude`\n",
        "* `i_magnitude`\n",
        "* `z_magnitude`\n",
        "* `redshift`\n",
        "\n",
        "[Researching more](https://skyserver.sdss.org/dr16/en/proj/advanced/hubble/simple.aspx), it seems that all of these columns are real observations that might be useful to our model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfKooQFts09b",
        "colab_type": "text"
      },
      "source": [
        "Now that we know our target and features, let's take a look at them.\n",
        "\n",
        "First, we'll see how balanced our dataset is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBlEIGkntK0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['class'].hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B3YQSFPtRYw",
        "colab_type": "text"
      },
      "source": [
        "Not really balanced. We have less than `100,000` quasars and over `350,000` galaxies. We'll need to stratify our testing split for sure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Auo5JfcwtpLB",
        "colab_type": "text"
      },
      "source": [
        "Let's see how our features are distributed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C98wJMPuFQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "features = [\n",
        "  'right_ascension',\n",
        "  'declination',\n",
        "  'u_magnitude',\n",
        "  'g_magnitude',\n",
        "  'r_magnitude',\n",
        "  'i_magnitude',\n",
        "  'z_magnitude',\n",
        "  'redshift',\n",
        "]\n",
        "\n",
        "fig, (axs) = plt.subplots(1, 8, figsize=(20, 10))\n",
        "for i in range(len(features)):\n",
        "  axs[i].title.set_text(features[i])\n",
        "  axs[i].boxplot(df[features[i]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1FobMTivy8X",
        "colab_type": "text"
      },
      "source": [
        "Wow, that is some spread. Ranges vary from single digits (ex. `redshift`) to large negatives (ex. `i_magnitude` and `z_magnitude`). Some values are relatively normally distributed like `declination`, while others are extremely focused with a wide range of outliers like in `redshift` and `u_magnitude`.\n",
        "\n",
        "Since we'll be using an XGBoost model backed by decision trees, we won't need to normalize this data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O8xkmgpwhal",
        "colab_type": "text"
      },
      "source": [
        "## The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR0sFenR7Uif",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYvMo2OlwjnM",
        "colab_type": "text"
      },
      "source": [
        "First off, we'll want to convert our target `class` to a numeric value instead of the string values that we currently have. To do that we'll use the [sklearn `LabelEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html). This encodes our three classes as the values `0`, `1`, and `2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cENCWJOTwrKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['encoded_class'] = df[['class']].apply(le.fit_transform)['class']\n",
        "_ = df['encoded_class'].hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naQfh5V-xHUy",
        "colab_type": "text"
      },
      "source": [
        "We should then split the data, being sure to stratify."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKSRVYEixJhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df[features],\n",
        "    df['encoded_class'],\n",
        "    test_size=0.2,\n",
        "    random_state=8675309,\n",
        "    shuffle=True,\n",
        "    stratify=df['encoded_class'])\n",
        "\n",
        "_ = y_train.hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtRVevzi7X7Q",
        "colab_type": "text"
      },
      "source": [
        "### Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jxOucDHxfbF",
        "colab_type": "text"
      },
      "source": [
        "Now it is time to actually build the model. It is possible to build an XGBoost model in a similar manner as you would a standard scikit-learn model. For example, to create a classifier we can use `XGBClassifer`:\n",
        "\n",
        "```python\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "*But you will likely regret that decision!*\n",
        "\n",
        "One of the strengths of XGBoost it its ability to quickly converge on a quality model. In order to do this, we need to set up an input data structure that is optimized for XGBoost processing. This structure is known as the `DMatrix`.\n",
        "\n",
        "To create a `DMatrix`, we load `xgboost` and pass our testing and training data to the `DMatrix` constructor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-gEtouePpao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "d_train = xgb.DMatrix(X_train, y_train)\n",
        "d_test = xgb.DMatrix(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OYkIA9n8Yys",
        "colab_type": "text"
      },
      "source": [
        "We can now train the model. But what type of model are we training?\n",
        "\n",
        "In the example above, we asked for an `XGBClassifer`, but with XGBoost you can also pass parameters that determine the type of model being trained. There are numerous [XGBoost parameters](https://xgboost.readthedocs.io/en/latest/parameter.html). One of the most important is the [`objective`](https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters). This tells XGBoost what type of model it should build.\n",
        "\n",
        "By default, the model is a regression model trained using squared error as a loss. In our case, we want a multiclass classification model. Options for building this type of model include `multi:softmax` and `multi:softprob`. We'll go with softmax since it has been used a few times in this course.\n",
        "\n",
        "There are also model-specific parameters. In our case we chose `multi:softmax`, which requires that we set the number of classes using the `num_class` parameter.\n",
        "\n",
        "There are general parameters that apply to all models. One of those, `booster`, is set to `gbtree` by default. With a tree booster, there are [specific parameters](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster) that you can set including `max_depth` for the max depth of the trees and `eta` for the learning rate.\n",
        "\n",
        "Note that we didn't set the number of trees. This is because XGBoost determines the size of the forest using a gradient that eventually converges.\n",
        "\n",
        "You can probably see that [XGBoost parameters](https://xgboost.readthedocs.io/en/latest/parameter.html) can be pretty complex. One parameter opens up the option for more and more. This is because the `xgboost` class is a wrapper over many different types of models, from classification to regression to ranking. And each of those models has many potential backing algorithms, each with their own parameters.\n",
        "\n",
        "> *Check your parameters carefully. XGBoost doesn't throw errors when you add useless parameters. Be sure you are using the correct parameters for your current model configuration.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP1KDFbMRDmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "params = {\n",
        "    # What type of model are we building?\n",
        "    'objective': 'multi:softmax',\n",
        "\n",
        "    # multi:softmax parameters\n",
        "    'num_class': 3,\n",
        "\n",
        "    # General parameters\n",
        "    'booster': 'gbtree',\n",
        "\n",
        "    # Tree-specific parameters\n",
        "    'max_depth': 10, \n",
        "    'eta': 0.1,\n",
        "}\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "model = xgb.train(params, d_train)\n",
        "\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdWqROf3FjJR",
        "colab_type": "text"
      },
      "source": [
        "We went ahead and timed this training operation just to get a feel for how fast XGBoost is. In our trial run, we trained in just under one minute.\n",
        "\n",
        "We can now see how well our trained model generalizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P41z96MhRM_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "preds = model.predict(d_test)\n",
        " \n",
        "print(np.round(f1_score(y_test, preds, average='micro')*100, 2), '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n7kPfIoGUIf",
        "colab_type": "text"
      },
      "source": [
        "`99.31%`, not bad!\n",
        "\n",
        "Let's not celebrate too soon. How well does this compare to a vanilla random forest?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4rrlhTQRUw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(random_state=8675309)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "end = time.time()\n",
        "print(end - start)\n",
        "\n",
        "preds = model.predict(X_test)\n",
        "\n",
        "print(np.round(f1_score(y_test, preds, average='micro')*100, 2), '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q41gop_gIQwr",
        "colab_type": "text"
      },
      "source": [
        "Well, isn't that interesting. The random forest actually did better: `99.39%`, but look at that timing. It took just over around `5` minutes to run in our tests.\n",
        "\n",
        "There are definitely cases where random forests can out-perform XGBoost, but the cost of training vs. the performance improvement, if it exists, can easily make up for the training time on large datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNJntYL9MzKB",
        "colab_type": "text"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoQsnTiBM1Dv",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 1: Galaxy or Not Galaxy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QTIzjzETws0",
        "colab_type": "text"
      },
      "source": [
        "In the example above, we tried to determine if an observation was of a star, galaxy, or quasar. The balance of the observations skewed heavily toward galaxies. In this exercise, you'll be tasked with using the same [SDSS 16 dataset](https://www.kaggle.com/joshmcadams/sdss-16) and performing a binary classification that determines if an observation is of a galaxy or not.\n",
        "\n",
        "You'll rely on XGBoost. Your task is to modify the data and the model to perform a binary classification, instead of the multiclass classification shown above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ChI8xh2VvHR",
        "colab_type": "text"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn--uZCRVx4x",
        "colab_type": "text"
      },
      "source": [
        "Using [XGBoost](https://github.com/dmlc/xgboost), create a binary classifier for the [SDSS 16 dataset](https://www.kaggle.com/joshmcadams/sdss-16) that determines if an observation is a galaxy or not.\n",
        "\n",
        "*Use as many code and text blocks as you need. Explain your work.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXrL3EoNWJMx",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAeNiodRWKII",
        "colab_type": "text"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgKJY8uGWMtC",
        "colab_type": "text"
      },
      "source": [
        "Download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_16nQzZUPaAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! kaggle datasets download joshmcadams/sdss-16\n",
        "! ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wqWFANAWOps",
        "colab_type": "text"
      },
      "source": [
        "Load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i_lD_oFPdhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('sdss-16.zip')\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-mZ9cgDWRhD",
        "colab_type": "text"
      },
      "source": [
        "Look at the current classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLqFXzRGPntg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['class'].hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJqubHc2WVFS",
        "colab_type": "text"
      },
      "source": [
        "Create a binary class for galaxy or not identification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBW6Uvi1P-Mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['is_galaxy'] = df['class'].apply(lambda x: int(x == 'GALAXY'))\n",
        "_ = df['is_galaxy'].hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZUAby1yWaW8",
        "colab_type": "text"
      },
      "source": [
        "Look at the features that matter (and that aren't cheating)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DOu-HUtPiEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "features = [\n",
        "  'right_ascension',\n",
        "  'declination',\n",
        "  'u_magnitude',\n",
        "  'g_magnitude',\n",
        "  'r_magnitude',\n",
        "  'i_magnitude',\n",
        "  'z_magnitude',\n",
        "  'redshift',\n",
        "]\n",
        "\n",
        "fig, (axs) = plt.subplots(1, 8, figsize=(20, 10))\n",
        "for i in range(len(features)):\n",
        "  axs[i].title.set_text(features[i])\n",
        "  axs[i].boxplot(df[features[i]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAbp7mFFWfmM",
        "colab_type": "text"
      },
      "source": [
        "Split the data using the `is_galaxy` column as a stratification. This isn't strictly necessary, since we have a pretty even distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHvcpGYfRx8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df[features],\n",
        "    df['is_galaxy'],\n",
        "    test_size=0.2,\n",
        "    random_state=8675309,\n",
        "    shuffle=True,\n",
        "    stratify=df['is_galaxy'])\n",
        "\n",
        "_ = y_train.hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5NfFIoRxpw",
        "colab_type": "text"
      },
      "source": [
        "Convert our data to a `DMatrix`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89p_gTnWRmQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "d_train = xgb.DMatrix(X_train, y_train)\n",
        "d_test = xgb.DMatrix(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39NE-eNUWvfi",
        "colab_type": "text"
      },
      "source": [
        "Use binary classification parameters and train the model.\n",
        "\n",
        "> *This is the real point of this exercise.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMtAiUXlNL9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {\n",
        "    # What type of model are we building?\n",
        "    'objective': 'binary:logistic',\n",
        "\n",
        "    # General parameters\n",
        "    'booster': 'gbtree',\n",
        "\n",
        "    # Tree-specific parameters\n",
        "    'max_depth': 10, \n",
        "    'eta': 0.1,\n",
        "}\n",
        "\n",
        "model = xgb.train(params, d_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFBjRNIwW4wR",
        "colab_type": "text"
      },
      "source": [
        "See how well we did."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wp4a4tuTF4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "preds = model.predict(d_test)\n",
        " \n",
        "print(np.round(f1_score(y_test, preds.round(), average='binary')*100, 2), '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BOLxK9gXArd",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    }
  ]
}