{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Decision Trees and Random Forests",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "copyright",
        "YdlbjnrP-Xuw",
        "exercise-1-key-1"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/v2/06_other_models/00_random_forests/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oXbDqPstu1RM",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yLFFlvePlEsJ"
      },
      "source": [
        "# Decision Tress and Random Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1Qm3o8i5lnAH"
      },
      "source": [
        "In this lab we will apply decision trees and random forest to perform machine learning tasks. These two model types are relatively easy to understand, yet they are very powerful tools.\n",
        "\n",
        "Random forests build upon decision tree models, so we'll start by creating a decision tree and then move to random forests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Dj52_5Wm1Oa",
        "colab_type": "text"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qlxCUdTql0aW"
      },
      "source": [
        "Let's start by loading some data. We'll use the familiar iris dataset from scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oyP6DVIEjQZL",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris_bunch = load_iris()\n",
        "\n",
        "feature_names = iris_bunch.feature_names\n",
        "target_name = 'species'\n",
        "\n",
        "iris_df = pd.DataFrame(\n",
        "    iris_bunch.data,\n",
        "    columns=feature_names\n",
        ")\n",
        "\n",
        "iris_df[target_name] = iris_bunch.target\n",
        "\n",
        "iris_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwJoZY7hm3Rh",
        "colab_type": "text"
      },
      "source": [
        "## Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW7jPu6cm5Vs",
        "colab_type": "text"
      },
      "source": [
        "Decision trees are models that create a tree structure that has a condition at each non-terminal leaf in the tree. The condition is used to choose which branch to traverse down the tree.\n",
        "\n",
        "Let's see what this would look like with a simple example.\n",
        "\n",
        "Say that we want to determine if a piece of fruit is a lemon, lime, orange, or grapefruit. We might have a tree that looks like:\n",
        "\n",
        "```txt\n",
        "                      ----------\n",
        "           -----------| color? |-----------\n",
        "          |           ----------           |\n",
        "          |               |                |\n",
        "       <green>         <orange>        <yellow>\n",
        "          |               |                |\n",
        "          |               |                |\n",
        "       ========           |            =========\n",
        "       | lime |           |            | lemon |\n",
        "       ========       ---------        =========\n",
        "                 -----| size? |-----\n",
        "                 |    ---------    |\n",
        "                 |                 |\n",
        "              <small>           <large>\n",
        "                 |                 |\n",
        "                 |                 |\n",
        "            ==========       ==============\n",
        "            | orange |       | grapefruit |\n",
        "            ==========       ==============\n",
        "```\n",
        "\n",
        "This would roughly translate to the following code:\n",
        "\n",
        "```python\n",
        "\n",
        "def fruit_type(fruit):\n",
        "  if fruit.color == \"green\":\n",
        "    return \"lime\"\n",
        "  if fruit.color == \"yellow\":\n",
        "    return \"lemon\"\n",
        "  if fruit.color == \"orange\":\n",
        "    if fruit.size == \"small\":\n",
        "      return \"orange\"\n",
        "    if fruit.size == \"large\":\n",
        "      return \"grapefruit\"\n",
        "```\n",
        "\n",
        "As you can see, the decision tree is very easy to interpret. If you use a decision tree to make predictions and then need to determine why the tree made the decision that it did, it is very easy to inspect.\n",
        "\n",
        "Also, decision trees aren't sensitive to data that isn't scaled or normalized, which is different from many types of models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E_ZO5q4xmOMU"
      },
      "source": [
        "### Create a Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWYYYG0prrPb",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the data loaded, we can create a decision tree. We'll use the [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) from scikit-learn to perform this task.\n",
        "\n",
        "Note that there is also a [`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) that can be used for regression models. In practice you'll typically see decision trees applied to classification problems more than regression.\n",
        "\n",
        "To build and train the model we create an instance of the classifer and then call the `fit()` method that is used for all scikit-learn models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PjkabO7nkCjt",
        "colab": {}
      },
      "source": [
        "from sklearn import tree\n",
        "\n",
        "dt = tree.DecisionTreeClassifier()\n",
        "\n",
        "dt.fit(\n",
        "    iris_df[feature_names],\n",
        "    iris_df[target_name]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt0GmCTOsUx8",
        "colab_type": "text"
      },
      "source": [
        "Note that if this were a real application we'd keep some data to the side for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K24b3sUdmj17"
      },
      "source": [
        "### Visualize the Tree\n",
        "\n",
        "We now have a decision tree and can use it to make predictions. But before we do that, let's take a look at the tree itself.\n",
        "\n",
        "To do this we create a [`StringIO`](https://docs.python.org/3/library/io.html) object that we can export dot-data to. The dot data is a graph description language that what we can plot with Python graphing utilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xmPNDQvKkeOd",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import pydotplus\n",
        "\n",
        "from IPython.display import Image  \n",
        "\n",
        "dot_data = io.StringIO()  \n",
        "\n",
        "tree.export_graphviz(\n",
        "    dt,\n",
        "    out_file=dot_data,  \n",
        "    feature_names=feature_names\n",
        ")  \n",
        "\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "\n",
        "Image(graph.create_png())  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f2ipXFP7n8Xg"
      },
      "source": [
        "That tree looks pretty complex. Many branches in the tree is a sign that we may have overfit the model. Let's create the tree again, only this time we'll limit the depth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WA0lUiePoIuZ",
        "colab": {}
      },
      "source": [
        "from sklearn import tree\n",
        "\n",
        "dt = tree.DecisionTreeClassifier(max_depth=2)\n",
        "\n",
        "dt.fit(\n",
        "    iris_df[feature_names],\n",
        "    iris_df[target_name]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LOU_tzOyoO6A"
      },
      "source": [
        "And plot to see the branching."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "plW0crKvoSY6",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import pydotplus\n",
        "\n",
        "from IPython.display import Image  \n",
        "\n",
        "dot_data = io.StringIO()  \n",
        "\n",
        "tree.export_graphviz(\n",
        "    dt,\n",
        "    out_file=dot_data,  \n",
        "    feature_names=feature_names\n",
        ")  \n",
        "\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "\n",
        "Image(graph.create_png())  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "67yJDrz9oUo6"
      },
      "source": [
        "This tree is less likely to be overfitting, since we forced it to have a depth of 2. Holding out a test sample and performing validation would be a good way to check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTw-HAn5syz4",
        "colab_type": "text"
      },
      "source": [
        "What are the `gini`, `samples`, and `value` items shown in the tree.\n",
        "\n",
        "`gini` is is the *Gini impurity*. This is a measure of the chance that you'll misclassify a random element in the dataset at this decision point. Smaller `gini` is better.\n",
        "\n",
        "`samples` is a count of the number of samples that have met the criteria to reach this leaf.\n",
        "\n",
        "`value` are the count of each class of data that has made it to this leaf. Summing `value` should equal `sample`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvbBaidx39o-",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgJIfXZ44BW0",
        "colab_type": "text"
      },
      "source": [
        "There are many hyperparameters that you can tweak in your decision tree models. One of those is `criterion`. `criterion` determines the quality measure that the model will use the determine the shape of the tree.\n",
        "\n",
        "The values are `gini` and `entrophy`. `gini` is the [Gini Impuirty](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity) while `entrophy` is a measure if [Information Gain](https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain).\n",
        "\n",
        "In the example below we switch the classifer to use \"entrophy\" for `criterion`. You'll see in the resultant tree that we now see \"entrophy\" instead of \"gini\", but the resultant trees are the same. For more complex models you might want to test the different criterion though."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CktGFTok5cFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import pydotplus\n",
        "\n",
        "from IPython.display import Image  \n",
        "from sklearn import tree\n",
        "\n",
        "dt = tree.DecisionTreeClassifier(\n",
        "    max_depth=2, \n",
        "    criterion=\"entropy\"\n",
        ")\n",
        "\n",
        "dt.fit(\n",
        "    iris_df[feature_names],\n",
        "    iris_df[target_name]\n",
        ")\n",
        "\n",
        "dot_data = io.StringIO()  \n",
        "\n",
        "tree.export_graphviz(\n",
        "    dt,\n",
        "    out_file=dot_data,  \n",
        "    feature_names=feature_names\n",
        ")  \n",
        "\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "\n",
        "Image(graph.create_png())  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVzXWuwg8x__",
        "colab_type": "text"
      },
      "source": [
        "We've limited the depth of the tree using `max_depth`. We can also limit the number of samples required to be present in a node for it to be considered for splitting using `min_samples_split` and can limit the minimum size of a leaf node using `min_samples_leaf`. All of these hyperparameters help you to prevent your model from overfitting.\n",
        "\n",
        "There are many other hyperparameters that can be found in the [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY_juAqE9emQ",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 1: Tuning Decision Tree Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgRtUMCA91UI",
        "colab_type": "text"
      },
      "source": [
        "In this exercise we will use a decision tree to classify wine quality in the [Red Wine Quality dataset](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009).\n",
        "\n",
        "The target column in the dataset is `quality`. Quality is an integer value between 1 and 10 (inclusive). You'll use the other columns in the dataset to build a decision tree to predict wine quality.\n",
        "\n",
        "For this exercise:\n",
        "\n",
        "* Hold out some data for final testing of model generalization.\n",
        "* Use [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) compare some hyperparameters for your model. You can choose which parameters to test.\n",
        "* Print the hyperparameters of the best performing model.\n",
        "* Print the accuracy of the best performing model and the hold out dataset.\n",
        "* Visualize the best performing tree.\n",
        "\n",
        "Use as many text and code cells as you need to perform this exercise. We'll get you started with the code to authenticate and download the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf2Da0VnKJqh",
        "colab_type": "text"
      },
      "source": [
        "First, upload your `kaggle.json` file and then run the code block below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnVpaP7XKODL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! chmod 600 kaggle.json && (ls ~/.kaggle 2>/dev/null || mkdir ~/.kaggle) && mv kaggle.json ~/.kaggle/ && echo 'Done'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BPOFhblKPwM",
        "colab_type": "text"
      },
      "source": [
        "Next, download the wine quality dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFkRa-83KTIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! kaggle datasets download uciml/red-wine-quality-cortez-et-al-2009\n",
        "! ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlxj0X-q-THj",
        "colab_type": "text"
      },
      "source": [
        "**Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4LcrR8H-UvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your Code Goes Here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCT2HzaZ-W_d",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdlbjnrP-Xuw",
        "colab_type": "text"
      },
      "source": [
        "##### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INPlrsLMKbMF",
        "colab_type": "text"
      },
      "source": [
        "Get authentication credentials in place and then download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du-LDlG0BpQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! chmod 600 kaggle.json && (ls ~/.kaggle 2>/dev/null || mkdir ~/.kaggle) && mv kaggle.json ~/.kaggle/ && echo 'Done'\n",
        "! kaggle datasets download uciml/red-wine-quality-cortez-et-al-2009\n",
        "! ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR3XSN0GKmfk",
        "colab_type": "text"
      },
      "source": [
        "Load the data, split the data, and then create train and test `DataFrame` objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Hk_0-ZyKrxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "df = pd.read_csv('red-wine-quality-cortez-et-al-2009.zip')\n",
        "\n",
        "feature_names = df.columns.values[:-1]\n",
        "target_name = df.columns.values[-1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df[feature_names],\n",
        "    df[target_name],\n",
        "    stratify=df[target_name],\n",
        "    train_size=0.2,\n",
        ")\n",
        "\n",
        "train_df = pd.DataFrame(\n",
        "    X_train,\n",
        "    columns=feature_names\n",
        ")\n",
        "train_df[target_name] = y_train\n",
        "\n",
        "train_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxqo6iU2K0OX",
        "colab_type": "text"
      },
      "source": [
        "Perform a cross-validation grid search on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaGvoSpMK6o4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import tree\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "dt = tree.DecisionTreeClassifier()\n",
        "grid_search = GridSearchCV(dt, {\n",
        "    'max_depth': [None, 2, 4, 6],\n",
        "    'min_samples_split': [2, 4, 16],\n",
        "    'min_samples_leaf': [1, 4, 8],\n",
        "})\n",
        "\n",
        "grid_search.fit(\n",
        "    train_df[feature_names],\n",
        "    train_df[target_name]\n",
        ")\n",
        "\n",
        "grid_search.cv_results_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTEM2rCrLOTe",
        "colab_type": "text"
      },
      "source": [
        "Print the best parameters and the best score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4-LRsl9IFSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(grid_search.best_params_)\n",
        "print(grid_search.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inPJXCP-LXWR",
        "colab_type": "text"
      },
      "source": [
        "Print the hold-out testing accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIsfbanwHdjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(\n",
        "    test_df[target_name],\n",
        "    grid_search.best_estimator_.predict(test_df[feature_names])\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URnj5XgXLmM5",
        "colab_type": "text"
      },
      "source": [
        "Visualize the best tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBB-PEneC13-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import pydotplus\n",
        "\n",
        "from IPython.display import Image  \n",
        "\n",
        "dot_data = io.StringIO()  \n",
        "\n",
        "tree.export_graphviz(\n",
        "    grid_search.best_estimator_,\n",
        "    out_file=dot_data,  \n",
        "    feature_names=feature_names\n",
        ")  \n",
        "\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "\n",
        "Image(graph.create_png())  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtjyB2S_-ZtK",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cege4SOkmo4A",
        "colab_type": "text"
      },
      "source": [
        "## Random Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XivwzvBmqlS",
        "colab_type": "text"
      },
      "source": [
        "Random forests are a simple yet powerful machine learning tool based on decision trees. Random forests are easy to understand, yet they touch upon many advanced machine learning concepts such as ensemble learning and bagging. These models can be used for both classification and regression. Also, since they are built from decision trees, they are not sensitive to unscaled data.\n",
        "\n",
        "You can think of a random forest a group decision made by a number of decision trees. For classification problems, the random forest creates multiple decision trees with different subsets of the data. When it is asked to classify a data point, it will ask all of the trees what they think and then take the majority decision.\n",
        "\n",
        "For regression problems the random forest will again use the opnions of multiple decision trees, but will take the mean (or some other summation) of the responses and use that as the regression value.\n",
        "\n",
        "This type of modeling where one model consists of other models is called *ensemble learning*. Ensemble learning can often lead to better models because taking the combined differing opnions of a group of models can reduce overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_2J1CZ4uohf2"
      },
      "source": [
        "### Create a Random Forest\n",
        "\n",
        "Creating a random forest is as easy as creating a decision tree.\n",
        "\n",
        "sckit-learn provides a [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), and a a [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html), that can be used to combine the predictive power of multiple decision trees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZH9xJB4ikyfv",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "iris_bunch = load_iris()\n",
        "\n",
        "feature_names = iris_bunch.feature_names\n",
        "target_name = 'species'\n",
        "\n",
        "iris_df = pd.DataFrame(\n",
        "    iris_bunch.data,\n",
        "    columns=feature_names\n",
        ")\n",
        "\n",
        "iris_df[target_name] = iris_bunch.target\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(\n",
        "    iris_df[feature_names],\n",
        "    iris_df[target_name]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PIoGwH2Lp2F4"
      },
      "source": [
        "You can look at different trees in the random forest to see how their decision branching differs. By default there are `100` decision trees created for the model.\n",
        "\n",
        "Let's view a few.\n",
        "\n",
        "Run the code below a few times and see if you notice a difference in the trees that are shown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XI_DkBXSpKDl",
        "colab": {}
      },
      "source": [
        "import pydotplus\n",
        "import random\n",
        "\n",
        "from IPython.display import Image  \n",
        "from sklearn.externals.six import StringIO  \n",
        "\n",
        "dot_data = StringIO()  \n",
        "\n",
        "tree.export_graphviz(\n",
        "    random.choice(rf.estimators_),\n",
        "    out_file=dot_data,  \n",
        "    feature_names=feature_names\n",
        ")  \n",
        "\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "\n",
        "Image(graph.create_png())  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i_xl-Et8qjTA"
      },
      "source": [
        "### Make Predictions\n",
        "\n",
        "Just like any other scikit-learn model you can use the `predict()` method to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ll4flhAWqA4W",
        "colab": {}
      },
      "source": [
        "print(rf.predict([iris_df.iloc[121][feature_names]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMYt-B05PQ2B",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD0KX-pcPTHZ",
        "colab_type": "text"
      },
      "source": [
        "Many of the hyperparameters available in decision trees are also available in random forest models. There are however some hyperparameters that are only available in random forests.\n",
        "\n",
        "The two most important are `bootstrap` and `oob_score`. These two hyperparameters are relevant to ensemble learning.\n",
        "\n",
        "`bootstrap` determines if the model will use [bootstrap sampling](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)). When you bootstrap only a sample of the dataset will be used for training each tree in the forest. Bootstrapping means that the full dataset will be used as the source of the sampling for each tree. There is \"replacement\" of the data. A datapoint can occur in more that one tree.\n",
        "\n",
        "`oob_score` stands for \"Out of bag score\". When you create a bootstrap sample this is referred to as a *bag* in machine learning parlance. When the tree is being scored only data points in the bag sampled for the tree will be used unless `oob_score` is set to true."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FLf1S7Hirvyy"
      },
      "source": [
        "### Exercise 2: Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2lToTdAbJA-M"
      },
      "source": [
        "In this exercise we will use the [UCI Abalone  dataset](https://www.kaggle.com/hurshd0/abalone-uci) to determine the age of sea snails.\n",
        "\n",
        "The target feature in the dataset is `rings`, which is a proxy for age in the snails. This is a numeric value, but it is stored as an integer and has a biological limit, so we can think of this as a classification problem and use a [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
        "\n",
        "You will download the dataset and train a random forest classifier. After you have fit the classifer the `feature_importances_` attribute of the model will be populated. Use the importance scores to print the least important feature.\n",
        "\n",
        "*Note that some of the features are catagorical string values. You'll need to convert these to numeric values to use them in the model.*\n",
        "\n",
        "Use as many text and code blocks as you need to below to perform this exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NlOyoIK8r6kc"
      },
      "source": [
        "**Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QNdrDofHr_XQ",
        "colab": {}
      },
      "source": [
        "# Your Code Goes Here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-1-key-1"
      },
      "source": [
        "#### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JncwHltVld-",
        "colab_type": "text"
      },
      "source": [
        "First we download the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uWBF4HE_JFlU",
        "colab": {}
      },
      "source": [
        "! kaggle datasets download hurshd0/abalone-uci\n",
        "! ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e0jubhrVn1E",
        "colab_type": "text"
      },
      "source": [
        "And load it into a `DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bQPkHRpSXtu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('abalone-uci.zip')\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGDKmyApVq72",
        "colab_type": "text"
      },
      "source": [
        "We check for missing values... there are none."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wHhl7GASfS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.isna().any()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aO4CEskVtn_",
        "colab_type": "text"
      },
      "source": [
        "However, sex was represented as a string. We convert it into an integer value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQJ6nhHaSksk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sexes = sorted(df['sex'].unique())\n",
        "sex_lookup = {sexes[i]: i for i in range(len(sexes))}\n",
        "df['sex'] = df['sex'].map(sex_lookup)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH9jFAFQVy2T",
        "colab_type": "text"
      },
      "source": [
        "Next we capture the feature and target names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nesavzo8TA-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_names = df.columns.values[:-1]\n",
        "target_name = df.columns.values[-1]\n",
        "\n",
        "target_name, feature_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44o7fjVSV3BK",
        "colab_type": "text"
      },
      "source": [
        "And then fit the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk10Lmb-TVz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "model.fit(df[feature_names], df[target_name])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89cfS_wIV5Cd",
        "colab_type": "text"
      },
      "source": [
        "Only to learn that `sex` is the least important feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjY_F5CTTk_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "feature_names[np.argmin(model.feature_importances_)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpQjczbJSNrl",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    }
  ]
}