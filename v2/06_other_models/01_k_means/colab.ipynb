{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "k-Means",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "copyright",
        "exercise-1-key-1",
        "exercise-2-key-1"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/v2/06_other_models/01_k_means/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DIk2MioaatFI",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "daX5oL8aDZLn"
      },
      "source": [
        "# k-means\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e5jEmmM1wQBC"
      },
      "source": [
        "k-means clustering is an *unsupervised* machine learning algorithm that can be used to group items into clusters.\n",
        "\n",
        "So far we have only worked with supervised algorithms. Supervised algorithms have training data with labels that identify the numeric value or class for each item. These algorithms use labeled data to build a model that can be used to make predictions.\n",
        "\n",
        "k-means clustering is different. The training data is not labeled. Unlabeled training data is fed into the model, which attempts to find relationships in the data and create clusters based on those relationships. Once these clusters are formed, predictions can be made about which cluster new data items belong to.\n",
        "\n",
        "The clusters can't easily be labeled in many cases. The clusters are \"emergent clusters\", which are created by the algorithm and don't always map to groupings that you might expect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ytHWSs1qG3e5"
      },
      "source": [
        "## Example: Groups of Mushrooms\n",
        "\n",
        "Let's start by looking at a real world use case involving mushrooms. The University of California Irvine has a [dataset containing various attributes of mushrooms](https://www.kaggle.com/uciml/mushroom-classification). One of those attributes is the edibility of the mushroom: Is it edible or is it poisonous?  We want to see if we can find clusters of mushroom attributes that can be used to determine if a mushroom is edible or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1NUs6PhaH9J7"
      },
      "source": [
        "### Load the data\n",
        "\n",
        "For this example we'll load the [Mushroom Classification](https://www.kaggle.com/uciml/mushroom-classification) data. The dataset attributes about over `8,000` different mushrooms.\n",
        "\n",
        "Upload your `kaggle.json` file and run the code block below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IXUekcf9kLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! chmod 600 kaggle.json && (ls ~/.kaggle 2>/dev/null || mkdir ~/.kaggle) && mv kaggle.json ~/.kaggle/ && echo 'Done'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90kxf9D89sh-",
        "colab_type": "text"
      },
      "source": [
        "And then use the Kaggle API to download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA7gddSDBP0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! kaggle datasets download uciml/mushroom-classification\n",
        "! ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XloYj7BNNzs9",
        "colab_type": "text"
      },
      "source": [
        "Unzip the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs3D-ZBXBXTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! unzip mushroom-classification.zip\n",
        "! ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKcL398EN2Vt",
        "colab_type": "text"
      },
      "source": [
        "And finally, load the training data into a `DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo2GFoXNBfFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('mushrooms.csv')\n",
        "data.sample(n=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShvO0AWGOAX3",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcvUECVCODNV",
        "colab_type": "text"
      },
      "source": [
        "Let's take a closer look at the data that we'll be working with, starting with a simple describe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vlqf1a3GaZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.describe(include='all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZE93JydGaEq",
        "colab_type": "text"
      },
      "source": [
        "It doesn't look like any columns are missing data since we see counts of `8,124` for every column.\n",
        "\n",
        "It does look like all of the data is categorical. We'll need to convert it into numeric values for the model to work. Let's do it for every column except `class`. We aren't trying to predict class, but we do want to see if we can get pure clusters of one type of class. So we don't want it included in our training data. Also, it is the only feature that isn't observable without having dire consequences!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxVI_cMiBvtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = [c for c in data.columns.values if c != 'class']\n",
        "id_to_value_mappings = {}\n",
        "value_to_id_mappings = {}\n",
        "\n",
        "for column in columns:\n",
        "  i_to_v = sorted(data[column].unique())\n",
        "  v_to_i = { v:i for i, v in enumerate(i_to_v)}\n",
        "  \n",
        "\n",
        "  numeric_column = column + '-id'\n",
        "  data[numeric_column] = [v_to_i[v] for v in data[column]]\n",
        "\n",
        "  value_to_id_mappings[column] = v_to_i\n",
        "  id_to_value_mappings[numeric_column] = i_to_v\n",
        "\n",
        "numeric_columns = id_to_value_mappings.keys()\n",
        "data[numeric_columns].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxHbDQi5RehL",
        "colab_type": "text"
      },
      "source": [
        "### Perform Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZTI-QWeRcXE",
        "colab_type": "text"
      },
      "source": [
        "We now have numeric data that a model can handle. To run k-means clustering on the data, we simply load [k-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) from scikit-learn and ask the model to find a specific number of clusters for us.\n",
        "\n",
        "Notice that we are scaling the data. The class IDs are integer values, and some columns have many more classes than others. Scaling helps make sure that columns with more classes don't have an undue influence on the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1kKMdvzC27A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "model = KMeans(n_clusters=10)\n",
        "model.fit(scale(data[numeric_columns]))\n",
        "\n",
        "print(model.inertia_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMOPqimXSlaQ",
        "colab_type": "text"
      },
      "source": [
        "We asked scikit-learn to create 10 clusters for us, and then we printed out the `inertia_` for the resultant clusters. *Inertia* is the sum of the squared distances of samples to their closest cluster center. Typically, the smaller the inertia the better.\n",
        "\n",
        "But why did we choose `10` clusters? And is the inertia that we received reasonable?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asbd5pyUSp2G",
        "colab_type": "text"
      },
      "source": [
        "### Find the optimal number of clusters\n",
        "\n",
        "With just one run of the algorithm, it is difficult to tell how many clusters we should have and what an appropriate inertia value is. k-means is trying to discover things about your data that you do not know. Picking a number of clusters at random isn't the best way to use k-means.\n",
        "\n",
        "Instead, you should experiment with a few different cluster values and measure the inertia of each. As you increase the number of clusters, your inertia should decrease."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WlVhQkWC_iK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import scale\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "clusters = list(range(5, 50, 5))\n",
        "inertias = []\n",
        "\n",
        "scaled_data = scale(data[numeric_columns])\n",
        "\n",
        "for c in clusters:\n",
        "  model = KMeans(n_clusters=c)\n",
        "  model = model.fit(scaled_data)\n",
        "  inertias.append(model.inertia_)\n",
        "\n",
        "plt.plot(clusters, inertias)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGrJ_QGkS3EF",
        "colab_type": "text"
      },
      "source": [
        "The resulting graph should start high and to the left and curve down as the number of clusters grows. The initial slope is steep, but begins to level off. Your optimal number of clusters is somewhere in the [\"elbow\" of the graph](https://en.wikipedia.org/wiki/Elbow_method_(clustering)), as the slope levels.\n",
        "\n",
        "Once you have this number, you need to then check to see if the number is reasonable for your use case. Say that the 'optimal' number of clusters for our customer segmentation is 15. Is that a reasonable number of clusters to deal with? If we have too many, we can overfit and make the model poor at generalizing. And what are the purposes of the clusters? If you are clustering mushrooms and want to find clusters that are definitely safe to eat, 15 or more clusters might be perfectly fine. If you are clustering customers for different advertising campaigns, 15 different campaigns might be more than your marketing department can handle.\n",
        "\n",
        "Clustering the data is often just the start of your journey. Once you have clusters, you'll need to look at each group and try to determine what makes them similar. What patterns did the clustering find? And will that clustering be useful to you?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVmJq7-ZTho3",
        "colab_type": "text"
      },
      "source": [
        "### Examining Clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_IFLvUbTlDq",
        "colab_type": "text"
      },
      "source": [
        "Let's say that `15` is a reasonable number of clusters. We can rebuild the model using that setting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfLmrYcwGKII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "model = KMeans(n_clusters=15)\n",
        "model.fit(scale(data[numeric_columns]))\n",
        "\n",
        "print(model.inertia_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjHeNiyETwLZ",
        "colab_type": "text"
      },
      "source": [
        "Now let's see if we have any 'pure' clusters. These are clusters with all-edible or all-poisonous mushrooms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q5DkPXBGs_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "for cluster in sorted(np.unique(model.labels_)):\n",
        "  num_edible = np.sum(data[model.labels_ == cluster]['class'] == 'e')\n",
        "  total = np.sum(model.labels_ == cluster)\n",
        "  print(cluster, num_edible / total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irNyQCzzT_el",
        "colab_type": "text"
      },
      "source": [
        "In our model we had clusters `0`, `1`, `6`, and `10` be `100%` edible. Clusters `2`, `4`, `7`, and `12` were all poisonous. The remaining were a mix of the two.\n",
        "\n",
        "Knowing this, let's look at one of the all-edible clusters and see what attributes we could look for to have confidence that we have an edible mushroom."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WhvIM6CMRp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "edible = data[model.labels_ == 1]\n",
        "\n",
        "for column in edible.columns:\n",
        "  if column.endswith('-id'):\n",
        "    continue\n",
        "  print(column, edible[column].unique())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-URbeexwVRtT",
        "colab_type": "text"
      },
      "source": [
        "The mapping of the letter codes to more descriptive text can be found in the [dataset description](https://www.kaggle.com/uciml/mushroom-classification)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t2ySDMeeP-6J"
      },
      "source": [
        "## Example: Classification of Digits\n",
        "\n",
        "Clustering for data exploration purposes can lead to interesting insights in to your data, but clustering can also be used for classification purposes.\n",
        "\n",
        "In the example below, we'll try to use k-means clustering to predict handwritten digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QePSEGmwQY2W"
      },
      "source": [
        "### Load the data\n",
        "\n",
        "We'll load the digits dataset packaged with scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C9pGH4rfQUgA",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5OTRl2plQgDi"
      },
      "source": [
        "### Scale the data\n",
        "\n",
        "It is good practice to scale the data to ensure that outliers don't have too big of an impact on the clustering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t0aBmYGPQnri",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import scale\n",
        "\n",
        "scaled_digits = scale(digits.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dwQ9Df4zQ_a_"
      },
      "source": [
        "### Fit a model\n",
        "\n",
        "We can then create a k-means model with 10 clusters. (We know there are 10 digits from 0 through 9.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gZaOn2gRQzsu",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "model = KMeans(n_clusters=10)\n",
        "model = model.fit(scaled_digits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M0bCgYN_SCw5"
      },
      "source": [
        "### Make predictions\n",
        "\n",
        "We can then use the model to predict which category a data point belongs to.\n",
        "\n",
        "In the case below, we'll just use some of the data that we trained with for illustrative purposes. The prediction will provide a numeric value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mpLDPTMyRm7i",
        "colab": {}
      },
      "source": [
        "cluster = model.predict([scaled_digits[0]])[0]\n",
        "\n",
        "cluster"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R_yDe1QZSaBj"
      },
      "source": [
        "What is this value? Is it the predicted digit?\n",
        "\n",
        "No. This number is the cluster that the model thinks the digit belongs to. To determine the predicted digit, we'll need to see what other digits are in the cluster and choose the most popular one for our classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RzccT4gMSxi2",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "labels = digits.target\n",
        "\n",
        "cluster_to_digit = [\n",
        "  np.argmax(\n",
        "      np.bincount(\n",
        "        np.array(\n",
        "          [labels[i] for i in range(len(model.labels_)) if model.labels_[i] == cluster]\n",
        "        )\n",
        "      )\n",
        "    ) for cluster in range(10)\n",
        "]\n",
        "\n",
        "cluster_to_digit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LtBucdm7j9vQ"
      },
      "source": [
        "Here we can see the digit that each cluster represents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y1aVtOwEVdQf"
      },
      "source": [
        "### Measure model quality\n",
        "\n",
        "If we do have labeled data, as is the case with our digits data, then we can measure the quality of our model using the [homogeneity](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score_) score and the [completeness](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score) score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r85vjdc0_l6S",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import homogeneity_score\n",
        "from sklearn.metrics import completeness_score\n",
        "\n",
        "homogeneity = homogeneity_score(labels, model.labels_)\n",
        "completeness = completeness_score(labels, model.labels_)\n",
        "homogeneity, completeness"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X5IZSU1OvPMN"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sCkXnAZHXEUL"
      },
      "source": [
        "## Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pzwm9vnEkoAf"
      },
      "source": [
        "Load the [iris dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html), create a k-means model with three clusters, and then find the homogeneity and completeness scores for the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "abvbIkWfvSl3"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s3rLeWvIXhoo",
        "colab": {}
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqSceBD3HDkU",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-1-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W0hKgICDufmp",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import homogeneity_score\n",
        "from sklearn.metrics import completeness_score\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "scaled_iris = scale(iris.data)\n",
        "\n",
        "model = KMeans(n_clusters=3)\n",
        "model = model.fit(scaled_iris)\n",
        "\n",
        "labels = iris.target\n",
        "\n",
        "cluster_to_species = [\n",
        "  np.argmax(\n",
        "      np.bincount(\n",
        "        np.array([\n",
        "                  labels[i]\n",
        "                  for i in range(len(model.labels_))\n",
        "                  if model.labels_[i] == cluster\n",
        "        ])\n",
        "      )\n",
        "    ) for cluster in range(3)\n",
        "]\n",
        "\n",
        "homogeneity = homogeneity_score(labels, model.labels_)\n",
        "completeness = completeness_score(labels, model.labels_)\n",
        "homogeneity, completeness"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg0Nc1gpHE1L",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fjo4qeMNYu1T"
      },
      "source": [
        "## Exercise 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5VetIjZ1kqNk"
      },
      "source": [
        "Load the [iris dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html), and then create a k-means model with three clusters using only two features. (Try to find the best two features for clustering.) Create a plot of the two features.\n",
        "\n",
        "For each datapoint in the chart, use a [marker](https://matplotlib.org/api/markers_api.html) to encode the actual/correct species. For instance, use a triangle for Setosa, a square for Versicolour, and a circle for Virginica. Color each marker green if the predicted class matches the actual. Color each marker red if the classes don't match."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6ewsx_qWvXM8"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q-tgFdNHYuI5",
        "colab": {}
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiHWlzVxHge1",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-2-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rFTz3DxtuiGV",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import homogeneity_score\n",
        "from sklearn.metrics import completeness_score\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "scaled_iris = scale(iris.data)\n",
        "scaled_iris = np.array(scaled_iris)\n",
        "labels = iris.target\n",
        "\n",
        "highest_score, best_i, best_j = 0, 0, 0\n",
        "\n",
        "for i in range(3):\n",
        "  for j in range(3):\n",
        "    if i != j:\n",
        "      model = KMeans(n_clusters=3)\n",
        "      model = model.fit(\n",
        "          list(zip(np.array(iris.data)[:, i], np.array(iris.data)[:, j])))\n",
        "\n",
        "      homogeneity = homogeneity_score(labels, model.labels_)\n",
        "      completeness = completeness_score(labels, model.labels_)\n",
        "      combined_score = homogeneity + completeness\n",
        "\n",
        "      if combined_score > highest_score:\n",
        "        highest_score = combined_score\n",
        "        best_i = i\n",
        "        best_j = j\n",
        "\n",
        "cluster_to_species = [\n",
        "  np.argmax(\n",
        "      np.bincount(\n",
        "        np.array([\n",
        "                  labels[i]\n",
        "                  for i in range(len(model.labels_))\n",
        "                  if model.labels_[i] == cluster\n",
        "        ])\n",
        "      )\n",
        "    ) for cluster in range(3)\n",
        "]\n",
        "\n",
        "correct = [cluster_to_species[model.labels_[i]] == labels[i] \n",
        "           for i in range(len(model.labels_))]\n",
        "\n",
        "df = pd.DataFrame()\n",
        "iris_data = np.array(iris.data)\n",
        "\n",
        "df[iris.feature_names[best_i]] = iris_data[:, best_i]\n",
        "df[iris.feature_names[best_j]] = iris_data[:, best_j]\n",
        "df['species'] = labels\n",
        "df['predicted_species'] = [cluster_to_species[model.labels_[i]] for i in range(len(model.labels_))]\n",
        "\n",
        "MARKERS = ('^', 's', 'o')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "for i in range(3):\n",
        "  selector = (df['species'] == i) & (df['predicted_species'] == i)\n",
        "  ax.scatter(\n",
        "      df[selector][iris.feature_names[best_i]],\n",
        "      df[selector][iris.feature_names[best_j]],\n",
        "      marker = MARKERS[i],\n",
        "      color = 'g',\n",
        "      label = iris.feature_names[i] + ' correct'\n",
        "  )\n",
        "  selector = (df['species'] == i) & (df['predicted_species'] != i)\n",
        "  ax.scatter(\n",
        "      df[selector][iris.feature_names[best_i]],\n",
        "      df[selector][iris.feature_names[best_j]],\n",
        "      marker = MARKERS[i],\n",
        "      color = 'r',\n",
        "      label = iris.feature_names[i] + ' incorrect'\n",
        "  )\n",
        "\n",
        "ax.legend()\n",
        "_ = plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tJxZ8s0HhX6",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    }
  ]
}
