{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Natural Language Processing",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "copyright",
        "exercise-1-key-1",
        "exercise-2-key-1",
        "exercise-3-key-1",
        "exercise-4-key-1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2019 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "24p97VuTvYVT",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CVmV0M74xwm7"
      },
      "source": [
        "# Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XuFjSsW53I9_"
      },
      "source": [
        "Look almost anywhere around you and you'll see an application of natural language processing (NLP) at work. This broad field covers everything from spellcheck, to translation between languages, to full machine understanding of human language.\n",
        "\n",
        "In this lesson, we'll work through the typical process of an NLP problem. We'll first use a bag-of-words approach to train a simple classifier model. Then we'll use a sequential approach (considering the order of words) to train an RNN model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jk3COdKGIB7l"
      },
      "source": [
        "## Problem and Data\n",
        "\n",
        "We will use the [Sentiment Labelled Sentences Data Set](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences) from the UCI Machine Learning Repository. This dataset contains 3000 user reviews from IMDB, Amazon, and Yelp with the corresponding sentiment of each review (positive: 1 or negative: 0). This supervised problem of predicting sentiment is often called a \"sentiment analysis task\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zu7rUHNXWBue"
      },
      "source": [
        "### Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PNHl0JpzQWZK",
        "colab": {}
      },
      "source": [
        "# Set random seeds for reproducible results.\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kZXDE22TPKDI",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "import io\n",
        "import shutil\n",
        "import urllib.request\n",
        "\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip'\n",
        "\n",
        "# Download zip file from url.\n",
        "zipdata = io.BytesIO()\n",
        "zipdata.write(urllib.request.urlopen(url).read())\n",
        "\n",
        "# Extract zip files.\n",
        "zfile = zipfile.ZipFile(zipdata)\n",
        "zfile.extractall()\n",
        "zfile.close()\n",
        "\n",
        "# Rename directory to \"data\".\n",
        "shutil.rmtree('./data', ignore_errors=True)\n",
        "shutil.move('sentiment labelled sentences', 'data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u6NIquihWLYa"
      },
      "source": [
        "### Process data into NumPy arrays\n",
        "\n",
        "The downloaded data is split across three files: `amazon_cells_labelled.txt`, `imdb_labelled.txt`, and `yelp_labelled.txt`. Each file has two tab-separated columns, one containing the review text and one containing the sentiment label. Let's combine all the files into one DataFrame, then get a sense of what the data looks like.\n",
        "\n",
        "Note: How would you split the two columns if there were tabs *within* the review text? We don't need to worry about it for this dataset, but you should generally check your labels to make sure everything is processed correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xm-tPGr7PqCg",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "filepath_dict = {\n",
        "    'amazon': 'data/amazon_cells_labelled.txt',\n",
        "    'imdb':   'data/imdb_labelled.txt',\n",
        "    'yelp':   'data/yelp_labelled.txt'\n",
        "}\n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['review', 'label'], sep='\\t')\n",
        "    # Add another column filled with the source name, which may be helpful for\n",
        "    # analysis.\n",
        "    df['source'] = source\n",
        "    df_list.append(df)\n",
        "\n",
        "df = pd.concat(df_list)\n",
        "df.sample(n=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5LdEDcofVyZH"
      },
      "source": [
        "Machine learning models expect separate arrays for input features and labels. We must store all reviews in one array and all corresponding labels in another array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yd4-rMqCVsOM",
        "colab": {}
      },
      "source": [
        "reviews = df['review'].values\n",
        "y = df['label'].values\n",
        "\n",
        "print('first review: \"{}\"'.format(reviews[0]))\n",
        "print('first label: {}'.format(y[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "awhrphCCpc0m"
      },
      "source": [
        "Finally, let's split the consolidated dataset so that 80% is used for training and the other 20% is used for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CPz5DCClTOWQ",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "reviews_train_raw, reviews_test_raw, y_train, y_test = train_test_split(\n",
        "  reviews, y, test_size=0.2, random_state=1000)\n",
        "\n",
        "print(len(y_train), len(y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Goq0Frawyuk"
      },
      "source": [
        "## Feature Extraction\n",
        "\n",
        "We will manually extract features from the raw text to use as input vectors for our first model. Remember that a bag-of-words model that does not consider the order of words in the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ED2m41ZfH9Vq"
      },
      "source": [
        "### spaCy: Industrial-strength NLP\n",
        "\n",
        "[spaCy](https://spacy.io) is a library for advanced NLP tools. It's built based on state-of-the-art research and designed to be efficient for industry use. spaCy is extremely useful for extracting more complex linguistic features from text. Another mature and popular Python NLP toolkit is [NLTK](https://www.nltk.org/), which is a little more academic-oriented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8hymZPfrRCYE"
      },
      "source": [
        "We must specify a linguistic model for spaCy to use. For this exercise, we'll use their \"medium-sized\" English language model. If you already have this model downloaded, you can skip to the `load` step below.\n",
        "\n",
        "**Note:** This is a large file, and may take a few minutes to download and process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "euRZuE9MLHd0",
        "colab": {}
      },
      "source": [
        "# Download the en_core_web_md model, if you don't already have it downloaded.\n",
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "en6zi294NrUn",
        "colab": {}
      },
      "source": [
        "# Load the model into our program.\n",
        "import en_core_web_md\n",
        "nlp = en_core_web_md.load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YhO3OKgoudYt"
      },
      "source": [
        "spaCy language models process raw text into a `Doc` object, which is a collection of `Token` objects. Each `Token` contains many useful [linguistic annotations](https://spacy.io/usage/linguistic-features). For example, `.text` stores the raw text of a `Token` and `.pos_` stores its Part of Speech (pos) tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4OxssSezviFt",
        "colab": {}
      },
      "source": [
        "tokens = nlp(reviews[0])\n",
        "for token in tokens:\n",
        "  print(token.text, token.pos_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FsWDY_aiwKSl"
      },
      "source": [
        "For our relatively small sentiment analysis task, we will augment each review with Part-of-Speech tags for each word in the review. Since we are using a bag-of-words approach, we can add these tags anywhere in the review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SuHvrx_GuG8H",
        "colab": {}
      },
      "source": [
        "# Given a review, adds a part-of-speech tag for each word after that word.\n",
        "def add_pos_tags(reviews_raw):\n",
        "  reviews = []\n",
        "  for i, review in enumerate(reviews_raw):\n",
        "    tokens = nlp(review)\n",
        "    review_with_pos = []\n",
        "    for token in tokens:\n",
        "      review_with_pos.append(token.text)\n",
        "      review_with_pos.append(token.pos_)\n",
        "    reviews.append(' '.join(review_with_pos))\n",
        "  return reviews"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-puZQHmT_ocQ",
        "colab": {}
      },
      "source": [
        "# These may take a litle while to run, as spaCy needs to parse each review.\n",
        "reviews_train = add_pos_tags(reviews_train_raw)\n",
        "reviews_test = add_pos_tags(reviews_test_raw)\n",
        "\n",
        "print(reviews_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S7mnDwNQW5Y4"
      },
      "source": [
        "## Bag-of-Words Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mNXIGKm4yTA1"
      },
      "source": [
        "We can't use these sentences directly to train models; we need to convert them into standardized-length vectors first. We will first use a bag-of-words (BOW) approach to vectorize the sentences. This means we will consider each review as a \"bag of words,\" where the order of the words does not matter.\n",
        "\n",
        "Sometimes our approaches must sacrifice potentially useful information (like the order of words in a sentence) in exchange for lower computational complexity. In other words, the importance of the order of the words is less than the increase in computational time or memory accounting for it would require. The relatively simple bag-of-words model has proven to be surprisingly effective for many problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yI0swU-j3SW_"
      },
      "source": [
        "The [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) in scikit-learn is a very useful tool for performing a bag-of-words vectorization. `CountVectorizer` supports more advanced feature extraction as well, such as n-grams, but we will use the default parameters for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EKv2hkxFWVKH",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(reviews_train)\n",
        "\n",
        "x_train = vectorizer.transform(reviews_train)\n",
        "x_test = vectorizer.transform(reviews_test)\n",
        "\n",
        "x_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uYV8zHsPuWqh"
      },
      "source": [
        "Let's take a closer look at what `CountVectorizer` did. First, `fit` creates a dictionary (\"vocabulary\"), mapping each unique word to a word index. Then, `transform` converts each review to a list of *counts*, where the element at index $i$ corresponds to the number of times the word at index $i$ in the vocabulary appeared in that review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_b860JlFu3cv",
        "colab": {}
      },
      "source": [
        "len(vectorizer.vocabulary_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fJQe83ZtHabt"
      },
      "source": [
        "### Logistic Regression model\n",
        "\n",
        "A common first model to try with classification problems is [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), because it trains fairly quickly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G3-hKCOls70F",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "print('Training accuracy: {}'.format(model.score(x_train, y_train)))\n",
        "print('Testing accuracy: {}'.format(model.score(x_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-dV5JPI6cIFW"
      },
      "source": [
        "Notice that this model achieves almost perfect accuracy on the training set but much lower accuracy on the testing set. This is a result of our model overfitting to the training data. To reduce this effect, we could try changing parameters of the [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model or reducing the number of features in the input data.\n",
        "\n",
        "We could also try using a completely different model, which we'll do now, adding in the information that bag-of-words ignores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w9Clq0KWL2dD"
      },
      "source": [
        "## Sequential model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f8DaPLm2tki9"
      },
      "source": [
        "Much of the meaning of language depends on the order of words: \"That movie was not really good\" is not quite the same as \"That movie was really not good\". For more complicated NLP tasks, a bag-of-words approach does not capture enough useful information. In this section, we will instead work with a Recurrent Neural Network (RNN) model, which is specifically designed to capture information about the order of sequences. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GnTNKN6-IDf6"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "We can't use `CountVectorizer` here, so we will need to do some slightly different preprocessing. We can first use the `keras` `Tokenizer` to learn a vocabulary, and transform each review into a list of indices. Note that we will not include part-of-speech information for this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LfcdK_js_dQd",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(reviews_train_raw)\n",
        "\n",
        "x_train = tokenizer.texts_to_sequences(reviews_train_raw)\n",
        "x_test = tokenizer.texts_to_sequences(reviews_test_raw)\n",
        "\n",
        "print(reviews_train_raw[0])\n",
        "print(x_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MpjwSl-cXioW"
      },
      "source": [
        "We need to pad our input so all vectors have the same length. A quick histogram of review lengths shows that almost all reviews have fewer than 100 words. Let's take a closer look at the distribution of lengths less than 100 words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wxrSvmFVWnes",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "review_lengths = [len(review) for review in x_train if len(review) < 100]\n",
        "plt.hist(review_lengths, density=True, cumulative=True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TD1eYtMgKm_z"
      },
      "source": [
        "Almost all reviews have fewer than 50 words! Therefore, will pad to a maximum review length of 50."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j2-Vzg7YpILj",
        "colab": {}
      },
      "source": [
        "maxlen = 50\n",
        "\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, padding='post',\n",
        "                                                     maxlen=maxlen)\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(x_test, padding='post',\n",
        "                                                    maxlen=maxlen)\n",
        "\n",
        "print(x_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i9g65GfQ920_"
      },
      "source": [
        "### Pre-trained word embeddings\n",
        "\n",
        "Word embeddings are foundational to most NLP tasks. It's common to experiment with embeddings, feature extraction, or a combination of both to determine what works best with your specific data and problem.\n",
        "\n",
        "In practice, instead of training our own embeddings, we can often take advantage of existing embeddings that have already been trained. This is especially useful when we have a small dataset, and want or need the richer meaning that comes from embeddings trained on a larger dataset. \n",
        "\n",
        "There are a variety of extensively pre-trained word embeddings. One of the most powerful and widely-used is [GloVe (Global Vectors for Word Representation)](https://nlp.stanford.edu/projects/glove/). Luckily for us, the spaCy model we downloaded is already integrated with 300-dimensional GloVe embeddings. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Knn-312W921J"
      },
      "source": [
        "All we need to do is load these embeddings into an `embedding_matrix` so each word index properly matches with the words in our dataset. We can access the `tokenizer`'s vocabulary using `.word_index`.\n",
        "\n",
        "*Note: This may take a few minutes to run.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Y0-sVSeuCW6",
        "colab": {}
      },
      "source": [
        "# Include an extra index for the \"<PAD>\" token.\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  token = nlp(word)[0]\n",
        "  # Make sure spaCy has an embedding for this token.\n",
        "  if not token.is_oov:\n",
        "    embedding_matrix[i] = token.vector\n",
        "\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NRgcHn2WMxwH"
      },
      "source": [
        "Loading the embeddings may take a little while to run. When it's done, we'll have an `embedding_matrix` where each word index corresponds to a 300-dimensional GloVe vector. We can load this into an `Embedding` layer to train a model, or visualize the embeddings.\n",
        "\n",
        "Note also that we have slightly more tokens now than from using `CountVectorizer`. This means that Keras's `Tokenizer` splits sentences into tokens using slightly different rules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v2XyVjq4KBxv"
      },
      "source": [
        "## RNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JFw-Fd4YFZZR"
      },
      "source": [
        "### Model setup\n",
        "\n",
        "This model will have three layers:\n",
        "\n",
        "1. `Embedding`\n",
        "\n",
        "   We initialize its weights using the `embedding_matrix` of pre-trained GloVe embeddings. We set `trainable=False` to prevent the weights from being updated during training. You can keep `trainable=True` to allow for additional training, or \"fine-tuning\", of these weights. We also set `mask_zero=True` to ensure we do not train parameters based on the `\"<PAD>\"` tokens.\n",
        "\n",
        "2. `LSTM` (Long Short-Term Memory)\n",
        "\n",
        "   This is a type of RNN architecture that is especially good at handling long sequences of information. This layer takes input of dimensions `(batch size, maxlen, embedding dimension)` and returns output of dimension `(batch size, 64)`. A larger output size means a more complex model; we have chosen 64 after tuning based on model performance.\n",
        "\n",
        "3. `Dense`\n",
        "\n",
        "   A final layer to return a prediction of either positive or negative sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GXGR-ledW0KK",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([\n",
        "  keras.layers.Embedding(\n",
        "    vocab_size,\n",
        "    embedding_dim,\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=False,\n",
        "    mask_zero=True\n",
        "  ),\n",
        "  keras.layers.LSTM(64),\n",
        "  keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VbPiVaMfRnZm"
      },
      "source": [
        "### Train and test model\n",
        "\n",
        "We will train this model for 10 epochs since it is slower to train per epoch and reaches high training accuracy after 10 epochs. We use a batch size of 64 based on hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ios8hOAOr6dy",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "  loss='binary_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size=64\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1ZjxVWCUr7l7",
        "colab": {}
      },
      "source": [
        "loss, acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy: {}'.format(acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rpSjxlrOeonH"
      },
      "source": [
        "Note that the final testing set accuracy is not significantly higher than that of our Logistic Regression model. We are using a complex model on a small dataset, which is prone to overfitting -- you can usually achieve more generalizable results with a larger dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SdqWCg3eLldl"
      },
      "source": [
        "# Exercises\n",
        "\n",
        "Natural Language Processing is a broad and quickly-changing field. These are just a few of the questions you can ask yourself as you approach NLP tasks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xs-QlHuc6IbK"
      },
      "source": [
        "## Exercise 1: Feature Experimentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "axURTCgJwOjY"
      },
      "source": [
        "Try to improve the test accuracy of either the Logistic Regression or RNN model. Techniques you could try include:\n",
        "\n",
        "* Using bigrams or larger n-grams as features\n",
        "* Ignoring \"stop words\" (very common words, e.g. \"the\", \"and\")\n",
        "* Including other types of features from spaCy\n",
        "* Adding regularization to either model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qf0XIoNI5Ejl"
      },
      "source": [
        "### Student Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sz5mgDO2XVtt"
      },
      "source": [
        "**Your analysis here:**\n",
        "\n",
        "*Comment on what you tried, and why it did or did not work.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NlafXcm_wMNc",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE ###\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-1-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-1-solution-1"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HAZ_xbII5Ipp",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fpCv9vyT5KmR"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-1XI0Vw65L8M",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OUnLTPP8jn5y"
      },
      "source": [
        "## Exercise 2: Error Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wu6jWTchF2qD"
      },
      "source": [
        "Find an example of a review where the RNN and Logistic Regression models made different predictions. Based on the review text and what you know about each model, why do you think this happened?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oZhGIexP5NN6"
      },
      "source": [
        "### Student Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vU3KFvvlXigw"
      },
      "source": [
        "**Your answer here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-2-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-2-solution-1"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KNlBuW1B5RSx",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "psDT6sxB5Sw3"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4aSSQ9aj5UAm",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KCwyrIppjlFq"
      },
      "source": [
        "## Exercise 3: What's Your Source?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uQ0sIpPjy0IP"
      },
      "source": [
        "Our dataset uses reviews from 3 sources, but each of these sources may have different patterns of sentiment. Do our models make better predictions for one source versus the others? Can we achieve better performance for the IMDB reviews by just training on data from IMDB, or does having a variety of sources help?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n2HshOA85VRS"
      },
      "source": [
        "### Student Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AFNGLjvOYVNk"
      },
      "source": [
        "**Your analysis here:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GfC9J6K5YVNo",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE ###\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-3-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-3-solution-1"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yOyDgONF5d0a",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "11OpDLa95e4S"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i1N4Kw895f6T",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KDf5-DpijdQN"
      },
      "source": [
        "## Exercise 4: Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T7EgQSQCwjrK"
      },
      "source": [
        "When loading word embeddings, we skipped all words that did not have a corresponding `GloVe` embedding. Let's take a closer look at these skipped words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UbCS3sqFY1oA",
        "colab": {}
      },
      "source": [
        "for word, i in tokenizer.word_index.items():\n",
        "  token = nlp(word)[0]\n",
        "  if token.is_oov:\n",
        "    print(token.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1OSNqr0BY-rk"
      },
      "source": [
        "Some of these are usernames or nonsense strings are spelling errors. Some are unintentional errors, in which case it'd be useful to recover the original word, while others are intentional errors, like \"waaaaaayyyyyyyyyy\", that are probably strong indicators of sentiment.\n",
        "\n",
        "In fact, this is a common problem when working with natural language: it's messy! That makes data preprocessing extremely difficult and vital. How could you improve our preprocessing to handle spelling errors, either intentional or unintentional?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BZ-c6uU05hdM"
      },
      "source": [
        "### Student Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p-fQklLTaQlD"
      },
      "source": [
        "**Your answer here:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tBTnDLI8ZBry",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE (if you write any) ###\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-4-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-4-solution-1"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VHc3FlPq5nzx",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ALMFk6qk5oiR"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MvNIJkxn5pyo",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}