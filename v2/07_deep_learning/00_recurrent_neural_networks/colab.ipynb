{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recurrent Neural Networks",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "5b8JqGUoPG78",
        "GH75xsE9eLRe",
        "T1KddFUTgjLF",
        "AuVUxQZjhtKl",
        "7TeNd4Bgjf8C"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b8JqGUoPG78",
        "colab_type": "text"
      },
      "source": [
        "#### Copyright 2019 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAAE3rDaPMGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnD_6o5-XdIb",
        "colab_type": "text"
      },
      "source": [
        "# Recurrent Neural Networks (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HLFzlpOXgq4",
        "colab_type": "text"
      },
      "source": [
        "Recurrent Neural Networks (RNNs) are an interesting application of deep learning that allow models to predict the future. While regression models attempt to fit an equation to existing data and extend the predictive power of the equation into the future, RNNs instead fit a model and use sequences of time series data to make step-by-step predictions about the next most likely output of the model.\n",
        "\n",
        "In this colab, we will generate some artificial stock price data and then create a recurrent neural network that can predict the price of that stock into the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq5Zt4HUWiLt",
        "colab_type": "text"
      },
      "source": [
        "## Generating the Dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSM_euzXWnll",
        "colab_type": "text"
      },
      "source": [
        "Getting familiar with models is often easier when we use an artificial dataset. The data is cleaner and we know the equation that is used to create the data. This means there is definitely some pattern for our model, and we know what it is.\n",
        "\n",
        "Let's start by importing NumPy and setting a random seed for reproducibility. Remember that you probably shouldn't set a random seed in your real code. In computers random numbers aren't really random and setting the seed allows us to use the same \"randomness\" for the sake of illustration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Uqpe9eAZvkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(1979)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjDqLSLNZ1N0",
        "colab_type": "text"
      },
      "source": [
        "We will create stock data for a fictional company. Let's create 10 years of stock data for a company that had and [initial public offering](https://en.wikipedia.org/wiki/Initial_public_offering) (IPO) of $5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLQbH7Xtagst",
        "colab_type": "text"
      },
      "source": [
        "Let's first create an array containing the price at the end of the day for our fictional stock for 10 years. Let's assume every day is a trading day so that we don't have to contend with weekends. And let's assume that every year has 365 days, for simplicity.\n",
        "\n",
        "For now, we'll just set the price at each day at zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzxIIxzBax96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "days_per_year = 365\n",
        "years = 10\n",
        "\n",
        "eod_prices = np.zeros(days_per_year*years)\n",
        "eod_prices.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktMtnCnjcAzH",
        "colab_type": "text"
      },
      "source": [
        "We can now set the price at the IPO (day 0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnsmxiAAcEjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eod_prices[0] = 5.0\n",
        "\n",
        "eod_prices[0:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luH3oiUYbiD0",
        "colab_type": "text"
      },
      "source": [
        "Starting with the $5 IPO, let's fill the remaining prices with random data added to the IPO price with a slight positive bias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySIvx67mbFas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bias = 0.0005\n",
        "\n",
        "for i in range(1, len(eod_prices)):\n",
        "  # Find yesterday's price.\n",
        "  yesterdays_price = eod_prices[i-1]\n",
        "  \n",
        "  # Generate a random a percentage change on the normal curve.\n",
        "  percentage_change = np.random.randn(1)[0]\n",
        "  \n",
        "  # The random number is a value on the standard normal distribution\n",
        "  # with a mean 0 and variance 1. This will give us a nice range of\n",
        "  # positive and negative values, but we need to divide by 100 to scale\n",
        "  # them down to reasonable percentages for daily stock price changes.\n",
        "  percentage_change /= 100\n",
        "\n",
        "  # And finally we give the change just a little bit of positive bias\n",
        "  # so that we get a nice growth curve.\n",
        "  percentage_change += bias\n",
        "  \n",
        "  # Calculate the new price.\n",
        "  todays_price = yesterdays_price + yesterdays_price * percentage_change\n",
        "  \n",
        "  # Store the price.\n",
        "  eod_prices[i] = todays_price"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PJeAYSEagTs",
        "colab_type": "text"
      },
      "source": [
        "We'll now use matplotlib to plot the values and make sure that it looks like a reasonable stock chart."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geUfAGJZdNTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(list(range(len(eod_prices))), eod_prices)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZByj8JneyHT",
        "colab_type": "text"
      },
      "source": [
        "Nice! That actually looks like a stock chart, for a well-performing company. We'll work from this dataset to predict stock prices for this company."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7_dmDYIa6bY",
        "colab_type": "text"
      },
      "source": [
        "## Configuring the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrksrA7-cAnW",
        "colab_type": "text"
      },
      "source": [
        "We now have a dataset of stock prices that we can use to train our model. Next, we'll build and train a model that can be used to make predictions about stock prices.\n",
        "\n",
        "There are four primary factors that we need to consider at this point:\n",
        "\n",
        "1. How many input features do we have?\n",
        "1. How many output targets do we want?\n",
        "1. How many time series steps do we want to feed the model?\n",
        "1. How many layers of neurons compose the hidden layer of the model?\n",
        "\n",
        "Let's address each of these factors in turn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad7JTRmLdjUs",
        "colab_type": "text"
      },
      "source": [
        "**How many input features do we have?**\n",
        "\n",
        "In this case, we only have one: stock price.\n",
        "\n",
        "We could have any number of other features, such as:\n",
        "- the overall price of the market\n",
        "- the price of the [S&P 500](https://en.wikipedia.org/wiki/S%26P_500_Index)\n",
        "\n",
        "We could even include features that may be less useful (but who knows!), such as:\n",
        "- the weather in Nantucket\n",
        "- the phase of the moon\n",
        "\n",
        "Feature engineering and selection is another topic that we've talked about in this course. When you are building your real-world models, you'll want to find features that have predictive power for your model. Don't restrict yourself to just one feature if you have other valuable inputs, but also realize that adding features adds computational cost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT0j4iZSen64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_inputs = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXGF7-vjeyx-",
        "colab_type": "text"
      },
      "source": [
        "**How many output targets do we want?**\n",
        "\n",
        "We are only interested in predicting the stock price on a given day, so we only want one prediction out of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qmq_lylpfjvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_outputs = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q9kkxi8fl1V",
        "colab_type": "text"
      },
      "source": [
        "**How many time series steps do we want to feed the model?**\n",
        "\n",
        "Next we need to think about how many steps through time that we want to feed the model each time we pass it training data. This parameter is a little more nebulous than the input and output parameters that we have set so far.\n",
        " \n",
        "The number of steps through time is the window that your model will see the world. If you make that window too small, the model won't be able to detect large cyclical patterns. Larger data can take longer to train on though.\n",
        " \n",
        "We'll start with a value of 100. 100 days is just over an accounting quarter, which might be a big enough window to detect some patterns. Your selection for models you build will need to be tested and tuned to find an optimal value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9u6j5D1i2wV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_steps = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL_tu0xZi5VY",
        "colab_type": "text"
      },
      "source": [
        "**How many layers of neurons compose the hidden layer of the model?**\n",
        "\n",
        "Earlier we chose to have 100 steps, which is the depth of the network unrolled over time. Units refers to the width of the network at each point in time.\n",
        "\n",
        "This is another one of those hyperparameters that will require some experimentation. In our case we will arbitrarily choose 25 as a starting value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMRNQdOwj5Qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_units = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9adgpJ24uJab",
        "colab_type": "text"
      },
      "source": [
        "Before creating the RNN we also reset the default graph. This is a necessity particular to Colab/Jupyter. In these environments you'll find yourself running the same block of code over and over. The underlying environment just needs to know that it is okay to start over.\n",
        "\n",
        "Technically this isn't necessary the first time you run this code, but if you want to re-run it you should reset the graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJKDU7pQuBaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaTStCrUeltr",
        "colab_type": "text"
      },
      "source": [
        "We now need to create a placeholder to hold the sequence of inputs that will be provided to the model over time. In this case we are creating a placeholder that expects a single input over 100 time steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QJLh6P3egQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STzBZD_Svp0g",
        "colab_type": "text"
      },
      "source": [
        "A placeholder for outputs also needs to be created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUUkb0TCvtNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzTcSk3ae53C",
        "colab_type": "text"
      },
      "source": [
        "The next step is to build a basic RNN cell. We tell the cell the number of units in each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y69NtxT-fpeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_units)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj-P4wgLgqlf",
        "colab_type": "text"
      },
      "source": [
        "We have a model that is 25 units wide, but all we are trying to do is predict a single stock price.\n",
        "\n",
        "We need to bring that wide array of output down to a regression that we can use. `OutputProjectionWrapper` is capibable of doing just that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoSr5dMThE03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cell = tf.contrib.rnn.OutputProjectionWrapper(rnn_cell, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM8Pg6D0hOXR",
        "colab_type": "text"
      },
      "source": [
        "And now we just need to wrap our RNN cell in a dynamic RNN wrapper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZVNyigIhNx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZsPrXLfu5Is",
        "colab_type": "text"
      },
      "source": [
        "The model will be tuned using an optimizer. In this case an `AdamOptimizer` will be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYFbRKA6vRI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.001\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOSQ65htvXZ0",
        "colab_type": "text"
      },
      "source": [
        "The optimizer must know what metric to optimize. We will use the mean error between the predictions and actual y values at each step of the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsQRMsAfAePg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = tf.reduce_mean(tf.square(outputs - y))\n",
        "training_op = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA_mrj4IwjQ1",
        "colab_type": "text"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jte2QUROxnYH",
        "colab_type": "text"
      },
      "source": [
        "In order to train the model we need to feed it two equal-length sequences of data, one starting at time N and the other starting an N+1. This will provide the model a single shift in time to train on.\n",
        "\n",
        "The `next_batch` function creates to equal-length arrays time shifted by one sample. The starting point for the arrays is randomly chosen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Own7bCDj1yW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def next_batch(batch_size, n_steps):\n",
        "  t0 = np.empty((batch_size, n_steps, 1))\n",
        "  t1 = np.empty((batch_size, n_steps, 1))\n",
        "\n",
        "  for i in range(batch_size):\n",
        "    start = np.random.randint(0, len(eod_prices) - n_steps - 1)\n",
        "    t0[i] = np.array(eod_prices[start:start+n_steps]).reshape(n_steps, 1)\n",
        "    t1[i] = np.array(eod_prices[start+1:start+n_steps+1]).reshape(n_steps, 1)\n",
        "\n",
        "  return t0, t1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH6PpgaDoB4w",
        "colab_type": "text"
      },
      "source": [
        "It is now time to decide on the number of training iterations that we want to perform and the size of the batch of training data that we want to feed the model.\n",
        "\n",
        "We have 3,650 data points. In the example below we choose to feed the optimizer 50 100-point time series lists in a batch and do that 10,000 times. This will allow the model to train on 5,000,000 data points in various batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP39VHpNAhMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_iterations = 10000\n",
        "batch_size = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbE44_GWq6RF",
        "colab_type": "text"
      },
      "source": [
        "Time to train the model. We'll initialize global variables and start a TensorFlow session. Then we'll loop through each iteration feeding a batch of data to the model and log the loss every iteration while reporting it every 500 iterations.\n",
        "\n",
        "*Note: This step might take a few minutes.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhWhoBuAqubV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_iter = []\n",
        "loss_mse = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver = tf.train.Saver()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for iteration in range(n_iterations):\n",
        "    Xb, yb = next_batch(batch_size, n_steps)\n",
        "    sess.run(training_op, feed_dict={X: Xb, y: yb})\n",
        "    mse = loss.eval(feed_dict={X: Xb, y: yb})\n",
        "    loss_iter.append(iteration)\n",
        "    loss_mse.append(mse)\n",
        "    if iteration % 500 == 0:\n",
        "      print(f'Iter {iteration}: MSE: {mse}')\n",
        "      \n",
        "  saver.save(sess, \"./stock_rnn\")\n",
        "\n",
        "print(f'Final MSE: {loss_mse[-1]}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB_hkENqvNRm",
        "colab_type": "text"
      },
      "source": [
        "Since we stored the loss at every iteration, we can visualize the model learning over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq_4cnTytTfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(loss_iter, loss_mse)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilk2kXwaPVRJ",
        "colab_type": "text"
      },
      "source": [
        "## Making Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC64TNU2Pcnq",
        "colab_type": "text"
      },
      "source": [
        "Our model is now trained and it should have done a pretty good job training to predict stock prices. But how do we make predictions using the model?\n",
        "\n",
        "To do that we can feed the model stock prices for the past 100 days (our unrolled time series size) and see what the model thinks the price will be on the 101st day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb6G1JwHQTYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess: \n",
        "  saver.restore(sess, \"./stock_rnn\")\n",
        "\n",
        "  X_batch, _ = next_batch(1, n_steps) # get 100 data points\n",
        "  y_pred = sess.run(outputs, feed_dict={X: [X_batch[0]]})\n",
        "  \n",
        "  today = X_batch[0][-1][0]\n",
        "  tomorrow = y_pred[0][-1][0]\n",
        "\n",
        "  print(f'Price today: {today:0.2f}, price tomorrow: {tomorrow:0.2f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiI5Zzq7Zn1L",
        "colab_type": "text"
      },
      "source": [
        "We can plot our predictions against the actual values of our stock prices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gOuSRvRpOj1n",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predictions = []\n",
        "\n",
        "with tf.Session() as sess: \n",
        "  saver.restore(sess, \"./stock_rnn\")\n",
        "\n",
        "  for i in range(0, len(eod_prices)-n_steps, n_steps):\n",
        "    y_pred = sess.run(outputs, feed_dict={\n",
        "        X: [np.array(eod_prices[i:i+n_steps]).reshape(n_steps, 1)]})\n",
        "    predictions.extend([x[0] for x in y_pred[0]])\n",
        "\n",
        "plt.plot(list(range(len(eod_prices[1:len(predictions)]))),\n",
        "         eod_prices[1:len(predictions)], color='green')\n",
        "plt.plot(list(range(len(predictions[0:len(predictions)-1]))),\n",
        "         predictions[0:len(predictions)-1], color='red')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlXtE7JlZyiu",
        "colab_type": "text"
      },
      "source": [
        "The lines seem to be fairly similar. In a real market situation, we'd never be this accurate... But it's fun to dream!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EjFEuw-CaDKp"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_9SaqCKaD1_",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 1: Adjust Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbZUPQqtaLkf",
        "colab_type": "text"
      },
      "source": [
        "Tuning a machine learning model can be as much of an art as it is a science. In this exercise we will adjust the number of time steps that are unrolled by the model and examine the results.\n",
        "\n",
        "Keeping the random seeds the same between runs, adjust the step size to:\n",
        "\n",
        "1. 10\n",
        "1. 50\n",
        "1. 100\n",
        "1. 300\n",
        "1. 500\n",
        "\n",
        "Store the values for each step size in a five-element iterable called `error_rates`.\n",
        "\n",
        "```\n",
        "error_rates = [0.0, 1.0, 2.0, 3.0, 4.0]\n",
        "```\n",
        "\n",
        "As a starting point, the combined code for the model is below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U09j1huye8jr",
        "colab_type": "text"
      },
      "source": [
        "### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-VABqanaaNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "error_rates = [] # TBD\n",
        "\n",
        "n_inputs = 1\n",
        "n_outputs = 1\n",
        "n_steps = 100\n",
        "n_units = 25\n",
        "batch_size = 50\n",
        "n_iterations = 10000\n",
        "\n",
        "tf.reset_default_graph()\n",
        "np.random.seed(1979)\n",
        "tf.random.set_random_seed(1979)\n",
        "\n",
        "loss_iter = []\n",
        "loss_mse = []\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n",
        "\n",
        "rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_units)\n",
        "cell = tf.contrib.rnn.OutputProjectionWrapper(rnn_cell, 1)\n",
        "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "loss = tf.reduce_mean(tf.square(outputs - y))\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver = tf.train.Saver()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for iteration in range(n_iterations):\n",
        "    Xb, yb = next_batch(batch_size, n_steps)\n",
        "    sess.run(training_op, feed_dict={X: Xb, y: yb})\n",
        "    mse = loss.eval(feed_dict={X: Xb, y: yb})\n",
        "    loss_iter.append(iteration)\n",
        "    loss_mse.append(mse)\n",
        "    if iteration % 500 == 0:\n",
        "      print(\"Iter {}: MSE: {}\".format(iteration, mse))\n",
        "\n",
        "  saver.save(sess, \"./stock_rnn\")\n",
        "\n",
        "print(f'Final MSE: {loss_mse[-1]}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH75xsE9eLRe",
        "colab_type": "text"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luCpuS9Oe_u0",
        "colab_type": "text"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfXHPY4QeM4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "error_rates = []\n",
        "\n",
        "n_inputs = 1\n",
        "n_outputs = 1\n",
        "step_values = [10, 50, 100, 300, 500]\n",
        "n_units = 25\n",
        "batch_size = 50\n",
        "n_iterations = 10000\n",
        "\n",
        "for n_steps in step_values:\n",
        "  tf.reset_default_graph()\n",
        "  np.random.seed(1979)\n",
        "  tf.random.set_random_seed(1979)\n",
        "\n",
        "  loss_iter = []\n",
        "  loss_mse = []\n",
        "\n",
        "  X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "  y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n",
        "\n",
        "  rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_units)\n",
        "  cell = tf.contrib.rnn.OutputProjectionWrapper(rnn_cell, 1)\n",
        "  outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
        "\n",
        "  learning_rate = 0.001\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.reduce_mean(tf.square(outputs - y))\n",
        "  training_op = optimizer.minimize(loss)\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    saver = tf.train.Saver()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for iteration in range(n_iterations):\n",
        "      Xb, yb = next_batch(batch_size, n_steps)\n",
        "      sess.run(training_op, feed_dict={X: Xb, y: yb})\n",
        "      mse = loss.eval(feed_dict={X: Xb, y: yb})\n",
        "      loss_iter.append(iteration)\n",
        "      loss_mse.append(mse)\n",
        "      if iteration % 500 == 0:\n",
        "        print(\"Iter {}: MSE: {}\".format(iteration, mse))\n",
        "\n",
        "    saver.save(sess, \"./stock_rnn\")\n",
        "\n",
        "  print(f'Final MSE at step size {n_steps}: {loss_mse[-1]}')\n",
        "  error_rates.append(loss_mse[-1])\n",
        "\n",
        "print(error_rates)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmvm-Mu7fT5S",
        "colab_type": "text"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2NFIN7XfVwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "final_error_rates = []\n",
        "\n",
        "n_inputs = 1\n",
        "n_outputs = 1\n",
        "step_values = [10, 50, 100, 300, 500]\n",
        "n_units = 25\n",
        "batch_size = 50\n",
        "n_iterations = 10000\n",
        "\n",
        "for n_steps in step_values:\n",
        "  tf.reset_default_graph()\n",
        "  np.random.seed(1979)\n",
        "  tf.random.set_random_seed(1979)\n",
        "\n",
        "  loss_iter = []\n",
        "  loss_mse = []\n",
        "\n",
        "  X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "  y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n",
        "\n",
        "  rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_units)\n",
        "  cell = tf.contrib.rnn.OutputProjectionWrapper(rnn_cell, 1)\n",
        "  outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
        "\n",
        "  learning_rate = 0.001\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.reduce_mean(tf.square(outputs - y))\n",
        "  training_op = optimizer.minimize(loss)\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    saver = tf.train.Saver()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for iteration in range(n_iterations):\n",
        "      Xb, yb = next_batch(batch_size, n_steps)\n",
        "      sess.run(training_op, feed_dict={X: Xb, y: yb})\n",
        "      mse = loss.eval(feed_dict={X: Xb, y: yb})\n",
        "      loss_iter.append(iteration)\n",
        "      loss_mse.append(mse)\n",
        "      if iteration % 500 == 0:\n",
        "        print(\"Iter {}: MSE: {}\".format(iteration, mse))\n",
        "\n",
        "    saver.save(sess, \"./stock_rnn\")\n",
        "\n",
        "  print(f'Final MSE at step size {n_steps}: {loss_mse[-1]}')\n",
        "  final_error_rates.append(loss_mse[-1])\n",
        "\n",
        "if error_rates == final_error_rates:\n",
        "  print('LGTM')\n",
        "else:\n",
        "  print(f'Wanted error rates {final_error_rates}, got {error_rates}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etH21lVZgCo5",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 2: Adjust the Iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9W1_CX0gMPG",
        "colab_type": "text"
      },
      "source": [
        "In this exercise we will adjust the number of iterations used by the model and examine the results.\n",
        "\n",
        "Keeping the random seeds the same between runs, adjust the step size to:\n",
        "\n",
        "1. 10\n",
        "1. 100\n",
        "1. 1000\n",
        "1. 10000\n",
        "\n",
        "Store the values for each step size in a four-element iterable called `error_rates`.\n",
        "\n",
        "```\n",
        "  error_rates = [0.0, 1.0, 2.0, 3.0]\n",
        "```\n",
        "\n",
        "As a starting point, the combined code for the model is below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QY9zSr5gdma",
        "colab_type": "text"
      },
      "source": [
        "### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUZCFq3tgfIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "error_rates = [] # TBD\n",
        "\n",
        "n_inputs = 1\n",
        "n_outputs = 1\n",
        "n_steps = 100\n",
        "n_units = 25\n",
        "batch_size = 50\n",
        "n_iterations = 10000\n",
        "\n",
        "tf.reset_default_graph()\n",
        "np.random.seed(1979)\n",
        "tf.random.set_random_seed(1979)\n",
        "\n",
        "loss_iter = []\n",
        "loss_mse = []\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n",
        "\n",
        "rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_units)\n",
        "cell = tf.contrib.rnn.OutputProjectionWrapper(rnn_cell, 1)\n",
        "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "loss = tf.reduce_mean(tf.square(outputs - y))\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver = tf.train.Saver()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for iteration in range(n_iterations):\n",
        "    Xb, yb = next_batch(batch_size, n_steps)\n",
        "    sess.run(training_op, feed_dict={X: Xb, y: yb})\n",
        "    mse = loss.eval(feed_dict={X: Xb, y: yb})\n",
        "    loss_iter.append(iteration)\n",
        "    loss_mse.append(mse)\n",
        "    if iteration % 500 == 0:\n",
        "      print(\"Iter {}: MSE: {}\".format(iteration, mse))\n",
        "\n",
        "  saver.save(sess, \"./stock_rnn\")\n",
        "\n",
        "print(f'Final MSE: {loss_mse[-1]}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1KddFUTgjLF",
        "colab_type": "text"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laGEs-IwlCGX",
        "colab_type": "text"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiUTDo3vgk7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "error_rates = []\n",
        "\n",
        "n_inputs = 1\n",
        "n_outputs = 1\n",
        "n_steps = 50\n",
        "n_units = 25\n",
        "batch_size = 50\n",
        "iteration_size = [10, 100, 1000, 10000]\n",
        "\n",
        "for n_iterations in iteration_size:\n",
        "  tf.reset_default_graph()\n",
        "  np.random.seed(1979)\n",
        "  tf.random.set_random_seed(1979)\n",
        "\n",
        "  loss_iter = []\n",
        "  loss_mse = []\n",
        "\n",
        "  X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "  y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n",
        "\n",
        "  rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_units)\n",
        "  cell = tf.contrib.rnn.OutputProjectionWrapper(rnn_cell, 1)\n",
        "  outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
        "\n",
        "  learning_rate = 0.001\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.reduce_mean(tf.square(outputs - y))\n",
        "  training_op = optimizer.minimize(loss)\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    saver = tf.train.Saver()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for iteration in range(n_iterations):\n",
        "      Xb, yb = next_batch(batch_size, n_steps)\n",
        "      sess.run(training_op, feed_dict={X: Xb, y: yb})\n",
        "      mse = loss.eval(feed_dict={X: Xb, y: yb})\n",
        "      loss_iter.append(iteration)\n",
        "      loss_mse.append(mse)\n",
        "      if iteration % 500 == 0:\n",
        "        print(\"Iter {}: MSE: {}\".format(iteration, mse))\n",
        "\n",
        "    saver.save(sess, \"./stock_rnn\")\n",
        "\n",
        "  print(f'Final MSE at step size {n_steps}: {loss_mse[-1]}')\n",
        "  error_rates.append(loss_mse[-1])\n",
        "\n",
        "print(error_rates)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2EHikdphCNq",
        "colab_type": "text"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnKxPLcphEXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "final_error_rates = []\n",
        "\n",
        "n_inputs = 1\n",
        "n_outputs = 1\n",
        "n_steps = 50\n",
        "n_units = 25\n",
        "batch_size = 50\n",
        "iteration_size = [10, 100, 1000, 10000]\n",
        "\n",
        "for n_iterations in iteration_size:\n",
        "  tf.reset_default_graph()\n",
        "  np.random.seed(1979)\n",
        "  tf.random.set_random_seed(1979)\n",
        "\n",
        "  loss_iter = []\n",
        "  loss_mse = []\n",
        "\n",
        "  X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "  y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n",
        "\n",
        "  rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_units)\n",
        "  cell = tf.contrib.rnn.OutputProjectionWrapper(rnn_cell, 1)\n",
        "  outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
        "\n",
        "  learning_rate = 0.001\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.reduce_mean(tf.square(outputs - y))\n",
        "  training_op = optimizer.minimize(loss)\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    saver = tf.train.Saver()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for iteration in range(n_iterations):\n",
        "      Xb, yb = next_batch(batch_size, n_steps)\n",
        "      sess.run(training_op, feed_dict={X: Xb, y: yb})\n",
        "      mse = loss.eval(feed_dict={X: Xb, y: yb})\n",
        "      loss_iter.append(iteration)\n",
        "      loss_mse.append(mse)\n",
        "      if iteration % 500 == 0:\n",
        "        print(\"Iter {}: MSE: {}\".format(iteration, mse))\n",
        "\n",
        "    saver.save(sess, \"./stock_rnn\")\n",
        "\n",
        "  print(f'Final MSE at step size {n_steps}: {loss_mse[-1]}')\n",
        "  final_error_rates.append(loss_mse[-1])\n",
        "\n",
        "if error_rates == final_error_rates:\n",
        "  print('LGTM')\n",
        "else:\n",
        "  print(f'Wanted error rates {final_error_rates}, got {error_rates}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6FgxbswhcPi",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 3: Adjust the Units"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcnjnkVghe_g",
        "colab_type": "text"
      },
      "source": [
        "In this exercise we will adjust width of the layers in the model and examine the results.\n",
        "\n",
        "Keeping the random seeds the same between runs, adjust the unit size to:\n",
        "\n",
        "1. 10\n",
        "1. 25\n",
        "1. 50\n",
        "1. 100\n",
        "1. 200\n",
        "\n",
        "Store the values for each step size in a five-element iterable called `error_rates`.\n",
        "\n",
        "```\n",
        "  error_rates = [0.0, 1.0, 2.0, 3.0, 4.0]\n",
        "```\n",
        "\n",
        "As a starting point, the combined code for the model is below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4doxTQuOhqGt",
        "colab_type": "text"
      },
      "source": [
        "### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzgHOg0IhrWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "error_rates = [] # TBD\n",
        "\n",
        "n_inputs = 1\n",
        "n_outputs = 1\n",
        "n_steps = 100\n",
        "n_units = 25\n",
        "batch_size = 50\n",
        "n_iterations = 10000\n",
        "\n",
        "tf.reset_default_graph()\n",
        "np.random.seed(1979)\n",
        "tf.random.set_random_seed(1979)\n",
        "\n",
        "loss_iter = []\n",
        "loss_mse = []\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n",
        "\n",
        "rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_units)\n",
        "cell = tf.contrib.rnn.OutputProjectionWrapper(rnn_cell, 1)\n",
        "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "loss = tf.reduce_mean(tf.square(outputs - y))\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver = tf.train.Saver()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for iteration in range(n_iterations):\n",
        "    Xb, yb = next_batch(batch_size, n_steps)\n",
        "    sess.run(training_op, feed_dict={X: Xb, y: yb})\n",
        "    mse = loss.eval(feed_dict={X: Xb, y: yb})\n",
        "    loss_iter.append(iteration)\n",
        "    loss_mse.append(mse)\n",
        "    if iteration % 500 == 0:\n",
        "      print(\"Iter {}: MSE: {}\".format(iteration, mse))\n",
        "\n",
        "  saver.save(sess, \"./stock_rnn\")\n",
        "\n",
        "print(f'Final MSE: {loss_mse[-1]}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuVUxQZjhtKl",
        "colab_type": "text"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO9q55lZhvEU",
        "colab_type": "text"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm7mjrbuhwnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "error_rates = []\n",
        "\n",
        "n_inputs = 1\n",
        "n_outputs = 1\n",
        "n_steps = 50\n",
        "unit_sizes = [10, 25, 50, 100, 200]\n",
        "batch_size = 50\n",
        "n_iterations = 10000\n",
        "\n",
        "for n_units in unit_sizes:\n",
        "  tf.reset_default_graph()\n",
        "  np.random.seed(1979)\n",
        "  tf.random.set_random_seed(1979)\n",
        "\n",
        "  loss_iter = []\n",
        "  loss_mse = []\n",
        "\n",
        "  X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "  y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n",
        "\n",
        "  rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_units)\n",
        "  cell = tf.contrib.rnn.OutputProjectionWrapper(rnn_cell, 1)\n",
        "  outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
        "\n",
        "  learning_rate = 0.001\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.reduce_mean(tf.square(outputs - y))\n",
        "  training_op = optimizer.minimize(loss)\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    saver = tf.train.Saver()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for iteration in range(n_iterations):\n",
        "      Xb, yb = next_batch(batch_size, n_steps)\n",
        "      sess.run(training_op, feed_dict={X: Xb, y: yb})\n",
        "      mse = loss.eval(feed_dict={X: Xb, y: yb})\n",
        "      loss_iter.append(iteration)\n",
        "      loss_mse.append(mse)\n",
        "      if iteration % 500 == 0:\n",
        "        print(\"Iter {}: MSE: {}\".format(iteration, mse))\n",
        "\n",
        "    saver.save(sess, \"./stock_rnn\")\n",
        "\n",
        "  print(f'Final MSE at step size {n_steps}: {loss_mse[-1]}')\n",
        "  error_rates.append(loss_mse[-1])\n",
        "\n",
        "print(error_rates)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QFE1XkliP_X",
        "colab_type": "text"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-92IoVobiRjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "final_error_rates = []\n",
        "\n",
        "n_inputs = 1\n",
        "n_outputs = 1\n",
        "n_steps = 50\n",
        "unit_sizes = [10, 25, 50, 100, 200]\n",
        "batch_size = 50\n",
        "n_iterations = 10000\n",
        "\n",
        "for n_units in unit_sizes:\n",
        "  tf.reset_default_graph()\n",
        "  np.random.seed(1979)\n",
        "  tf.random.set_random_seed(1979)\n",
        "\n",
        "  loss_iter = []\n",
        "  loss_mse = []\n",
        "\n",
        "  X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "  y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n",
        "\n",
        "  rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_units)\n",
        "  cell = tf.contrib.rnn.OutputProjectionWrapper(rnn_cell, 1)\n",
        "  outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
        "\n",
        "  learning_rate = 0.001\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "  loss = tf.reduce_mean(tf.square(outputs - y))\n",
        "  training_op = optimizer.minimize(loss)\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    saver = tf.train.Saver()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for iteration in range(n_iterations):\n",
        "      Xb, yb = next_batch(batch_size, n_steps)\n",
        "      sess.run(training_op, feed_dict={X: Xb, y: yb})\n",
        "      mse = loss.eval(feed_dict={X: Xb, y: yb})\n",
        "      loss_iter.append(iteration)\n",
        "      loss_mse.append(mse)\n",
        "      if iteration % 500 == 0:\n",
        "        print(\"Iter {}: MSE: {}\".format(iteration, mse))\n",
        "\n",
        "    saver.save(sess, \"./stock_rnn\")\n",
        "\n",
        "  print(f'Final MSE at step size {n_steps}: {loss_mse[-1]}')\n",
        "  final_error_rates.append(loss_mse[-1])\n",
        "\n",
        "if error_rates == final_error_rates:\n",
        "  print('LGTM')\n",
        "else:\n",
        "  print(f'Wanted error rates {final_error_rates}, got {error_rates}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gn4dZ_Yirrw",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 4: Your Own RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9NkSokLiu2s",
        "colab_type": "text"
      },
      "source": [
        "So far we have only played with artificial time series data. For this exercise you will import [actual stock data from Kaggle](https://www.kaggle.com/szrlee/stock-time-series-20050101-to-20171231) and attempt to predict future prices.\n",
        "\n",
        "Download the [McDonald's stock data file](https://www.kaggle.com/szrlee/stock-time-series-20050101-to-20171231#MCD_2006-01-01_to_2018-01-01.csv) and create an RNN that will attempt to predict the closing price for the stock."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqAaucLZjcXm",
        "colab_type": "text"
      },
      "source": [
        "### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ai6SuKMjeAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code goes here."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TeNd4Bgjf8C",
        "colab_type": "text"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfyf_PkHjhkn",
        "colab_type": "text"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNwmasUpjjbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO(joshmcadams)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}