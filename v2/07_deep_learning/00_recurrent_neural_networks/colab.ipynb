{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recurrent Neural Networks",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "5b8JqGUoPG78",
        "45xublipBcE5",
        "NNwQ-S2KPVRI"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/applied-machine-learning-intensive/blob/master/v2/07_deep_learning/00_recurrent_neural_networks/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b8JqGUoPG78",
        "colab_type": "text"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAAE3rDaPMGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnD_6o5-XdIb",
        "colab_type": "text"
      },
      "source": [
        "# Recurrent Neural Networks (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HLFzlpOXgq4",
        "colab_type": "text"
      },
      "source": [
        "Recurrent Neural Networks (RNNs) are an interesting application of deep learning that allow models to predict the future. While regression models attempt to fit an equation to existing data and extend the predictive power of the equation into the future, RNNs instead fit a model and use sequences of time series data to make step-by-step predictions about the next most likely output of the model.\n",
        "\n",
        "In this colab, we will create a recurrent neural network that can predict engine vibrations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq5Zt4HUWiLt",
        "colab_type": "text"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PJeAYSEagTs",
        "colab_type": "text"
      },
      "source": [
        "We'll be using the [Engine Vibrations data](https://www.kaggle.com/joshmcadams/engine-vibrations) from Kaggle. This dataset contians artificial engine vibration values that we will use to train a model that can predict future values.\n",
        "\n",
        "To load the data, upload your `kaggle.json` file and run the code block below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i53xIkD-t-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! chmod 600 kaggle.json && (ls ~/.kaggle 2>/dev/null || mkdir ~/.kaggle) && mv kaggle.json ~/.kaggle/ && echo 'Done'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH3nEJSF_T6C",
        "colab_type": "text"
      },
      "source": [
        "Next, download the data from Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD0OyLnq_WYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle datasets download joshmcadams/engine-vibrations\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0gY53BBAE_q",
        "colab_type": "text"
      },
      "source": [
        "Now load the data into a `DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHkcYf8IAB9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('engine-vibrations.zip')\n",
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWW6wLrhtkGC",
        "colab_type": "text"
      },
      "source": [
        "We know that data contains readings of engine vibration over time. Let's see how that looks on a line chart."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geUfAGJZdNTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(24, 8))\n",
        "plt.plot(list(range(len(df['mm']))), df['mm'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rpb9dOHqt31W",
        "colab_type": "text"
      },
      "source": [
        "That's quite a tough chart to read. Let's sample it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khEnEOkgt8IS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(24, 8))\n",
        "plt.plot(list(range(100)), df['mm'].iloc[:100])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDzwV2rLucrc",
        "colab_type": "text"
      },
      "source": [
        "See if any of the data is missing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pzg5wM_uYjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.isna().any()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6Ez86Cmunox",
        "colab_type": "text"
      },
      "source": [
        "And finally we'll do a box plot to see if the data is evenly distributed, which is is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd0Ha-VOufGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "_ = sns.boxplot(df['mm'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9K3fN5_utmQ",
        "colab_type": "text"
      },
      "source": [
        "There is not much more EDA that we need to do at this point. Let's move into modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7_dmDYIa6bY",
        "colab_type": "text"
      },
      "source": [
        "## Peparing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMZXi7_pv-JM",
        "colab_type": "text"
      },
      "source": [
        "Currently we have series of data that contains a single list of vibration values over time. When training our model and when asking for predictions, we'll want to instead feed the model a subset of our sequence.\n",
        "\n",
        "We first need to determine our subsequence lengh and then create in-order subsequences of that length.\n",
        "\n",
        "We'll create a lists of lists called `X` that contains subsequences. We'll also create a list called `y` that contains the next value after each subsequence stored in `X`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kjythCQ-8xX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "sseq_len = 50\n",
        "for i in range(0, len(df['mm']) - sseq_len - 1):\n",
        "  X.append(df['mm'][i:i+sseq_len])\n",
        "  y.append(df['mm'][i+sseq_len+1])\n",
        "\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "\n",
        "X.shape, y.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAhFLYpsyYsP",
        "colab_type": "text"
      },
      "source": [
        "We also need to explicitly set the final dimension of the data in order to have it pass through our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSqMsXK4sYTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.expand_dims(X, axis=2)\n",
        "y = np.expand_dims(y, axis=1)\n",
        "\n",
        "X.shape, y.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4ARNFpJypCc",
        "colab_type": "text"
      },
      "source": [
        "We'll also standardize our data for the model. Note that we don't normalize here because we need to be able to reproduce negative values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnVcS3sxVgSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_std = df['mm'].std()\n",
        "data_mean = df['mm'].mean()\n",
        "\n",
        "X = (X - data_mean) / data_std\n",
        "y = (y - data_mean) / data_std\n",
        "\n",
        "X.max(), y.max(), X.min(), y.min()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7xD_PkVyyZR",
        "colab_type": "text"
      },
      "source": [
        "And for final testing after model training we'll split off 20% of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X2rePx2WjaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI6KMXyry75a",
        "colab_type": "text"
      },
      "source": [
        "## Setting a Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T7NIsw2y9qp",
        "colab_type": "text"
      },
      "source": [
        "We are only training with 50 data points at a time. This is well within the bounds of what a standard deep neural network can handle so let's first see what a very simple neural network can do."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5787JSoWbS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  keras.layers.Flatten(input_shape=[sseq_len, 1]),\n",
        "  keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  loss='mse',\n",
        "  optimizer='Adam',\n",
        "  metrics=['mae', 'mse'],\n",
        ")\n",
        "\n",
        "stopping = tf.keras.callbacks.EarlyStopping(\n",
        "  monitor='loss',\n",
        "  min_delta=0,\n",
        "  patience=2)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=50, callbacks=[stopping])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "rmse = math.sqrt(np.mean(keras.losses.mean_squared_error(y_test, y_pred)))\n",
        "print(\"RMSE Scaled: {}\\nRMSE Base Units: {}\".format(\n",
        "    rmse, rmse * data_std + data_mean))\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(list(range(len(history.history['mse']))), history.history['mse'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPr-FVCZ0BC-",
        "colab_type": "text"
      },
      "source": [
        "We quickly converged and got a baseline quality value of `0.03750885081060467`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJSxRb6i0LPo",
        "colab_type": "text"
      },
      "source": [
        "## The Most Basic RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaSl6sHz0SzH",
        "colab_type": "text"
      },
      "source": [
        "Let's contrast a basic feedforward neural network with a baisc RNN. To do this we simply need to use the [`SimpleRNN` layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN) in or network in the place of the `Dense` layer in our network above. Notice that there is no need to flatten the data before we feed it into the model in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btjmwN7eZvfP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(0)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  loss='mse',\n",
        "  optimizer='Adam',\n",
        "  metrics=['mae', 'mse'],\n",
        ")\n",
        "\n",
        "stopping = tf.keras.callbacks.EarlyStopping(\n",
        "  monitor='loss',\n",
        "  min_delta=0,\n",
        "  patience=2)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=50, callbacks=[stopping])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "rmse = math.sqrt(np.mean(keras.losses.mean_squared_error(y_test, y_pred)))\n",
        "print(\"RMSE Scaled: {}\\nRMSE Base Units: {}\".format(\n",
        "    rmse, rmse * data_std + data_mean))\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(list(range(len(history.history['mse']))), history.history['mse'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEzJ2o5N1h4Y",
        "colab_type": "text"
      },
      "source": [
        "Our model converged a little slower, but got an error of only `0.8974118571865628`, which is not an improvement over the baseline model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umjB2bG12AHi",
        "colab_type": "text"
      },
      "source": [
        "## A Deep RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q15DdJkM1-gm",
        "colab_type": "text"
      },
      "source": [
        "Let's try to build a deep RNN and see if we can get better results.\n",
        "\n",
        "In the model below we stich together four layers ranging in width from `50` nodes to our final output of `1`.\n",
        "\n",
        "Notice that all of the layers except the output layer have `return_sequences=True` set. This causes the layer to pass outputs for all time stamps to the next layer. If you don't include this argument, only the output for the last timestamp is passed and intermediate layers will complain about the wrong shape of input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BzFeGita_dx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(0)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(50, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
        "    keras.layers.SimpleRNN(10, return_sequences=True),\n",
        "    keras.layers.SimpleRNN(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  loss='mse',\n",
        "  optimizer='Adam',\n",
        "  metrics=['mae', 'mse'],\n",
        ")\n",
        "\n",
        "stopping = tf.keras.callbacks.EarlyStopping(\n",
        "  monitor='loss',\n",
        "  min_delta=0,\n",
        "  patience=2)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=50, callbacks=[stopping])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "rmse = math.sqrt(np.mean(keras.losses.mean_squared_error(y_test, y_pred)))\n",
        "print(\"RMSE Scaled: {}\\nRMSE Base Units: {}\".format(\n",
        "    rmse, rmse * data_std + data_mean))\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(list(range(len(history.history['mse']))), history.history['mse'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcWDYuDX3PmL",
        "colab_type": "text"
      },
      "source": [
        "Woah! What happened. Our mse during training looked nice: `0.0496`, but our final testing didn't performed much better than our simple model. We seem to have overfit!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpFsyAT0ZGRp",
        "colab_type": "text"
      },
      "source": [
        "We can try to simplify the model and add dropout layers to reduce overfitting, but even with a very basic model like the one below we still get very different MSE between the training and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nscnyybb06H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(0)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(2, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.SimpleRNN(1),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  loss='mse',\n",
        "  optimizer='Adam',\n",
        "  metrics=['mae', 'mse'],\n",
        ")\n",
        "\n",
        "stopping = tf.keras.callbacks.EarlyStopping(\n",
        "  monitor='loss',\n",
        "  min_delta=0,\n",
        "  patience=2)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=50, callbacks=[stopping])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "rmse = math.sqrt(np.mean(keras.losses.mean_squared_error(y_test, y_pred)))\n",
        "print(\"RMSE Scaled: {}\\nRMSE Base Units: {}\".format(\n",
        "    rmse, rmse * data_std + data_mean))\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(list(range(len(history.history['mse']))), history.history['mse'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgStKfGWFnO0",
        "colab_type": "text"
      },
      "source": [
        "Even with these measures we still seem to be overfitting a bit. We could keep tuning, but instead let's look at some other types of neurons found in RNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNKv2NFEZfGc",
        "colab_type": "text"
      },
      "source": [
        "## Long Short Term Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4v2XkdYZmuY",
        "colab_type": "text"
      },
      "source": [
        "The RNN layers that we have been using are basic neurons that have a very short memory. They tend to learn patterns that they have recently seen, but quickly forget older training data.\n",
        "\n",
        "The **Long Short Term Memory (LSTM)** neruon was built to combat this forgetfulness. The neuron outputs values for the next layer in the network, but also outputs two other values: one for short term memory and one for long term memory. These weights are then fed back into the neuron at the next iteration of the network. This backfeed is similar to that of a `SimpleRNN`, expect that the `SimpleRNN` only has one backfeed.\n",
        "\n",
        "We can replace the `SimpleRNN` with an `LSTM` layer as you can see below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_VG4uC0KsKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(0)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.LSTM(1, input_shape=[None, 1]),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  loss='mse',\n",
        "  optimizer=tf.keras.optimizers.Adam(),\n",
        "  metrics=['mae', 'mse'],\n",
        ")\n",
        "\n",
        "stopping = tf.keras.callbacks.EarlyStopping(\n",
        "  monitor='loss',\n",
        "  min_delta=0,\n",
        "  patience=2)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=100, callbacks=[stopping])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "rmse = math.sqrt(np.mean(keras.losses.mean_squared_error(y_test, y_pred)))\n",
        "print(\"RMSE Scaled: {}\\nRMSE Base Units: {}\".format(\n",
        "    rmse, rmse * data_std + data_mean))\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(list(range(len(history.history['mse']))), history.history['mse'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkbr61Cq4B09",
        "colab_type": "text"
      },
      "source": [
        "We get a test RMSE of `0.8989123704842217`, which is still not better than our `SimpleRNN`. And in the more complex model below we get close to the baseline, but still don't beat it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9uAJtpcc8Hg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(0)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.LSTM(10),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  loss='mse',\n",
        "  optimizer='Adam',\n",
        "  metrics=['mae', 'mse'],\n",
        ")\n",
        "\n",
        "stopping = tf.keras.callbacks.EarlyStopping(\n",
        "  monitor='loss',\n",
        "  min_delta=0,\n",
        "  patience=2)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=50, callbacks=[stopping])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "rmse = math.sqrt(np.mean(keras.losses.mean_squared_error(y_test, y_pred)))\n",
        "print(\"RMSE Scaled: {}\\nRMSE Base Units: {}\".format(\n",
        "    rmse, rmse * data_std + data_mean))\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(list(range(len(history.history['mse']))), history.history['mse'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5qJ4WGa4vN6",
        "colab_type": "text"
      },
      "source": [
        "LSTM neurons can be very useful, but as we have seen, they aren't always the best option.\n",
        "\n",
        "Let's look at one more neuron commonly found in RNN models, the GRU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8IkQTQ447N2",
        "colab_type": "text"
      },
      "source": [
        "## Gated Recurrent Unit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21OFa1UC5UgB",
        "colab_type": "text"
      },
      "source": [
        "The Gated Recurrent Unit (GRU) is another special neruon that often shows up in Recurrent Neural Networks. The GRU is similar to the LSTM in that it feeds output back into itself. The difference is that the GRU feeds a single weight back into itself and then makes long and short term state adjustments based on that single backfeed.\n",
        "\n",
        "The GRU tends to train faster than LSTM and has similar performance. Let's see how a network containing one GRU performs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9JjfseAdl2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(0)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(1),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  loss='mse',\n",
        "  optimizer='Adam',\n",
        "  metrics=['mae', 'mse'],\n",
        ")\n",
        "\n",
        "stopping = tf.keras.callbacks.EarlyStopping(\n",
        "  monitor='loss',\n",
        "  min_delta=0,\n",
        "  patience=2)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=50, callbacks=[stopping])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "rmse = math.sqrt(np.mean(keras.losses.mean_squared_error(y_test, y_pred)))\n",
        "print(\"RMSE Scaled: {}\\nRMSE Base Units: {}\".format(\n",
        "    rmse, rmse * data_std + data_mean))\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(list(range(len(history.history['mse']))), history.history['mse'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MunF57N07iWG",
        "colab_type": "text"
      },
      "source": [
        "We got a RMSE of `0.9668634342193015`, which isn't bad, but still performs worse then our baseline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGZRgWNF7pJF",
        "colab_type": "text"
      },
      "source": [
        "## Convolutional Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xELjm9uh7sEZ",
        "colab_type": "text"
      },
      "source": [
        "Convolutional layers are limited to image classification models. They can also be really handy when training RNNs. For training on a sequence of data we use the [`Conv1D` class](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D) as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbYLGqn2eG9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(0)\n",
        "\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=\"valid\",\n",
        "                        input_shape=[None, 1]),\n",
        "    keras.layers.GRU(2, input_shape=[None, 1], activation='relu'),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(1),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  loss='mse',\n",
        "  optimizer='Adam',\n",
        "  metrics=['mae', 'mse'],\n",
        ")\n",
        "\n",
        "stopping = tf.keras.callbacks.EarlyStopping(\n",
        "  monitor='loss',\n",
        "  min_delta=0,\n",
        "  patience=2)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=50, callbacks=[stopping])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "rmse = math.sqrt(np.mean(keras.losses.mean_squared_error(y_test, y_pred)))\n",
        "print(\"RMSE Scaled: {}\\nRMSE Base Units: {}\".format(\n",
        "    rmse, rmse * data_std + data_mean))\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(list(range(len(history.history['mse']))), history.history['mse'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVTGTJiP8Rea",
        "colab_type": "text"
      },
      "source": [
        "Recurrent Neural Networks are a powerful tool for sequence generation and prediction. However, they aren't the only mechanism for sequence prediction. If the sequence you are predicting is short enough, then a standard deep neural network might be able to provide the predictions that you are looking for.\n",
        "\n",
        "Also note that we created a model that took a series of data and output one value. It is possible to create RNNs that input one or more values and output one or more values. Each use case is different."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay3NBOQ39C4M",
        "colab_type": "text"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MRMSH_l9ETz",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 1: Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8v9_mQm9GF6",
        "colab_type": "text"
      },
      "source": [
        "Create a plot containing a series of at least 50 predicted points. Plot that series against the actual.\n",
        "\n",
        "> *Hint: Pick a sequence of 100 values from the original data. Plot datapoints 50-00 as the actual line. Then predict 50 single values starting with the features 0-49, 1-50, etc.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BryuihNkBYhx",
        "colab_type": "text"
      },
      "source": [
        "**Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHp2MEpjBaPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzZAmgPUBbf6",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45xublipBcE5",
        "colab_type": "text"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzcdDNd994Mp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(list(range(50)),\n",
        "         [x * data_std + data_mean for x in X[51].ravel()])\n",
        "plt.plot(list(range(50)),\n",
        "         [x * data_std + data_mean for x in model.predict(X[0:50]).ravel()])\n",
        "_ = plt.legend(['Actual', 'Predicted'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxkXec37JP-Z",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfAND89KJSPS",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 2: Stock Price Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdL2S7oVO7aV",
        "colab_type": "text"
      },
      "source": [
        "Using the [`Stonks!`](https://www.kaggle.com/joshmcadams/stonks) dataset create a recurrent neural network that can predict the stock price for the 'AAA' ticker. Calculate your RMSE with some holdout data.\n",
        "\n",
        "Use as many text and code cells as you need to complete this exercise.\n",
        "\n",
        "> *Hint: if predicting absolute prices doesn't yield a good model, look into other ways to represent the change in data day-to-day*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0G2cMfuPK1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESKR9NfEJu3o",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNwQ-S2KPVRI",
        "colab_type": "text"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6653ZL1PZGN",
        "colab_type": "text"
      },
      "source": [
        "Download the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efjvqAqUPaWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle datasets download joshmcadams/stonks\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xe7E-XBPjmo",
        "colab_type": "text"
      },
      "source": [
        "Decompress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IxHDcDsPfC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! unzip stonks.zip\n",
        "! ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dszSUI8cPk5z",
        "colab_type": "text"
      },
      "source": [
        "Load into a `DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtUYXJm2Jvxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('stonks.csv')\n",
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQJIzh9VPoE4",
        "colab_type": "text"
      },
      "source": [
        "Visualize the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vIviv8XJ8Yq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "for c in df.columns.values:\n",
        "  plt.plot(list(range(len(df[c]))), df[c])\n",
        "plt.legend(df.columns.values)\n",
        "_ = plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNQQAMngPqJ8",
        "colab_type": "text"
      },
      "source": [
        "Check for missing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7TkZPG0Pruz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.isna().any()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FisEVxPOPxWb",
        "colab_type": "text"
      },
      "source": [
        "Create batches of data for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJeydqR0KU2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "batch_count = len(df['AAA']) - sseq_len - 1\n",
        "\n",
        "X = np.zeros((batch_count, sseq_len, 1))\n",
        "y = np.zeros((batch_count, 1))\n",
        "sseq_len = 50\n",
        "for i in range(0, len(df['AAA']) - sseq_len - 1):\n",
        "  X[i] = df[['AAA']].iloc[i:i+sseq_len].to_numpy()\n",
        "  y[i] = df[['AAA']].iloc[i+sseq_len+1].to_numpy()\n",
        "\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "\n",
        "X.shape, y.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yk5HPeZP9JD",
        "colab_type": "text"
      },
      "source": [
        "Normalize the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iZofka5P_aL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_max = df['AAA'].max()\n",
        "data_min = df['AAA'].min()\n",
        "\n",
        "X = (X - data_min) / (data_max - data_min)\n",
        "y = (y - data_min) / (data_max - data_min)\n",
        "\n",
        "X.min(), y.min(), X.max(), y.max()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdqJ2fdlQTeN",
        "colab_type": "text"
      },
      "source": [
        "Split out some holdout data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVS6VgDgLVmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTd1zpTGQV3N",
        "colab_type": "text"
      },
      "source": [
        "Train a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dJq8Re1NleG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(0)\n",
        "\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(2, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.SimpleRNN(1, activation='sigmoid'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  loss='mse',\n",
        "  optimizer='Adam',\n",
        "  metrics=['mae', 'mse'],\n",
        ")\n",
        "\n",
        "stopping = tf.keras.callbacks.EarlyStopping(\n",
        "  monitor='loss',\n",
        "  min_delta=0,\n",
        "  patience=2)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=500, callbacks=[stopping])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "rmse = math.sqrt(np.mean(keras.losses.mean_squared_error(y_test, y_pred)))\n",
        "print(\"RMSE Scaled: {}\\nRMSE Base Units: {}\".format(\n",
        "    rmse, rmse * data_std + data_mean))\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(list(range(len(history.history['mse']))), history.history['mse'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ8XPf98TVAg",
        "colab_type": "text"
      },
      "source": [
        "And we can visualize our model. At this point the base model is not great."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmAdGvdFONng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(list(range(50)),\n",
        "         [(x * (data_max - data_min)) + data_min for x in X[51].ravel()])\n",
        "plt.plot(list(range(50)),\n",
        "         [(x * (data_max - data_min)) + data_min for x in model.predict(X[0:50]).ravel()])\n",
        "_ = plt.legend(['Actual', 'Predicted'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H65UnWpWwi-",
        "colab_type": "text"
      },
      "source": [
        "That's not great performance. Another option might be to try to predict percentage change instead of an absolute price.\n",
        "\n",
        "First, convert the data to day-over-day percent differeces. This data is already well within the range of `(-1, 1)` so we don't need to modify it more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAqY_3SLTjCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "batch_count = len(df['AAA']) - sseq_len - 1\n",
        "\n",
        "X = np.zeros((batch_count, sseq_len, 1))\n",
        "y = np.zeros((batch_count, 1))\n",
        "sseq_len = 50\n",
        "for i in range(0, batch_count):\n",
        "  for j in range(sseq_len):\n",
        "    d0_price = df['AAA'].iloc[i + j]\n",
        "    d1_price = df['AAA'].iloc[i + j + 1]\n",
        "    percent_change = (d1_price - d0_price) / d0_price\n",
        "    X[i][j] = percent_change\n",
        "  d0_price = df['AAA'].iloc[i + j + 1]\n",
        "  d1_price = df['AAA'].iloc[i + j + 2]\n",
        "  percent_change = (d1_price - d0_price) / d0_price\n",
        "  y[i] = percent_change\n",
        "\n",
        "X.min(), y.min(), X.max(), y.max()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZa1qbJ0U4lD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao6VFShhVBEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(0)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(2, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.SimpleRNN(1),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  loss='mse',\n",
        "  optimizer='Adam',\n",
        "  metrics=['mae', 'mse'],\n",
        ")\n",
        "\n",
        "stopping = tf.keras.callbacks.EarlyStopping(\n",
        "  monitor='loss',\n",
        "  min_delta=0,\n",
        "  patience=2)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=500, callbacks=[stopping])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "rmse = math.sqrt(np.mean(keras.losses.mean_squared_error(y_test, y_pred)))\n",
        "print(\"RMSE Scaled: {}\\nRMSE Base Units: {}\".format(\n",
        "    rmse, rmse * data_std + data_mean))\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(list(range(len(history.history['mse']))), history.history['mse'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_Av1pXGXK1m",
        "colab_type": "text"
      },
      "source": [
        "We can then plot percent differences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJUjQqgSV4nB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(list(range(50)), X[51].ravel())\n",
        "plt.plot(list(range(50)), model.predict(X[0:50]).ravel())\n",
        "_ = plt.legend(['Actual', 'Predicted'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MaQLujGXNJz",
        "colab_type": "text"
      },
      "source": [
        "Or actual price predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4ZHXl3vXOqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "actuals = df['AAA'][0:50]\n",
        "predictions = [x[0] + x[0] * x[1] \n",
        "               for x in zip(df['AAA'][0:50], model.predict(X[0:50]).ravel())]\n",
        "\n",
        "plt.plot(list(range(50)), actuals)\n",
        "plt.plot(list(range(50)), predictions)\n",
        "_ = plt.legend(['Actual', 'Predicted'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etEjrP4iYWiu",
        "colab_type": "text"
      },
      "source": [
        "Which looks pretty good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW3HexNJTTaI",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    }
  ]
}