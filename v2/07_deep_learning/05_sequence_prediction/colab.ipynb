{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advanced Sequence Prediction",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "mW4Y4c6tvYvG",
        "exercise-1-key-1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW4Y4c6tvYvG",
        "colab_type": "text"
      },
      "source": [
        "#### Copyright 2019 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24p97VuTvYVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVmV0M74xwm7",
        "colab_type": "text"
      },
      "source": [
        "# Advanced Sequence Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuFjSsW53I9_",
        "colab_type": "text"
      },
      "source": [
        "Sequence prediction involves predicting the next value of a given input sequence. You've seen one application of this type of problem already: predicting stock prices in the RNN colab. Some other common applications include:\n",
        "\n",
        "* *Weather Forecasting*: Given a sequence of observations about the weather over time, predict the weather tomorrow.\n",
        "\n",
        "* *Product Recommendation*: Given a sequence of products added to a shopping list by a user, predict the user's next purchase.\n",
        "\n",
        "* *Language Modeling*: Given a sequence of words, predict the next word in the sentence.\n",
        "\n",
        "These problems have been around for a long time, and are often approached using [statistical methods](http://www.statsoft.com/Textbook/Time-Series-Analysis). In recent years, RNNs have gained popularity for solving these problems, since they are specially designed to handle sequential input.\n",
        "\n",
        "In this lesson we will use more powerful types of RNNs: a GRU (Gated Recurrent Unit) and an LSTM (Long Short-Term Memory), to perform sequence prediction on stock prices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LboFaJ47r6x-",
        "colab_type": "text"
      },
      "source": [
        "## Data\n",
        "\n",
        "We will use a generated dataset of stock prices, similar to the dataset we created for the RNN colab. We will create 10 years of stock data for a fictional company that had an initial public offering (IPO) of $42."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhd_d5vzrc_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set random seeds for reproducible results.\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lc9aDiOQym6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "days_per_year = 365\n",
        "years = 10\n",
        "\n",
        "eod_prices = np.zeros(days_per_year*years)\n",
        "eod_prices[0] = 42.0\n",
        "\n",
        "bias = 0.0002\n",
        "\n",
        "for i in range(1, len(eod_prices)):\n",
        "  # Find yesterday's price.\n",
        "  yesterdays_price = eod_prices[i-1]\n",
        "  \n",
        "  # Generate a random a percentage change on the normal curve.\n",
        "  percentage_change = np.random.randn(1)[0]\n",
        "  \n",
        "  # The random number is a value on the standard normal distribution\n",
        "  # with a mean 0 and variance 1. This will give us a nice range of\n",
        "  # positive and negative values, but we need to divide by 100 to scale\n",
        "  # them down to reasonable percentages for daily stock price changes.\n",
        "  percentage_change /= 100\n",
        "\n",
        "  # Finally we give the change just a little bit of positive bias so\n",
        "  # that we get a more interesting curve.\n",
        "  percentage_change += bias\n",
        "  \n",
        "  # Calculate the new price.\n",
        "  todays_price = yesterdays_price + yesterdays_price * percentage_change\n",
        "  \n",
        "  # Store the price.\n",
        "  eod_prices[i] = todays_price"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot30Je_Nb-I6",
        "colab_type": "text"
      },
      "source": [
        "Let's quickly visualize the data to make sure it looks reasonably realistic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUKFZuuuQ41-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(list(range(len(eod_prices))), eod_prices)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2edEgGVicErj",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "RNNs are sensitive to the scale of the input data, specifically when using the `sigmoid` or `tanh` activation functions. It can be good practice to normalize or rescale the data to a range of 0 to 1. The [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) in scikit-learn is a good tool for normalization.\n",
        "\n",
        "MinMaxScaler expects an input of shape `(n_samples, n_features)`. Our data only has one feature (price), so we must reshape it to work with the scaler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGOps0s_ORv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "# you can use \"-1\" as a stand-in for one dimension when reshaping NumPy arrays,\n",
        "# and NumPy will determine the correct value based on the other dimensions.\n",
        "eod_prices = np.reshape(eod_prices, (-1, 1))\n",
        "eod_prices = scaler.fit_transform(eod_prices)\n",
        "\n",
        "eod_prices = np.reshape(eod_prices, -1)\n",
        "eod_prices[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meCQFCCCd73_",
        "colab_type": "text"
      },
      "source": [
        "Notice that the final stock price, around $180, has been normalized to 0.095. To follow the previous RNN colab, let's also use stocks in a 100-day window to predict the next day's stock price."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5IXDS_BNdhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_steps = 100\n",
        "n_samples = len(eod_prices) - n_steps\n",
        "\n",
        "# Split eod_prices into X (100 day windows) and y (next day prices).\n",
        "X = np.zeros((n_samples, n_steps))\n",
        "y = np.zeros(n_samples)\n",
        "\n",
        "for i in range(n_samples):\n",
        "  X[i] = eod_prices[i:i + n_steps]\n",
        "  y[i] = eod_prices[i + n_steps]\n",
        "\n",
        "y[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am1qBEoiwxID",
        "colab_type": "text"
      },
      "source": [
        "We need to create a training and testing set. To model real-life forecasting problems, we'll use the stock performance from the first 80% of days as training and the remaining days as testing (modeling predicting the \"future\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoTkcr-tQ3TY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_index = int(n_samples * 0.8)\n",
        "\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk3COdKGIB7l",
        "colab_type": "text"
      },
      "source": [
        "## RNN Model in Keras\n",
        "\n",
        "Keras makes it easy to experiment with different [RNN models](https://keras.io/layers/recurrent/). We will train a model using a basic RNN to show the basic structure of using an RNN model in Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxi5adthxycv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDNFvqlCxJU8",
        "colab_type": "text"
      },
      "source": [
        "RNN models in Keras expect a 3-dimensional input with dimensions `(n_samples, n_timesteps, n_features)`. In the documentation, this is described as `(batch_size, timesteps, input_dim)`.\n",
        "\n",
        "Each of our samples has 100 timesteps and 1 feature (price). We'll need to make `X_train` and `X_test` three dimensional using `np.reshape`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G7QAyLTxMIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2trsd0Jyrnvp",
        "colab_type": "text"
      },
      "source": [
        "### Training\n",
        "\n",
        "Now let's set up our model. We'll use:\n",
        "\n",
        "1.   A `SimpleRNN` layer with 25 hidden units (as in the RNN colab).\n",
        "2.   A `Dense` layer with 1 output unit (the predicted price).\n",
        "\n",
        "This model is complex enough to converge fairly quickly, so we'll train it for 5 epochs and with a batch size of 50. Since this is a regression problem, we'll use [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) as the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw0UZ5fLzRYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_units = 25\n",
        "n_epochs = 5\n",
        "batch_size = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2APcshyNH46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_model = keras.Sequential([\n",
        "  keras.layers.SimpleRNN(n_units),\n",
        "  keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "rnn_model.compile(\n",
        "  loss='mean_squared_error',\n",
        "  optimizer='adam'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euRZuE9MLHd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=n_epochs,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm_5QwIiz5va",
        "colab_type": "text"
      },
      "source": [
        "### Testing\n",
        "\n",
        "Remember that we normalized the data to a [0, 1] range before training. To properly evaluate the model, we'll need to un-normalize the predictions, then calculate the Mean Squared Error based on the original data values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXnPAN0rUrBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def evaluate(model):\n",
        "  predicted_train = model.predict(X_train)\n",
        "  predicted_test = model.predict(X_test)\n",
        "\n",
        "  # Un-normalize predictions.\n",
        "  predicted_train = scaler.inverse_transform(predicted_train)\n",
        "  predicted_test = scaler.inverse_transform(predicted_test)\n",
        "  # Get original data values.\n",
        "  y_train_raw = scaler.inverse_transform([y_train])[0]\n",
        "  y_test_raw = scaler.inverse_transform([y_test])[0]\n",
        "  # Calculate mean squared error.\n",
        "  trainScore = mean_squared_error(y_train_raw, predicted_train[:,0])\n",
        "  print('Train Score: %.2f MSE' % (trainScore))\n",
        "  testScore = mean_squared_error(y_test_raw, predicted_test[:,0])\n",
        "  print('Test Score: %.2f MSE' % (testScore))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io7B7FKyTVJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate(rnn_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpQ_n0rpQmcs",
        "colab_type": "text"
      },
      "source": [
        "## GRU Model\n",
        "\n",
        "With longer input sequences, RNNs sometimes struggle to \"remember\" input that they saw earlier in the sequence. The [GRU (Gated Recurrent Unit)](https://arxiv.org/pdf/1406.1078v3.pdf) aims to improve upon this by using an *update gate* and *reset gate* to control what information is kept from previous steps and what is \"forgotten\". \n",
        "\n",
        "We won't get into the details of the architecture here. (If you're interested, see the Resources section below). What's important to know is that Keras makes it very straightforward to use a GRU instead of an RNN. It's a one-line change!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6kv18EPQogT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gru_model = keras.Sequential([\n",
        "  keras.layers.GRU(n_units), # This line is the only change we have to make.\n",
        "  keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVmgP2Hz4tcq",
        "colab_type": "text"
      },
      "source": [
        "Let's train this GRU model and see how it does. You'll notice that training is slower than with the RNN, due to extra computations needed with the two gates, but the final results should be better due, since the GRU captures more information from previous steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdwktVUq4olY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gru_model.compile(\n",
        "  loss='mean_squared_error',\n",
        "  optimizer='adam'\n",
        ")\n",
        "\n",
        "gru_model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=n_epochs,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "evaluate(gru_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9zEfRZ-yPt6",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Model\n",
        "\n",
        "[LSTM (Long Short Term Memory)](http://www.bioinf.jku.at/publications/older/2604.pdf) networks are even better at learning such long-term dependencies. Similar to GRUs, they also use gates to control information between steps. \n",
        "\n",
        "LSTMs use an explicit *cell state* to control this information flow, which interacts with the *hidden state* that we're used to thinking about. An *input gate*, *forget gate*, and *output gate* control how we update the cell state at each step.\n",
        "\n",
        "Again, don't worry too much about the specifics for now (and see Resources if you're interested)! We only need to change one line to use an LSTM instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-kP-QzDouvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_model = keras.Sequential([\n",
        "  keras.layers.LSTM(n_units), # Another magic one-line change!\n",
        "  keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxY3j6ZK6pmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_model.compile(\n",
        "  loss='mean_squared_error',\n",
        "  optimizer='adam'\n",
        ")\n",
        "\n",
        "lstm_model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=n_epochs,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "evaluate(lstm_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE4RtnAY7ClV",
        "colab_type": "text"
      },
      "source": [
        "LSTM models usually take the longest to train (they need the most computations). They are also usually the most powerful, though they may need more epochs to converge.\n",
        "\n",
        "In practice, it's a good idea to start with an RNN and then upgrade to either a GRU or LSTM if you need a more complex model. For very long input sequences, LSTMs typically work better at capturing the longer-term dependencies in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgZAH4s8-TxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize LSTM predictions.\n",
        "\n",
        "predicted_train = lstm_model.predict(X_train)\n",
        "predicted_test = lstm_model.predict(X_test)\n",
        "\n",
        "# Un-normalize predictions.\n",
        "predicted_train = scaler.inverse_transform(predicted_train)\n",
        "predicted_test = scaler.inverse_transform(predicted_test)\n",
        "\n",
        "plt.plot(range(len(eod_prices)), scaler.inverse_transform([eod_prices])[0])\n",
        "plt.plot(range(n_steps, len(predicted_train) + n_steps), predicted_train)\n",
        "plt.plot(range(len(predicted_train) + n_steps, len(eod_prices)), predicted_test)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXPmtj5swDnm",
        "colab_type": "text"
      },
      "source": [
        "# Resources\n",
        "\n",
        "*   https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "*   https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdqWCg3eLldl",
        "colab_type": "text"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs-QlHuc6IbK",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axURTCgJwOjY",
        "colab_type": "text"
      },
      "source": [
        "Train a GRU or LSTM model on a dataset of your choice. Visualize its performance and compare its performance to a basic RNN model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMw0t6QpAF8f",
        "colab_type": "text"
      },
      "source": [
        "### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ptus2D5__5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-1-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-1-solution-1"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AKb-7oqrcfox",
        "colab": {}
      },
      "source": [
        "# Solutions will vary; make sure students use an appropriate dataset (i.e. sequential data),\n",
        "# properly construct both an RNN and advanced RNN model,\n",
        "# and appropriately visualize the data."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}