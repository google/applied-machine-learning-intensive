{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Embeddings",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "copyright",
        "exercise-1-key-1",
        "exercise-2-key-1",
        "exercise-3-key-1",
        "exercise-4-key-1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2019 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "24p97VuTvYVT",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CVmV0M74xwm7"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XuFjSsW53I9_"
      },
      "source": [
        "Embeddings are a powerful way to represent data for deep learning (neural network) models.\n",
        "\n",
        "In this exercise, we will work specifically with embeddings of words. This is one of the most common applications of this technique. We will train word embeddings from scratch on a small dataset and visualize those embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "<img height=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/2/2d/Tensorflow_logo.svg\" align=\"left\" hspace=\"10px\">\n",
        "This lab is developed based on [TensorFlow.org's Word embeddings](https://www.tensorflow.org/alpha/tutorials/sequences/word_embeddings)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jk3COdKGIB7l"
      },
      "source": [
        "# Learn embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LboFaJ47r6x-"
      },
      "source": [
        "Embeddings can be trained using either supervised or unsupervised learning. For this exercise, we will train a supervised sentiment classifier on IMDB movie reviews. In the process, we will learn our own word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zu7rUHNXWBue"
      },
      "source": [
        "### Setup\n",
        "\n",
        "First, we will set random seeds for reproducible results. This makes sure that we get the same results every time we run this lab, by controlling how any random numbers are generated. **This should almost never be used in production code.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uI4PxR8OcADw",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.set_random_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1zoVKdwCZTK5"
      },
      "source": [
        "## Data and Problem\n",
        "\n",
        "We will use the [IMDB dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/). This dataset is conveniently packaged in Keras. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xhd_d5vzrc_h",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "imdb = keras.datasets.imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
        "    index_from=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vr02kdK2aHnJ"
      },
      "source": [
        "The reviews have been pre-processed so that the text of each review (a list of words) has been converted to a list of integers, where each integer represents a specific word. We set the smallest word index as 3 (`index_from=2`) to handle some special tokens, which we'll describe in the next section.\n",
        "\n",
        "Each sentiment label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review. Thus this is a binary classification problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KERrbc1Xarhj",
        "colab": {}
      },
      "source": [
        "print(train_data[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Goq0Frawyuk"
      },
      "source": [
        "### Convert the integers back to words\n",
        "\n",
        "It may be useful to know how to convert a list of integers back to text. Here, we'll create a helper function to query a dictionary object that contains the integer to string mapping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LfcdK_js_dQd",
        "colab": {}
      },
      "source": [
        "# A dictionary mapping each word to an integer index.\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# Reserve the first 3 indices for special tokens.\n",
        "word_index = {k:(v+2) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2\n",
        "\n",
        "vocab_size = len(word_index)\n",
        "print('# unique words: {}'.format(vocab_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y5fVfsydTeCB",
        "colab": {}
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "decode_review(train_data[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u617UoBvzI_A"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Note that there are three special tokens in the `word_index` dictionary:\n",
        "\n",
        "- `\"<START>\"` marks the beginning of each review\n",
        "- `\"<UNK>\"` handles \"unknown\" words, or words that appear in the test dataset but not in the training dataset (so they are effectively \"unknown\" to the model)\n",
        "- `\"<PAD>\"` adds extra tokens to the end of shorter reviews\n",
        "\n",
        "Movie reviews can be different lengths, but inputs to a neural network must all be the same length. Let's take a look at the distribution of review lengths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i5oIPHvKMqvt",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "review_lengths = [len(review) for review in train_data]\n",
        "plt.hist(review_lengths, density=True, cumulative=True)\n",
        "# plt.yticks(np.arange(0, 1.1, step=0.1))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QyUvPXstTup9"
      },
      "source": [
        "Even though the longest review is close to 2500 words long, over 90% of the reviews have fewer than 500 words. \n",
        "\n",
        "We can use the `pad_sequences` function to standardize the lengths of the reviews to 500 words long. Any reviews longer than this will have the extra words truncated, while any reviews shorter than 500 words will have extra `\"<PAD>\"` tokens added to the end. \n",
        "\n",
        "Choosing a standardized length requires balancing efficiency (longer lengths mean slower training) and information loss (shorter lengths may truncate too much valuable information). Aiming for a length that fully covers 90% of samples is generally reasonable, and you can further tune this as a hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nX-sWA3Z2uu3",
        "colab": {}
      },
      "source": [
        "maxlen = 500\n",
        "\n",
        "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
        "                                                        value=word_index[\"<PAD>\"],\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=maxlen)\n",
        "\n",
        "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
        "                                                       value=word_index[\"<PAD>\"],\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r8Uv3iI725Mb"
      },
      "source": [
        "Let's inspect the first padded review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O-6VBoda3agD",
        "colab": {}
      },
      "source": [
        "print(train_data[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NW1OYV_piA5M"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B7Cl6fRyDuDI"
      },
      "source": [
        "### Using the Embedding layer\n",
        "\n",
        "Keras makes it easy to use embeddings. The [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer takes at least two arguments: the number of possible words in the vocabulary and the dimensionality of the embeddings. We will start by using a small embedding size of 2 to make visualization easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bsve7z4TFJyk",
        "colab": {}
      },
      "source": [
        "embedding_dim = 2\n",
        "\n",
        "embedding_layer = keras.layers.Embedding(vocab_size, embedding_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ucstp6tBFt94"
      },
      "source": [
        "The Embedding layer can be understood as a lookup table that maps from integer indices (which stand for specific words) to dense vectors (their embeddings). The dimensionality (or width) of the embedding is a parameter you can experiment with to see what works well for your problem, much in the same way you would experiment with the number of neurons in a Dense layer.\n",
        "\n",
        "When we create an Embedding layer, the weights for the embedding are randomly initialized (just like any other layer). During training, they are gradually adjusted via backpropagation. Once trained, the learned word embeddings will roughly encode similarities between words (as they were learned for the specific problem our model is trained on).\n",
        "\n",
        "As input, the Embedding layer takes a 2D tensor of integers, of shape `(num_samples, sequence_length)`, where each sample is a sequence of integers. As output, the embedding layer returns a 3D floating point tensor, of shape `(num_samples, sequence_length, embedding_dimensionality)`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w9Clq0KWL2dD"
      },
      "source": [
        "### Configure the Model\n",
        "\n",
        "1. The first layer is an Embedding layer. This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: `(batch size, sequence length, embedding size)`.\n",
        "\n",
        "1. Next, we flatten the output from a 2-d array to a 1-d array.\n",
        "\n",
        "1. The last layer is densely connected with a single output node. Using the sigmoid activation function, this value is a float between 0 and 1, representing a probability (or confidence level) that the review is positive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3OnpePqclffu",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([\n",
        "  keras.layers.Embedding(vocab_size, embedding_dim, input_length=maxlen),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f8DaPLm2tki9"
      },
      "source": [
        "### Compile and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Itc2oZQMnz7t",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "  # Calculate loss for a binary classification problem.\n",
        "  loss='binary_crossentropy',\n",
        "\n",
        "  # Adam is one of the most commonly used optimizers.\n",
        "  optimizer=tf.train.AdamOptimizer(),\n",
        "\n",
        "  # We will only track accuracy for this task.\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ysiCSDsVn2SC"
      },
      "source": [
        "The number of epochs and batch size used during training are both hyperparameters, meaning you may need to experiment with different values to improve performance. This also means there's no magic answer for which values to choose.\n",
        "\n",
        "If you've implemented your model correctly, you should observe decreasing training loss within a few epochs. With more epochs (more training), the model will experience more overfitting. You will typically train for at least 5 epochs; for this model, we train for 10 epochs as a reasonable tradeoff between learning (as the validation accuracy is still increasing) and overfitting.\n",
        "\n",
        "Since we have a fairly large dataset, we also want to process data in batches instead of the entire dataset at once. If we choose batch sizes that are too small, we will get slower training per epoch, while larger batch sizes may require more epochs to train and even cause out-of-memory errors. It's good practice to start with a small power of two (e.g. 32), then experiment with continuously doubling your batch size. For this model, we are able to use a batch size of 512 with good performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mPekaK45pdOc",
        "colab": {}
      },
      "source": [
        "# Train the model.\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    train_labels,\n",
        "    epochs=10,\n",
        "    batch_size=512,\n",
        "    validation_split=0.2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VFcv3PYzJwjl",
        "colab": {}
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.figure(figsize=(12,9))\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim((0.5,1))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ODIrlPDiJrDP"
      },
      "source": [
        "With this approach our model reaches a validation accuracy of around 87%. (Note that the model is already beginning to overfit, as reflected in the diverging training and validation accuracy curves.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uLCSnMlwJy3j"
      },
      "source": [
        "# Visualize embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PMqbq-qMnCR7"
      },
      "source": [
        "## Retrieve the learned embeddings\n",
        "\n",
        "Next, let's retrieve the word embeddings learned during training. This will be a matrix of shape `(vocab size, embedding dimension)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KncdCMmgnHtU",
        "colab": {}
      },
      "source": [
        "e = model.layers[0]\n",
        "embedding_matrix = e.get_weights()[0]\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ilLX1zdS4PIn"
      },
      "source": [
        "Let's explore the learned word embeddings on a small set of terms (looking at all 80,000+ at once would be overwhelming). Most of these terms are generally strongly indicative of sentiment, but some are added just for fun."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D3nzbrDtp1VO",
        "colab": {}
      },
      "source": [
        "informative_terms = [ \"bad\", \"great\", \"best\", \"worst\", \"fun\", \"beautiful\",\n",
        "                      \"excellent\", \"poor\", \"boring\", \"awful\", \"terrible\",\n",
        "                      \"definitely\", \"perfect\", \"liked\", \"worse\", \"waste\",\n",
        "                      \"entertaining\", \"loved\", \"unfortunately\", \"amazing\",\n",
        "                      \"enjoyed\", \"favorite\", \"horrible\", \"brilliant\", \"highly\",\n",
        "                      \"simple\", \"annoying\", \"today\", \"hilarious\", \"enjoyable\",\n",
        "                      \"dull\", \"fantastic\", \"poorly\", \"fails\", \"disappointing\",\n",
        "                      \"disappointment\", \"not\", \"him\", \"her\", \"good\", \"time\",\n",
        "                      \"sad\", \"exciting\", \"slow\", \"movie\", \"film\", \"action\",\n",
        "                      \"comedy\", \"drama\", \"fabulous\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "axURTCgJwOjY"
      },
      "source": [
        "Now we plot each word in `informative_terms` on a 2-dimensional graph. Recall that we are using 2-dimensional embeddings, so consider the 1st value as an x-value and the 2nd value as a y-value. \n",
        "\n",
        "*Note: If the displayed plot is too small, try running the cell again.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p2kNso7P6fem",
        "colab": {}
      },
      "source": [
        "for word in informative_terms:\n",
        "  word_num = word_index[word]\n",
        "  embeddings = embedding_matrix[word_num]\n",
        "  plt.text(embeddings[0], embeddings[1], word)\n",
        "\n",
        "# Do a little set-up to make sure the plot displays nicely.\n",
        "plt.rcParams[\"figure.figsize\"] = (25, 25)\n",
        "plt.xlim(1.2 * embedding_matrix.min(), 1.2 * embedding_matrix.max())\n",
        "plt.ylim(1.2 * embedding_matrix.min(), 1.2 * embedding_matrix.max())\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gE8uZeJwWsqC"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hIx-sOKz6qLl"
      },
      "source": [
        "## Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZvHk7PYtkTY6"
      },
      "source": [
        "What structures do you see in the embeddings of the words?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LMeBYRbEW2eB"
      },
      "source": [
        "### Student Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jUWVyvqq6vj7"
      },
      "source": [
        "**Your answer here: **\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-1-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-1-solution-1"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UQHCogc2W6nX",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bCkk08wYW7rv"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x1FmjKHmW-gB",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tI9DB-Dn76O-"
      },
      "source": [
        "## Exercise 2: Higher-dimensional embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qhlN0BGnkVf3"
      },
      "source": [
        "Now we can visualize 2-dimensional embeddings, but recall that our original vocabulary had over 80,000 words; we're missing a lot of information if we only use 2 dimensions!\n",
        "\n",
        "Typical embedding dimensions are 50, 100, 200, 300, and sometimes even larger. In this section we'll introduce the [Embedding Projector](http://projector.tensorflow.org/), a tool to visualize high-dimensional embeddings.\n",
        "\n",
        "Re-train our earlier model, with the same hyperparameters, using **50-dimensional** embeddings and retrieve the learned embeddings from this trained model. You should observe a slower training time and much higher training accuracy after 10 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CU2jvjHHXE0t"
      },
      "source": [
        "### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D5ab4EOm8aYW",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Re-train model using 50-dimensional embeddings.\n",
        "\n",
        "embedding_matrix = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-2-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-2-solution-1"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d28wAL638nWP",
        "colab": {}
      },
      "source": [
        "# Re-train model using 50-dimensional embeddings.\n",
        "\n",
        "embedding_dim = 50\n",
        "\n",
        "# Keep the same model and training structure.\n",
        "\n",
        "model = keras.Sequential([\n",
        "  keras.layers.Embedding(vocab_size, embedding_dim, input_length=maxlen),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  loss='binary_crossentropy',\n",
        "  optimizer=tf.train.AdamOptimizer(),\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    train_labels,\n",
        "    epochs=10,\n",
        "    batch_size=512,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Retrieve embeddings.\n",
        "\n",
        "e = model.layers[0]\n",
        "embedding_matrix = e.get_weights()[0]\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NDlCRA_cXJnF"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g7hTFWPYXNCl",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q-2TJScF9DEM"
      },
      "source": [
        "We will now write the embeddings to disk. To use the Embedding Projector, we will upload two files in tab separated format: a file of vectors (containing the embeddings), and a file of metadata (containing the words). We will again only analyze our small set of terms from above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ub8R-OiRz1PE",
        "colab": {}
      },
      "source": [
        "import io\n",
        "\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "for word in informative_terms:\n",
        "  word_num = word_index[word]\n",
        "  embeddings = embedding_matrix[word_num]\n",
        "  # for compatibility with python 2 and 3\n",
        "  out_m.write(word.encode(\"utf-8\").decode(\"utf-8\")  + \"\\n\")\n",
        "  out_v.write('\\t'.join(\n",
        "      [str(x).encode(\"utf-8\").decode(\"utf-8\") for x in embeddings]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()\n",
        "\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download('vecs.tsv')\n",
        "  files.download('meta.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UIhhw4L_93aO"
      },
      "source": [
        "Finally, open the [Embedding Projector](http://projector.tensorflow.org/):\n",
        "\n",
        "*   Click on \"Load data\".\n",
        "*   Upload the two files we created above: `vecs.tsv` and `meta.tsv`.\n",
        "\n",
        "The embeddings we have trained will now be displayed. You can search for words to find their closest neighbors.\n",
        "\n",
        "*Note: Your results may be a bit different, depending on how weights were randomly initialized before training the embedding layer.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kWqW-CGA-SZ_"
      },
      "source": [
        "## Exercise 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7ZHtRLBwka1x"
      },
      "source": [
        "How do the structures of these embeddings compare to the 2-dimensional embeddings we visualized earlier? How do the model accuracies compare? What might this suggest about the effect of increasing embedding size?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ecRq1mCfXRIH"
      },
      "source": [
        "### Student Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O8l3IWi7-oNr"
      },
      "source": [
        "**Your answer here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-3-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-3-solution-1"
      },
      "source": [
        "**Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-MfIQ_rNXWJp",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q1bdsl5fXT_X"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DiuMs4SwXVOk",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f3xNZuVGuBlY"
      },
      "source": [
        "## Exercise 4: Beyond word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gc5Cvirukdeq"
      },
      "source": [
        "The Embedding layer in Keras makes it easy to represent any data that has an index-based lookup. What are some other kinds of data (besides words) that this could be useful for?\n",
        "\n",
        "On the flip side, what are some kinds of data that don't work as well with an index-based lookup? Let's say we still want to achieve the advantages of using embeddings, namely transforming this data into 1) efficient, dense vectors that 2) capture similarities between data. How could we do this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TPsA5mDGXXkt"
      },
      "source": [
        "### Student Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZvBPczGg67m2"
      },
      "source": [
        "**Your answer here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-4-key-1"
      },
      "source": [
        "### Answer Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exercise-4-solution-1"
      },
      "source": [
        "**Solution**"
      ]
    }
  ]
}