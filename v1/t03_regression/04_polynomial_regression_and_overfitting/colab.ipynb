{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "copyright"
   },
   "source": [
    "#### Copyright 2019 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "za20dNxkuKgG"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PWoQYCYUuRhq"
   },
   "source": [
    "# Polynomial Regression and Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mtccx8anFhLG"
   },
   "source": [
    "We have spent a substantial amount of time on linear models so far in this course. Most of these models have been \"straight-line\" models where we attempt to draw a line that fits a regression or a line that is used to make a binary decision to a single feature.\n",
    "\n",
    "In cases where we have had more than one feature, we have relied on underlying [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition) implementations to project important factors about the features onto a data plane that we can dissect with a straight line.\n",
    "\n",
    "We will diverge from this a bit today and build a model based on a [polynomial equation](https://en.wikipedia.org/wiki/Polynomial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OD7gtGpyRZLa"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgOFKoOPRbuC"
   },
   "source": [
    "### Learning Objectives\n",
    "\n",
    "* Apply polynomial models to regression problems\n",
    "* Recognize when a model might be overfitting\n",
    "* Combat overfitting using techniques such as Lasso, Ridge, and ElasticNet regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VZe4Xj9tyQRD"
   },
   "source": [
    "### Prerequisites\n",
    "\n",
    "* TODO(joshmcadams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Sq6gKspRdvJ"
   },
   "source": [
    "### Estimated Duration\n",
    "\n",
    "60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a0nmCEeoRfyS"
   },
   "source": [
    "### Grading Criteria\n",
    "\n",
    "Each exercise is worth 3 points. The rubric for calculating those points is:\n",
    "\n",
    "| Points | Description |\n",
    "|--------|-------------|\n",
    "| 0      | No attempt at exercise |\n",
    "| 1      | Attempted exercise, but code does not run |\n",
    "| 2      | Attempted exercise, code runs, but produces incorrect answer |\n",
    "| 3      | Exercise completed successfully |\n",
    "\n",
    "There are 2 exercises in this Colab so there are 6 points available. The grading scale will be 6 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PknJfIfBwzu5"
   },
   "source": [
    "## Generate sample data\n",
    "\n",
    "For this Colab let's start by generating some data on a polynomial curve with a degree 2 polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgmmzEFXw-qi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_items = 100\n",
    "\n",
    "np.random.seed(seed=420)\n",
    "X = np.random.randn(num_items, 1)\n",
    "\n",
    "y = 0.6*(X**2) - 0.4*X + 1.3\n",
    "\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LPrjFFtr0-Ml"
   },
   "source": [
    "That gives us a nice polynomial grin, but let's add some randomness to create a more realistic dataset and plot the randomized data points and the fit line again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wi5xcUqLqVyr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_items = 100\n",
    "\n",
    "np.random.seed(seed=420)\n",
    "X = np.random.randn(num_items, 1)\n",
    "y = 0.6*(X**2) - 0.4*X + 1.3 + np.random.randn(num_items, 1)/2\n",
    "\n",
    "X_line = np.linspace(X.min(), X.max(), num=num_items)\n",
    "y_line = (X_line**2) - X_line + 1.3\n",
    "\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.plot(X_line, y_line, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0_ts9GCoF4rW"
   },
   "source": [
    "That looks much better! Now we can see that a 2-degree polynomial function fits this data reasonably well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "57bc0JsX1-ly"
   },
   "source": [
    "## Polynomial fitting\n",
    "\n",
    "At this point we have one feature and can see a pretty obvious 2-degree polynomial in the scatterplot. How do we model this?\n",
    "\n",
    "Scikit Learn offers a `PolynomialFeatures` class that handles polynomial combinations on a linear model. In this case we have seen that a 2-degree polynomial looks like a good fit. Let's see if the model works.\n",
    "\n",
    "We begin by creating a `PolynomialFeatures` instance of degree 2.\n",
    "\n",
    "You might be wondering what the `include_bias` parameter is? By default it is `True` and by it causes the first exponent to be 0, which always creates a value of 1.\n",
    "\n",
    "What does this do? It adds a constant bias term to the equation. When we ask for no bias we start our exponents at 1 instead of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pYj3qjd9q8OA"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pf = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = pf.fit_transform(X)\n",
    "\n",
    "X.shape, X_poly.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hhgV7MVq3EGL"
   },
   "source": [
    "This preprocessor generates new feature matrix consisting of all polynomial combinations of the features. Notice how or input shape of `(100, 1)` becomes `(100, 2)` after transformation.\n",
    "\n",
    "In this simple case we just doubled the number of features since we asked for a 2-degree polynomial and only had one input feature. The number of generated features actually grows exponentially as the number of features and polynomial degrees increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R1q9mXXXHYLr"
   },
   "source": [
    "## Fit the model\n",
    "\n",
    "We can now fit the model by passing our polynomial preprocessing data to the linear regressor.\n",
    "\n",
    "How close did the intercept and coefficient match the values in the function we used to generate our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xwc2Bdh4rSVQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QyFK6PhcIBdE"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "And we can plot our fitted line against the actual equation used to generate the data. Our fitted line is green and the actual is red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eEd9rGnFr_mc"
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed=420)\n",
    "\n",
    "X_line_fitted = np.linspace(X.min(), X.max(), num=100)\n",
    "y_line_fitted = lin_reg.coef_[0][1] * X_line_fitted**2 + lin_reg.coef_[0][0] * X_line_fitted + lin_reg.intercept_\n",
    "\n",
    "plt.plot(X_line_fitted, y_line_fitted, 'g-')\n",
    "plt.plot(X_line, y_line, 'r-')\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLfOr-euIJ4v"
   },
   "source": [
    "# Overfitting\n",
    "\n",
    "When using polynomial regressions it can be easy to *overfit* the data so that it performs well on the training and testing data, but doesn't perform well in the real world.\n",
    "\n",
    "To understand what overfitting is, let's create a visualization to illustrate. We will do this by creating a fake dataset generated off of a linear equation, but then use a polynomial regression as the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I-yRiIqex9zA"
   },
   "source": [
    "## Generate sample data\n",
    "\n",
    "First we will generate some data using a purely linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q4FHYtfrwVEr"
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed=420)\n",
    "\n",
    "num_items = 50\n",
    "X = 6 * np.random.rand(num_items, 1)\n",
    "y = X + 2 + np.random.randn(num_items, 1)\n",
    "\n",
    "X_line = np.array([X.min(), X.max()])\n",
    "y_line = X_line + 2\n",
    "\n",
    "plt.plot(X_line, y_line, 'r-')\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-axAp_pzxHQe"
   },
   "source": [
    "## Fit a polynomial model\n",
    "\n",
    "Let's now create a 10 degree polynomial to fit the linear data and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4lflXXT24pRN"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "np.random.seed(seed=420)\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=10, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "regression = LinearRegression()\n",
    "regression.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPXZ46X8xXFB"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "We will now draw the polynomial line that we fit to the linear model. To draw the line we need to execute the 10 degree polynomial equation.\n",
    "\n",
    "```\n",
    "  a + b*x^1 + c*x^2 + d*x^3 + ... + j*x^9 + k*x^10\n",
    "```\n",
    "\n",
    "Creating the above equation by hand is tedious and error-prone. It also makes it difficult to change the degree of the polynomial we are fitting.\n",
    "\n",
    "Let's peek into the data provided by the `PolynomialFeatures` and `LinearRegression` functions and see if there is a way to write the code more dynamically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y-XxBVnPzB9o"
   },
   "source": [
    "Our `PolynomialFeatures` class provides us with a list of exponents that we can use for each portion of the polynomial equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIatzCwvy8QD"
   },
   "outputs": [],
   "source": [
    "poly_features.powers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-GRE5mgzYmI"
   },
   "source": [
    "Our `LinearRegression` provides us with a list of coefficients that line up with the powers provided by `PolynomialFeatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e-RmO7-KzSMe"
   },
   "outputs": [],
   "source": [
    "regression.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w1nb49h7zjeR"
   },
   "source": [
    "It also provides us with an intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "14UXMC2nzgzO"
   },
   "outputs": [],
   "source": [
    "regression.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jM1pOywgzm8d"
   },
   "source": [
    "Having this information, we can take a set of X values (in the code below we use 100) and then run our equation on those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PjZem434xSZf"
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed=420)\n",
    "\n",
    "# Create 100 even-spaced x-values\n",
    "X_line_fitted = np.linspace(X.min(), X.max(), num=100)\n",
    "\n",
    "# Start our equation with the intercept\n",
    "y_line_fitted = regression.intercept_\n",
    "\n",
    "# For each exponent raise the X value to that exponent and multiple it by the\n",
    "# appropriate coefficient\n",
    "for i in range(len(poly_features.powers_)):\n",
    "  exponent = poly_features.powers_[i][0]\n",
    "  y_line_fitted = y_line_fitted + regression.coef_[0][i] * (X_line_fitted**exponent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H98LMnMp0JC0"
   },
   "source": [
    "We can now plot the data points, the actual line used to generate them, and our fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PH8z0hQt0PQf"
   },
   "outputs": [],
   "source": [
    "plt.plot(X_line, y_line, 'r-')\n",
    "plt.plot(X_line_fitted, y_line_fitted, 'g-')\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lt6mmUD80VXn"
   },
   "source": [
    "Notice how our line is very wavy and likely spikes up or down at the ends. This is a sign of overfitting. The line fits the training data reasonably well, but will not likely be very useful on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aXaa2K9h0pCX"
   },
   "source": [
    "## Using a simpler model\n",
    "\n",
    "The most obvious way to prevent overfitting in this example is to simply reduce the degree of the polynomial or revert to a linear model.\n",
    "\n",
    "The code below uses a two degree polynomial and seems to fit the data much better. A linear model would be good too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9khOTN-y09qO"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "regression = LinearRegression()\n",
    "regression.fit(X_poly, y)\n",
    "\n",
    "X_line_fitted = np.linspace(X.min(), X.max(), num=100)\n",
    "y_line_fitted = regression.intercept_\n",
    "for i in range(len(poly_features.powers_)):\n",
    "  exponent = poly_features.powers_[i][0]\n",
    "  y_line_fitted = y_line_fitted + regression.coef_[0][i] * (X_line_fitted**exponent)\n",
    "\n",
    "plt.plot(X_line, y_line, 'r-')\n",
    "plt.plot(X_line_fitted, y_line_fitted, 'g-')\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqqCYcOt1iUF"
   },
   "source": [
    "## Lasso regularization\n",
    "\n",
    "You don't always have such an obvious case where a simpler model is definitely the correct choice. Sometimes you have to rely on regularization methods that penalize large coefficients.\n",
    "\n",
    "Least Absolute Shrinkage and Selection Operator (Lasso) regularization, also called L1 regularization, is a regularization method that adds the sum of the absolute values of the coefficients as a penalty in a cost function.\n",
    "\n",
    "In Scikit learn we can use the [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) model which performs a linear regression with an L1 regression penalty.\n",
    "\n",
    "In the resultant graph you can see that the regression smooths out our polynomial curve quite a bit, despite the polynomial being a degree 10 polynomial.\n",
    "\n",
    "Note that Lasso regression can make the impact of less important features completely disappear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "16_BAxX_mAVt"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=10, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "lasso_reg = Lasso(alpha=5.0)\n",
    "lasso_reg.fit(X_poly, y)\n",
    "\n",
    "X_line_fitted = np.linspace(X.min(), X.max(), num=100)\n",
    "y_line_fitted = lasso_reg.intercept_\n",
    "for i in range(len(poly_features.powers_)):\n",
    "  exponent = poly_features.powers_[i][0]\n",
    "  y_line_fitted = y_line_fitted + lasso_reg.coef_[i] * (X_line_fitted**exponent)\n",
    "\n",
    "plt.plot(X_line, y_line, 'r-')\n",
    "plt.plot(X_line_fitted, y_line_fitted, 'g-')\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PV1Geq_n3N47"
   },
   "source": [
    "## Ridge regularization\n",
    "\n",
    "Similar to Lasso regularization, [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) regularization adds a penalty to the cost function of a model. In the case of Ridge regularization, also called L2 regularization, the penalty is the square of the coefficient.\n",
    "\n",
    "Again we can see that the regression smooths out the curve of our large-degree polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WlS8-f-ynEEy"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=10, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "ridge_reg = Ridge(alpha=0.5)\n",
    "ridge_reg.fit(X_poly, y)\n",
    "\n",
    "X_line_fitted = np.linspace(X.min(), X.max(), num=100)\n",
    "y_line_fitted = ridge_reg.intercept_\n",
    "for i in range(len(poly_features.powers_)):\n",
    "  exponent = poly_features.powers_[i][0]\n",
    "  y_line_fitted = y_line_fitted + ridge_reg.coef_[0][i] * (X_line_fitted**exponent)\n",
    "\n",
    "plt.plot(X_line, y_line, 'r-')\n",
    "plt.plot(X_line_fitted, y_line_fitted, 'g-')\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vkoP4T6W4Vjl"
   },
   "source": [
    "## ElasticNet regularization\n",
    "\n",
    "Another common form of regularization is [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) regularization. This regularization method combines the concepts of L1 and L2 regularization by applying a penalty containing both a squared value and an absolute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phYvL4o1oPKv"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=10, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "elastic_reg = ElasticNet(alpha=2.0, l1_ratio=0.5)\n",
    "elastic_reg.fit(X_poly, y)\n",
    "\n",
    "X_line_fitted = np.linspace(X.min(), X.max(), num=100)\n",
    "y_line_fitted = elastic_reg.intercept_\n",
    "for i in range(len(poly_features.powers_)):\n",
    "  exponent = poly_features.powers_[i][0]\n",
    "  y_line_fitted = y_line_fitted + elastic_reg.coef_[i] * (X_line_fitted**exponent)\n",
    "\n",
    "plt.plot(X_line, y_line, 'r-')\n",
    "plt.plot(X_line_fitted, y_line_fitted, 'g-')\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0AfrlEyb-Dlu"
   },
   "source": [
    "## Other Strategies\n",
    "\n",
    "Aside from regularization, there are other strategies that can be used to prevent overfitting. These include:\n",
    "\n",
    "* Early stopping\n",
    "* Cross-validation\n",
    "* Simplifying your model\n",
    "* Removing features\n",
    "* Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZztAQi3YPptV"
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PsCX81qd-0v3"
   },
   "source": [
    "For these exercises we will be working with the diabetes dataset that comes packages with Scikit Learn. The code below loads the data and plots BMI vs. S3. We can see from the plots that a polynomial model might work okay for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfqqluBM_mVl"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = load_diabetes()\n",
    "df = pd.DataFrame(data.data, columns=[data.feature_names])\n",
    "\n",
    "X = df['bmi'].values\n",
    "y = df['s3'].values.ravel()\n",
    "\n",
    "plt.plot(X.ravel(), y, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PGZdi0rqClcW"
   },
   "source": [
    "## Exercise 1: Polynomial Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGvum6-lnTnL"
   },
   "source": [
    "* Create a degree-10 polynomial preprocessor for our regression\n",
    "* Create a linear regression\n",
    "* Fit and transform our X values with the polynomial features preprocessor\n",
    "* Fit our transformed data using the linear regression\n",
    "* Plot the fitted line over a scatterplot of the data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2at-qnRUP9yK"
   },
   "source": [
    "### Student Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eXs9gz-3QAH-"
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exercise-1-key-1"
   },
   "source": [
    "### Answer Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exercise-1-solution-1"
   },
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vxBY7sF4Aw2u"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=10, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "regression = LinearRegression()\n",
    "regression.fit(X_poly, y)\n",
    "\n",
    "X_line_fitted = np.linspace(X.min(), X.max(), num=100)\n",
    "y_line_fitted = regression.intercept_\n",
    "\n",
    "for i in range(len(poly_features.powers_)):\n",
    "  exponent = poly_features.powers_[i][0]\n",
    "  y_line_fitted = y_line_fitted + regression.coef_[i] * (X_line_fitted**exponent)\n",
    "\n",
    "plt.plot(X_line_fitted, y_line_fitted, 'g-')\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nnit7-_bQE21"
   },
   "source": [
    "**Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HY-rGv47QGFm"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1tV79QxhDP_i"
   },
   "source": [
    "## Exercise 2: Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZyLCzEanVoD"
   },
   "source": [
    "Copy your code from the exercise above and experiment with the Lasso, Ridge, and/or ElasticNet classes in the place of the `LinearRegression`. Adjust the parameters for whichever regularization class you use until you create a line that doesn't look to be under or over-fitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wilt9sLuQI50"
   },
   "source": [
    "### Student Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owQ0vctpQP_6"
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exercise-2-key-1"
   },
   "source": [
    "### Answer Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exercise-2-solution-1"
   },
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGljYKcVB8AL"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=10, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "regression = Ridge(alpha=0.0001)\n",
    "regression.fit(X_poly, y)\n",
    "\n",
    "X_line_fitted = np.linspace(X.min(), X.max(), num=100)\n",
    "y_line_fitted = regression.intercept_\n",
    "\n",
    "for i in range(len(poly_features.powers_)):\n",
    "  exponent = poly_features.powers_[i][0]\n",
    "  y_line_fitted = y_line_fitted + regression.coef_[i] * (X_line_fitted**exponent)\n",
    "\n",
    "plt.plot(X_line_fitted, y_line_fitted, 'g-')\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uqqfa8teQSOs"
   },
   "source": [
    "**Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dB6Duf2TQTJo"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nso7fOpVEWJb"
   },
   "source": [
    "## Exercise 3: Challenge (Ungraded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jBqG09r615k"
   },
   "source": [
    "Experiment with the [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) in the place of the `LinearRegression` class. Can you get the SGDRegressor to fit a polynomial curve? After you get a curve to fit, experiment with the `early_stopping` setting on the `SGDRegressor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a5DJdaaw6n7a"
   },
   "source": [
    "### Student Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DHhJ1nUZEuV7"
   },
   "outputs": [],
   "source": [
    "# Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exercise-3-key-1"
   },
   "source": [
    "### Answer Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exercise-3-solution-1"
   },
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQOGNKFb6r38"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u-IESeyK6ssH"
   },
   "source": [
    "**Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Y94aa986uFc"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "copyright",
    "exercise-1-key-1",
    "exercise-2-key-1",
    "exercise-3-key-1"
   ],
   "name": "Polynomial Regression and Overfitting",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
